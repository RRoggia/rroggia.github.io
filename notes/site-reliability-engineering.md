---
title: 'Site Reliability Engineering'
language: 'en-US'
status: 'Reading'
coverPath: 'site-reliability-engineering'
date: '2021-05-07'
---

# Foreword
> Tools were only components in processes, working alongside chains of software, people, and data. Nothing here tells us how to solve problems universally, but that is the point. Stories like these are far more valuable than the code or designs they resulted in. Implementations are ephemeral, but the documented reasoning is priceless. Rarely do we have access to this kind of insight.

# Preface

> We apply the principles of computer science and engineering to the design and development of computing systems: generally, large distributed ones. Sometimes, our task is writing the software for those systems alongside our product development counterparts; sometimes, our task is building all the additional pieces those systems need, like backups or load balancing, ideally so they can be reused across systems; and sometimes, our task is figuring out how to apply existing solutions to new problems.

> a system isn’t very useful if nobody can use it! Because reliability is so critical, SREs are focused on finding ways to improve the design and operation of systems to make them more scalable, more reliable, and more efficient.

>  it’s still worth putting lightweight reliability support in place early on, because it’s less costly to expand a structure later on than it is to introduce one that is not present.

# Part I - Introduction

## Chapter 1 - Introduction

### The Sysadmin Approach to Service Management

> Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.

> At their core, the development teams want to launch new features and see them adopted by users. At *their* core, the ops teams want to make sure the service doesn’t break while they are holding the pager. Because most outages are caused by some kind of change—a new configuration, a new feature launch, or a new type of user traffic—the two teams’ goals are fundamentally in tension.

> The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.

### Google’s Approach to Service Management: Site Reliability Engineering

> Site Reliability Engineering teams focus on hiring software engineers to run our products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.

> SRE is what happens when you ask a software engineer to design an operations team.

> Common to all SREs is the belief in and aptitude for developing software systems to solve complex problems.

> The result of our approach to hiring for SRE is that we end up with a team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.

> By design, it is crucial that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload.

> Google places *a 50% cap on the aggregate "ops" work for all SREs*—tickets, on-call, manual tasks, etc.

> Because SREs are directly modifying code in their pursuit of making Google’s systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. Such teams are relatively inexpensive—supporting the same service with an ops-oriented team would require a significantly larger number of people. 

### DevOps or SRE?

> One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.

### Tenets of SRE

> SRE team is responsible for the *availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning* of their service(s).

#### Ensuring a Durable Focus on Engineering

>  by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on.

> Postmortems should be written for all significant incidents, regardless of whether or not they paged

> This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time

> Google operates under a *blame-free postmortem culture*, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.

#### Pursuing Maximum Change Velocity Without Violating a Service’s SLO

> thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.

> - What level of availability will the users be happy with, given how they use the product?
> - What alternatives are available to users who are dissatisfied with the product’s availability?
> - What happens to users’ usage of the product at different availability levels?

> Once that target is established, the error budget is one minus the availability target.

> An outage is no longer a "bad" thing—it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear.

#### Monitoring

> Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.

#### Alerts

> Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.

#### Tickets

> Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.

#### Logging

> No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.

#### Emergency Response

>  Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). The most relevant metric in evaluating the effectiveness of emergency response is how quickly the response team can bring the system back to health—that is, the MTTR.

> Humans add latency. Even if a given system experiences more *actual* failures, a system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention

> When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a "playbook" produces roughly a 3x improvement in MTTR as compared to the strategy of "winging it."

#### Change Management

> SRE has found that roughly 70% of outages are due to changes in a live system

> - Implementing progressive rollouts
> - Quickly and accurately detecting problems
> - Rolling back changes safely when problems arise

#### Demand Forecasting and Capacity Planning

> Capacity planning should take both organic growth (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.

#### Provisioning

> Adding new capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results. Thus, it is a riskier operation than load shifting, which is often done multiple times per hour, and must be treated with a corresponding degree of extra caution.

#### Efficiency and Performance

> Resource use is a function of demand (load), capacity, and software efficiency.

### My Summary

Historically, companies use the sysadmin model. Which is responsible for dividing the Developers and Operations. At its core developers and operations have different goals. Developers want to deliver code to production as fast as possible. Operations wants to have a reliable system. Since, changes might cause outage, there is a tension between developers and operations goals. This model is also expensive due to the need to add more people as the software scales.

Google's SRE teams are composed of software engineers hired to do operations. These teams have the skill to automate the process and can be rapidly bored by repetitive tasks. Therefore, creating a great combination for continuous improvement.

The SRE skills are very similar to developers skills, UNIX and networking being the main differentiators.

It's important that these teams focus on engineering. Otherwise, it will be required to increase the staff of the teams. To achieve the focus on engineering, Google provides a cap of 50% of work for engineering. If they workload on operations exceeds this number, bugs, on-call and so on are redirected to the development team.

SRE can be considered a Google's implementation of DevOps.

SRE team is responsible for the:

- Availability

  There's no gain to have a 100% reliable system, because there are other players into play with lower reliability (WiFi, ISP, power grid, ...)

  The availability is a product decision. The error budget is value of permitted unavailability. Developers may use this budget to speed up releases. The aim is not to have zero outages, but to not exceed the error budget.

  Postmortem it's important to expose faults and improve systems. A culture of fixing errors instead of hiding or minimizing them.

- Latency

- Efficiency and Performance

- Change Management:

  - Implementing progressive rollouts
  - Quickly and accurately detecting problems
  - Rolling back changes safely when problems arise

- Monitoring: 

  Keep track of a system’s health and availability. Software monitors human are notified when need to take action.

  - Alerts: Human needs to take an action immediately in response to something that is either happening or about to happen.
  - Tickets: a human needs to take action, but not immediately. If a human takes action in a few days, no damage will result
  - Logging: Recorded for diagnostic or forensic purposes. People read the logs when something prompts them to do so.

- Emergency Response:

  How quickly the response team can bring the system back to health. Adding a "playbook" produces roughly a 3x improvement.

  - MTTR (Repair)
  - MTTF (Failure)

- Capacity Planning: 

  There is sufficient capacity and redundancy to serve projected future demand with the required availability

  - Organic growth (natural adoption)
  - Inorganic growth (feature launches, marketing campaign, ...)

## Chapter 2 - The Production Environment at Google, from the Viewpoint of an SRE

### System Software That "Organizes" the Hardware

> Given the large number of hardware components in a cluster, hardware failures occur quite frequently. In a single cluster in a typical year, thousands of machines fail and thousands of hard disks break; when multiplied by the number of clusters we operate globally, these numbers become somewhat breathtaking.

#### Networking

> In order to minimize latency for globally distributed services, we want to direct users to the closest datacenter with available capacity.

### Our Development Environment

> When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.

### My Summary

This chapter they describe Google's Environment.

The cluster operating system Borg handles resource allocation. Borg is the precedent of Kubernetes. r

# Part II - Principles

## Chapter 3 - Embracing Risk

> It turns out that past a certain point, however, increasing reliability is worse for a service (and its users) rather than better! Extreme reliability comes at a cost: maximizing stability limits how fast new features can be developed and how quickly products can be delivered to users, and dramatically increases their cost, which in turn reduces the numbers of features a team can afford to offer

>  Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient service operations, so that users’ overall happiness—with features, service, and performance—is optimized.

### Managing Risks

> We strive to make a service reliable enough, but no *more* reliable than it needs to be.

> That is, when we set an availability target of 99.99%,we want to exceed it, but not by much: that would waste opportunities to add features to the system, clean up technical debt, or reduce its operational costs.

### Measuring Service Risk

> As standard practice at Google, we are often best served by identifying an objective metric to represent the property of a system we want to optimize. By setting a target, we can assess our current performance and track improvements or degradations over time.

> For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime.

#### Time-based availability

> Availability = uptime / (uptime + downtime)

#### Aggregate availability

>  Availability = successful request / total requests

> Most often, we set quarterly availability targets for a service and track our performance against those targets on a weekly, or even daily, basis. This strategy lets us manage the service to a high-level availability objective by looking for, tracking down, and fixing meaningful deviations as they inevitably arise

## Risk Tolerance of Services

> SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer. In this case, the business goals we’re concerned about have a direct impact on the performance and reliability of the service offered.

### Identifying the Risk Tolerance of Consumer Services

> - What level of availability is required?
> - Do different types of failures have different effects on the service?
> - How can we use the service cost to help locate a service on the risk continuum?
> - What other service metrics are important to take into account?

#### Target level of availability

> - What level of service will the users expect?
> - Does this service tie directly to revenue (either our revenue, or our customers’ revenue)?
> - Is this a paid service, or is it free?
> - If there are competitors in the marketplace, what level of service do those competitors provide?
> - Is this service targeted at consumers, or at enterprises?

#### Types of failures

> Which is worse for the service: a constant low rate of failures, or an occasional full-site outage? Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.

> Because most of this work takes place during normal business hours, we determined that occasional, regular, scheduled outages in the form of maintenance windows would be acceptable, and we counted these scheduled outages as planned downtime, not unplanned downtime.

#### Cost

> It may be harder to set these targets when we do not have a simple translation function between reliability and revenue.

### Identifying the Risk Tolerance of Infrastructure Services

> A fundamental difference is that, by definition, infrastructure components have multiple clients, often with varying needs.

## Motivation for Error Budgets

> The product developers have more visibility into the time and effort involved in writing and releasing their code, while the SREs have more visibility into the service’s reliability (and the state of production in general).

> Instead, our goal is to define an objective metric, agreed upon by both sides, that can be used to guide the negotiations in a reproducible way.

#### Forming Your Error Budget

> The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter.

> - Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter.
> - The actual uptime is measured by a neutral third party: our monitoring system.
> - The difference between these two numbers is the "budget" of how much "unreliability" is remaining for the quarter.
> - As long as the uptime measured is above the SLO—in other words, as long as there is error budget remaining—new releases can be pushed.

### Benefits

> The main benefit of an error budget is that it provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability.

> If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make the system more resilient, improve its performance, and so on.

>  If the team is having trouble launching new features, they may elect to loosen the SLO (thus increasing the error budget) in order to increase innovation.

### My Summary

The cost:

- The redundant compute resources
- The opportunity cost

Typical tension:

- Software fault tolerance
- Testing
- Push frequency
- Canary duration and size

## Chapter 4 - Service Level Objectives

