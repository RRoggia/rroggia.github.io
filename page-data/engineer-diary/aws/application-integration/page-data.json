{
    "componentChunkName": "component---src-templates-folder-template-js",
    "path": "/engineer-diary/aws/application-integration",
    "result": {"pageContext":{"parentPath":"/engineer-diary/aws/application-integration","files":[{"fileAbsolutePath":"/home/runner/work/rroggia.github.io/rroggia.github.io/content/engineer-diary/AWS/application-integration/event-bridge.md","html":"<p><a href=\"https://docs.aws.amazon.com/eventbridge/\">Documentation</a></p>\n<p>Demo:</p>\n<ul>\n<li><a href=\"https://github.com/RRoggia/aws-workloads/tree/main/event-bridge\">Publish to Event Bus and send to SQS Target</a></li>\n</ul>\n<h2>Overview</h2>\n<p>Event Bridge provides functionality to integrate applications using events. The event bridge is a regional service hosted by AWS where you can create your own Event Buses or Pipes and configure it to send the events to other services and applications.</p>\n<p>AWS Event Bridge offers two types channels to process events:</p>\n<ul>\n<li><strong>Event Bus</strong>: It receives events from multiple sources and sends to multiple targets accordingly to the Event Rules. Publish/Subscribe allowing many to many.</li>\n<li><strong>Pipes</strong>: It receives events from a single source and sends to a single target. Point to point.</li>\n</ul>\n<h3>Integrating with Event Buses</h3>\n<p>An Event bus is a router that receives <strong>events</strong> from multiple <strong>sources</strong> and evaluates the <strong>Event Rules</strong> for each event received to identify whether it should and how to process each event. There are 3 types of Event bus:</p>\n<ul>\n<li>AWS Service (<strong>default</strong>): Each region in a AWS Account has one. AWS services send its events to the default bus.</li>\n<li>Custom: Customer created event bus. Can be used to integrate your applications.</li>\n<li>SaaS provider: It's used to receive events from AWS Partners like Salesforce.</li>\n</ul>\n<p><img src=\"/images/engineer-diary/AWS/application-integration/event-bridge/event-bus-types.png\" alt=\"Event bus types\"></p>\n<p>Each Event bus can have up to 300 Event rules, an event rule can be configured to send the event to other Event bus in the same and different accounts and in different regions.</p>\n<p>An <strong>Event Rule</strong> matches incoming events and send to targets for processing. An event rule is assigned to an event bus. AWS provides managed rules which are created by them. There are two types of Event rules:</p>\n<ul>\n<li>\n<p><strong>Event Pattern:</strong> Event pattern allows to trigger actions based on matching the event's metadata, data structure or content.</p>\n<ul>\n<li><code>{ \"source\": [\"app1\"]}</code>: Triggers action if event's source attribute has value <code>app1</code>.</li>\n</ul>\n</li>\n<li>\n<p><strong>Schedule (only for default bus):</strong> It allows to use rate expressions or cron expressions to trigger actions.</p>\n<ul>\n<li><code>rate(1 hour)</code>: Triggers action every hour.</li>\n</ul>\n</li>\n</ul>\n<p>Besides the event matching mechanism the Event Rule also expects from 1 to 5 targets. A <strong>Target</strong> is the resource that Event Bridge sends the event when it matches to a Event Rule pattern.  A target can be an AWS Services, another Event Bus or an <strong>API Destination</strong> (third party targets invoked through HTTP endpoints).</p>\n<p>A Target provides some extra functionality, for example it allows you to configure <strong>Input transformation</strong> to manipulate the event data before sending it to its target, it allows to send the event to a SQS Dead Letter Queue in case the targets fails to receive the event and configure retrial policies. In case an Event rule is triggered and has multiple targets they run in parallel.</p>\n<p>Event bus also supports <strong>Archive</strong>. An archive receives all events of a given event bus, or, you can specify Event Patterns to filter events that are sent to the archive. Archives can have indefinite retention periods or any custom time.</p>\n<p>You can <strong>Replay</strong> events in an archive. When you create a Replay you specify a time frame, then, it uses an archive as source and replays the events in that time frame to the same event bus that initially recorded the event. You can configure which Event Rules should be evaluated in the target event bus during the replay.</p>\n<h3>Integrating with Pipes</h3>\n<p><strong>Pipes</strong> offer a similar functionality than Event Bus, however, it's a point to point channel. Therefore, only one source and one target are allowed.</p>\n<p>In addition, pipes supports optional Filtering and Enhancement steps. In the <strong>Filtering step</strong>, you can use the Event Pattern to filter the events sent through the pipe. In the <strong>Enhancement step</strong>, you can configure AWS Lambda, API Gateway or a Step function or an API destination to enhance the event data. </p>\n<p>The pipes supports Input transformation before sending the event data to both the Enhancement step and to the target.</p>\n<h3>Security</h3>\n<p>It's important to notice that for both pipes and Event Bus, the target needs to allow the Event Bridge to interact its resource. For example, if an Event Rule sends event data to a SQS queue. The queue needs to allow the Event rule to send message to the queue.</p>","frontmatter":{"title":"Event Bridge"},"isFile":true,"fileName":"event-bridge"},{"fileAbsolutePath":"/home/runner/work/rroggia.github.io/rroggia.github.io/content/engineer-diary/AWS/application-integration/simple-queue-service.md","html":"<p><a href=\"https://docs.aws.amazon.com/sqs/index.html\">Documentation</a></p>\n<h2>Overview</h2>\n<p>SQS is a regional fully managed queue service commonly used to integrate distributed systems in a decoupled, highly available and highly durable manner. It's a public service, therefore, it's possible to access from anywhere where you have access to the AWS public space.</p>\n<h2>Core Concept</h2>\n<p>The main core concept of SQS are the queues. It supports two types of <strong>Queues</strong>:</p>\n<ul>\n<li><strong>FIFO</strong>: The messages are <strong>delivered exactly once</strong> using the First In, First Out as the <strong>ordering mechanism</strong>. However, it has a limit of 3000 messages per second. Its performance is reduced due to the order and delivery guarantees. It supports up to 300 messages per second or using batch (10 events in a message) 3000 messages. Queues require the <strong>.fifo</strong> suffix.</li>\n<li><strong>Standard</strong>: Best effort ordering mechanism (<strong>order is not guaranteed</strong>), and a message is delivered <strong>at least once</strong> (potentially more times).</li>\n</ul>\n<p>Once you create the queue you can't change its type.</p>\n<h3>Sending a message</h3>\n<p>The queue receive events through the SQS <code>sendMessage</code> or <code>sendMessageBatch</code> API. The <strong>message size</strong> can be up to 256 KB for a single message or in the Batch API the number of message times the maximum message size. </p>\n<h3>Storing a message and handling its lyfecicle</h3>\n<p>A message remains in the queue until it is deleted, is moved or expires. Messages expire when reach the maximum <strong>retention period</strong>. It can range between 1 minute to 14 days. To move a message you configure a <strong>Redrive policy</strong> to send undeliverable messages (could not be processed after up to 1000 attempts or expired messages) to a <strong>Dead Letter Queue (DLQ)</strong>. The Redrive policy specifies the source queue, the DLQ where the message is going to be moved and the number of times a consumer attempts to process a message (<code>maxReceiveCount</code>).</p>\n<p>DLQ are used to handle the lifecycle of unconsumed messages . By default, all queues can be used as DLQ. However, the source and DLQ must have the same type both standard or both FIFO). You can also enable the <strong>Redrive allow policy</strong> in a queue to detail the source queues you want to allow or deny sources queues to move messages to the queue.</p>\n<p>A queue can have a <strong>Delivery Delay</strong> of up to 15 minutes, these are called delay queues. When the message arrives it starts to count the delay time, while the message is within the delay time it's not visible to the consumers to retrieve it. If you configure a delay to an existing queue there are two different behaviors depending on the queue type.</p>\n<ul>\n<li>Standard: The delay do not applies retroactively on messages</li>\n<li>FIFO: The delay do not applies retroactively on messages.</li>\n</ul>\n<p>Standard queues also support <strong>Message Timers</strong>, which enable to send per message delivery delay by specifying the <code>DelaySeconds</code> in the <code>sendMessage</code> or <code>sendMessageBatch</code> API.</p>\n<h3>Reading a message</h3>\n<p>To read messages from the queue you can use the <code>receiveMessage</code> API. There is two options to consume this API that vary depending on the queue's attribute <strong>Receive Message Wait time</strong> or the SDK.</p>\n<ul>\n<li><strong>Short pooling</strong>: When no <code>WaitTimeSeconds</code> is specified or equals to <code>0</code>. The default behavior. Sends the response right away from a subset of all distributed servers. You might receive empty responses even though there are messages to be consumed.</li>\n<li><strong>Long poling</strong>: When the <code>WaitTimeSeconds</code> is between <code>1</code> and <code>20</code>. SQS sends the response after it can find all the messages before the time elapses or returns the messages it got when the specified time elapses.</li>\n</ul>\n<p>Long pooling is a cheaper approach because it reduces the number of false empty responses and the number of HTTP requests.</p>\n<p>The <code>receiveMessage</code> response returns the messages from the queue to process that requested it, the process needs to send a <code>deleteMessage</code> or <code>deleteMessageBatch</code> to acknowledge the queue the message was processed and should be deleted. The <strong>Visibility Timeout</strong> configuration specifies the time the message is going to be hidden in the queue once it's returned in the <code>receiveMessage</code>, the process has the visibility timeout time to send the delete request with the message <strong>Receipt Id</strong>, otherwise, the message is visible again in the queue.</p>\n<p>When a Redrive policy is configured for the queue, each time the message is received the <code>receiveCountAttribute</code> is increased, when it reaches the maximum attempts it moves the message to the DLQ. When the message is moved to the DLQ its enqueue time, which is used to compute its expiration continues the same, therefore, you want the DLQ retention period to be higher than the source queue's retention period.</p>\n<h1>TODO</h1>\n<ul>\n<li>ASG can scale based on queue size</li>\n<li>Lambdas can be triggered by messages in queue</li>\n<li>SNS send to SQS</li>\n</ul>","frontmatter":{"title":"SQS - Simple Queue Service"},"isFile":true,"fileName":"simple-queue-service"}],"folders":[]}},
    "staticQueryHashes": ["1507822185","2095566405","2894216461","425755332"]}