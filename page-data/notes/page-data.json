{"componentChunkName":"component---src-pages-notes-js","path":"/notes/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"","frontmatter":{"title":"Como ser um programador melhor","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Data Science do Zero","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Effective Java - Second Edition","language":"en-US","coverPath":null,"status":"Read","date":null}}},{"node":{"html":"","frontmatter":{"title":"Estatistica - O que é, para que serve e como funciona","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Implementando o desenvolvimento Lean de Software","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Lean Inception","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Management 3.0","language":"en-US","coverPath":null,"status":"Planning","date":null}}},{"node":{"html":"","frontmatter":{"title":"Microserviços prontos para a produção","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"O Mitico homem-mes","language":"pt-BR","coverPath":"o-mitico-homem-mes","status":"Reading","date":null}}},{"node":{"html":"","frontmatter":{"title":"O Sistema toyota de proução","language":"pt-BR","coverPath":null,"status":"Read","date":null}}},{"node":{"html":"","frontmatter":{"title":"Padrões de Projeto: soluções reutilizáveis de software orientado a objetos","language":"pt-BR","coverPath":null,"status":"Reading","date":null}}},{"node":{"html":"","frontmatter":{"title":"Qualidade de Software","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"TDD - Desenvolvimento Guiado por Testes","language":"pt-BR","coverPath":null,"status":"Reading","date":null}}},{"node":{"html":"","frontmatter":{"title":"Trabalho eficaz com código legado","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Treinamento de equipes ageis","language":"pt-BR","coverPath":null,"status":"Backlog","date":null}}},{"node":{"html":"","frontmatter":{"title":"Unlocking Agility","language":"en-US","coverPath":"unlocking-agility","status":"Backlog","date":null}}},{"node":{"html":"<h1>Preface</h1>\n<blockquote>\n<p>If you do not see at first how to solve a problem, don’t give up, think about it some more; be sure you understand all the terminology used in the problem, play with some ideas. If no approach presents itself, let it be and think about it again later. Repeat this process for days on end. When you finally wake up in the middle of the night with an idea, you’ll know you are putting in the right amount of effort for this course.”</p>\n</blockquote>\n<h1>1 - Formal Logic</h1>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> A <strong>statement</strong> (or <strong>proposition</strong>) is a sentence that is either true or false.</p>\n</blockquote>\n<blockquote>\n<p>...A, B, and C, are used to represent statements and are called statement letters;</p>\n</blockquote>\n<blockquote>\n<p>the symbol <code>^</code> is a logical connective representing <em>and</em></p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> The expression <code>A ^ B</code> is called the <strong>conjunction</strong> of A and B, and A and B are called the <strong>conjuncts</strong> of this expression</p>\n</blockquote>\n<blockquote>\n<p>Another connective is the word <em>or</em>, denoted by the symbol <code>V</code>.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> The expression <code>A V B</code> (read “A or B”) is called the <strong>disjunction</strong> of A and B, and A and B are called the <strong>disjuncts</strong> of this expression</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> The logical connective here is <strong>implication</strong>, and it conveys the meaning that the truth of A implies or leads to the truth of B. In the implication <code>A -> B</code>, A stands for the <strong>antecedent</strong> statement and B stands for the <strong>consequent</strong> statement.</p>\n</blockquote>\n<blockquote>\n<p>By convention, <code>A -> B</code> is considered true if A is false, regardless of the truth value of B.</p>\n</blockquote>\n<p>Implication: Either both are true (like an if statement) or if the antecedent is false the result is true.</p>\n<blockquote>\n<p>The <strong>equivalence</strong> connective is symbolized by <code>&#x3C;-></code>. </p>\n</blockquote>\n<blockquote>\n<p>(It's a shortcut) The expression <code>A &#x3C;-> B</code> stands for <code>(A -> B) ^ (B -> A)</code>.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> The connectives we’ve seen so far are called <strong>binary connectives</strong> because they join two expressions together to produce a third expression.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> a <strong>unary connective</strong>, a connective acting on one expression to produce a second expression</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> <strong>Negation</strong> is a unary connective.</p>\n</blockquote>\n<blockquote>\n<p>The negation of A—­symbolized by <code>A′</code>—is read “not A.”</p>\n</blockquote>\n<p>“If there is smoke, then there is fire.”</p>\n<ul>\n<li>If there is smoke (T) then there is fire (T) -> true</li>\n<li>If there is smoke (T) then there is fire (F)  -> false</li>\n<li>If there is smoke (F) then there is fire (T)  -> true</li>\n<li>If there is smoke (F) then there is fire (F)  -> true</li>\n</ul>\n<blockquote>\n<p>This\norder of precedence is</p>\n<ol>\n<li>connectives within parentheses, innermost parentheses first</li>\n<li><code>'</code></li>\n<li><code>V ^</code></li>\n<li><code>-></code></li>\n<li><code>&#x3C;-></code></li>\n</ol>\n</blockquote>","frontmatter":{"title":"Mathematical Structure for Computer Science","language":"en-US","coverPath":"mathematical-structure-for-computer-science","status":"Reading","date":"2022-08-29"}}},{"node":{"html":"<p><a href=\"https://datatracker.ietf.org/doc/html/rfc7636\">RFC7636 -  Proof Key for Code Exchange by OAuth Public Clients</a></p>\n<h1>Abstract</h1>\n<blockquote>\n<p>OAuth 2.0 public clients utilizing the Authorization Code Grant are susceptible to the authorization code interception attack. This specification describes the attack as well as a technique to mitigate against the threat through the use of Proof Key for Code Exchange (PKCE, pronounced \"pixy\").</p>\n</blockquote>\n<h1>1. Introduction</h1>\n<blockquote>\n<p>In this attack, the attacker intercepts the authorization code returned from the authorization endpoint within a communication path not protected by Transport Layer Security (TLS), such as inter- application communication within the client’s operating system.</p>\n</blockquote>\n<blockquote>\n<p>To mitigate this attack, this extension utilizes a dynamically created cryptographically random key called \"code verifier\". A unique code verifier is created for every authorization request, and its transformed value, called \"code challenge\", is sent to the authorization server to obtain the authorization code. The authorization code obtained is then sent to the token endpoint with the \"code verifier\", and the server compares it with the previously received request code so that it can perform the proof of possession of the \"code verifier\" by the client. </p>\n</blockquote>\n<h1>1.1 Protocol FLow</h1>\n<blockquote>\n<p>A. The client creates and records a secret named the \"code<em>verifier\" and derives a transformed version \"t(code</em>verifier)\" (referred to as the \"code<em>challenge\"), which is sent in the OAuth 2.0 Authorization Request along with the transformation method \"t</em>m\".</p>\n<p>B. The Authorization Endpoint responds as usual but records \"t(code_verifier)\" and the transformation method.</p>\n<p>C. The client then sends the authorization code in the Access Token Request as usual but includes the \"code_verifier\" secret generated at (A).</p>\n<p>D. The authorization server transforms \"code_verifier\" and compares it to \"t(cod</p>\n</blockquote>\n<h1>3. Terminology</h1>\n<blockquote>\n<p><strong>code verifier</strong> A cryptographically random string that is used to correlate the authorization request to the token request.</p>\n</blockquote>\n<blockquote>\n<p><strong>code challenge</strong> A challenge derived from the code verifier that is sent in the authorization request, to be verified against later.</p>\n</blockquote>\n<blockquote>\n<p><strong>code challenge method</strong> A method that was used to derive code challenge.</p>\n</blockquote>\n<h1>4. Protocol</h1>\n<h2>4.1 Client Creates a Code Verifier</h2>\n<blockquote>\n<p>The client first creates a code verifier, \"code_verifier\", for each OAuth 2.0 [RFC6749] Authorization Request</p>\n</blockquote>\n<h2>4.2. Client Creates the Code Challenge</h2>\n<blockquote>\n<p>The client then creates a code challenge derived from the code verifier by using one of the following transformations on the code verifier:</p>\n<p><strong>plain</strong> code<em>challenge = code</em>verifier</p>\n<p> <strong>S256</strong> code<em>challenge = BASE64URL-ENCODE(SHA256(ASCII(code</em>verifier)))</p>\n</blockquote>\n<blockquote>\n<p>If the client is capable of using \"S256\", it MUST use \"S256\"</p>\n</blockquote>\n<h2>4.3. Client Sends the Code Challenge with the Authorization Request</h2>\n<p>The client sends both the <code>code_challenge</code> and the <code>code_challenge_method</code> in the OAuth authorization request.</p>\n<h2>4.4. Server Returns the Code</h2>\n<blockquote>\n<p>When the server issues the authorization code in the authorization response, it MUST associate the \"code<em>challenge\" and \"code</em>challenge_method\" values with the authorization code so it can be verified later.</p>\n</blockquote>\n<blockquote>\n<p>The server MUST NOT include the \"code_challenge\" value in client requests in a form that other entities can extract.</p>\n</blockquote>\n<h2>4.5. Client Sends the Authorization Code and the Code Verifier to the Token Endpoint</h2>\n<p>In addition to the authorization code the client also sends the <code>code_verifier</code>to the authorization server in the <code>/token</code> OAuth HTTP request.</p>\n<h2>4.6. Server Verifies code_verifier before Returning the Tokens</h2>\n<blockquote>\n<p>Upon receipt of the request at the token endpoint, the server verifies it by calculating the code challenge from the received \"code<em>verifier\" and comparing it with the previously associated \"code</em>challenge\", after first transforming it according to the \"code<em>challenge</em>method\" method specified by the client.</p>\n</blockquote>\n<h1>7. Security Considerations</h1>\n<h2>7.1. Entropy of the code_verifier</h2>\n<blockquote>\n<p>The security model relies on the fact that the code verifier is not learned or guessed by the attacker.</p>\n</blockquote>\n<h2>7.2. Protection against Eavesdroppers</h2>\n<blockquote>\n<p>The \"S256\" method protects against eavesdroppers observing or intercepting the \"code_challenge\", because the challenge cannot be used without the verifier. </p>\n</blockquote>\n<blockquote>\n<p>Because of this, \"plain\" SHOULD NOT be used and exists only for compatibility with deployed implementations where the request path is already protected.</p>\n</blockquote>","frontmatter":{"title":"RFC7636 -  Proof Key for Code Exchange by OAuth Public Clients","language":"en-US","coverPath":null,"status":"Read","date":"2022-07-17"}}},{"node":{"html":"<h1>Introduction</h1>\n<blockquote>\n<p>This document defines a new JWT-based mode to encode authorization responses. Clients are enabled to request the transmission of the authorization response parameters along with additional data in JWT format. This mechanism enhances the security of the standard authorization response since it adds support for signing and encryption, sender authentication, audience restriction as well as protection from replay, credential leakage, and mix-up attacks. It can be combined with any response type.</p>\n</blockquote>\n<h2>4.1. The JWT Response Document</h2>\n<blockquote>\n<p>The JWT always contains the following data utilized to secure the transmission:</p>\n<ul>\n<li>iss - the issuer URL of the authorization server that created the response</li>\n<li>aud - the client_id of the client the response is intended for</li>\n<li>exp - expiration of the JWT</li>\n</ul>\n</blockquote>\n<h3>4.1.1. Response Type Code</h3>\n<p>Example of authorization code grant flow sending the <code>code</code> and <code>state</code> within the JWT:</p>\n<pre><code>{\n\"iss\":\"https://accounts.example.com\",\n\"aud\":\"s6BhdRkqt3\",\n\"exp\":1311281970,\n\"code\":\"PyyFaux2o7Q0YfXBU32jhw.5FXSQpvr8akv9CeRDSd0QA\",\n\"state\":\"S8NJ7uqk5fY4EjNvP_G_FtyJu6pUsvH9jsYni9dMAJw\"\n}\n</code></pre>\n<p>Example of the <code>token</code>  response:</p>\n<pre><code>{\n\"iss\":\"https://accounts.example.com\",\n\"aud\":\"s6BhdRkqt3\",\n\"exp\":1311281970,\n\"access_token\":\"2YotnFZFEjr1zCsicMWpAA\",\n\"state\":\"S8NJ7uqk5fY4EjNvP_G_FtyJu6pUsvH9jsYni9dMAJw\",\n\"token_type\":\"bearer\",\n\"expires_in\":\"3600\",\n\"scope\":\"example\"\n}\n</code></pre>\n<h2>4.2. Signing and Encryption</h2>\n<blockquote>\n<p>The JWT is either signed, or signed and encrypted. If the JWT is both signed and encrypted, the JSON document will be signed then encrypted, with the result being a Nested JWT, as defined in [RFC7519].</p>\n</blockquote>\n<h2>4.3. Response Encoding</h2>\n<blockquote>\n<p>This draft defines the following response mode values:</p>\n<ul>\n<li>query.jwt</li>\n<li>fragment.jwt</li>\n<li>form_post.jwt</li>\n<li>jwt</li>\n</ul>\n</blockquote>\n<h3>4.3.1. Response Mode \"query.jwt\"</h3>\n<p>Redirects to client redirect URI adding a parameter <code>response</code> containing the JWT. The Content Type is <code>application/x-www-form-urlencoded</code>.</p>\n<pre><code>HTTP/1.1 302 Found\nLocation: https://client.example.com/cb?\nresponse=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLm\nV4YW1wbGUuY29tIiwiYXVkIjoiczZCaGRSa3F0MyIsImV4cCI6MTMxMTI4MTk3MCwiY29kZSI6IlB5eU\nZhdXgybzdRMFlmWEJVMzJqaHcuNUZYU1FwdnI4YWt2OUNlUkRTZDBRQSIsInN0YXRlIjoiUzhOSjd1cW\ns1Zlk0RWpOdlBfR19GdHlKdTZwVXN2SDlqc1luaTlkTUFKdyJ9.HkdJ_TYgwBBj10C-aWuNUiA062Amq\n2b0_oyuc5P0aMTQphAqC2o9WbGSkpfuHVBowlb-zJ15tBvXDIABL_t83q6ajvjtq_pqsByiRK2dLVdUw\nKhW3P_9wjvI0K20gdoTNbNlP9Z41mhart4BqraIoI8e-L_EfAHfhCG_DDDv7Yg\n</code></pre>\n<blockquote>\n<p>Note: \"query.jwt\" MUST NOT be used in conjunction with response types that contain \"token\" or \"id_token\" unless the response JWT is encrypted to prevent token leakage in the URL.</p>\n</blockquote>\n<h3>4.3.2. Response Mode \"fragment.jwt\"</h3>\n<p>Same as query, but using fragment instead of a parameter.</p>\n<pre><code>HTTP/1.1 302 Found\nLocation: https://client.example.com/cb#\nresponse=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL2FjY291bnRzLm\nV4YW1wbGUuY29tIiwiYXVkIjoiczZCaGRSa3F0MyIsImV4cCI6MTMxMTI4MTk3MCwiYWNjZXNzX3Rva2\nVuIjoiMllvdG5GWkZFanIxekNzaWNNV3BBQSIsInN0YXRlIjoiUzhOSjd1cWs1Zlk0RWpOdlBfR19GdH\nlKdTZwVXN2SDlqc1luaTlkTUFKdyIsInRva2VuX3R5cGUiOiJiZWFyZXIiLCJleHBpcmVzX2luIjoiMz\nYwMCIsInNjb3BlIjoiZXhhbXBsZSJ9.bgHLOu2dlDjtCnvTLK7hTN_JNwoZXEBnbXQx5vd9z17v1Hyzf\nMqz00Vi002T-SWf2JEs3IVSvAe1xWLIY0TeuaiegklJx_gvB59SQIhXX2ifzRmqPoDdmJGaWZ3tnRyFW\nNnEogJDqGFCo2RHtk8fXkE5IEiBD0g-tN0GS_XnxlE\n</code></pre>\n<h3>4.3.3. Response Mode \"form_post.jwt\"</h3>\n<blockquote>\n<p>The response parameter containing the JWT is encoded as HTML form value that is auto-submitted in the User Agent, and thus is transmitted via the HTTP POST method to the Client, with the result parameters being encoded in the body using the \"application/x-www-form-urlencoded\" format.</p>\n</blockquote>\n<pre><code>HTTP/1.1 200 OK\nContent-Type: text/html;charset=UTF-8\nCache-Control: no-cache, no-store\nPragma: no-cache\n&#x3C;html>\n    &#x3C;head>&#x3C;title>Submit This Form&#x3C;/title>&#x3C;/head>\n    &#x3C;body onload=\"javascript:document.forms[0].submit()\">\n        &#x3C;form method=\"post\" action=\"https://client.example.com/cb\">\n            &#x3C;input type=\"hidden\" name=\"response\"\n            value=\"eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL2\n            FjY291bnRzLmV4YW1wbGUuY29tIiwiYXVkIjoiczZCaGRSa3F0MyIsImV4cCI6MTM\n            xMTI4MTk3MCwiYWNjZXNzX3Rva2VuIjoiMllvdG5GWkZFanIxekNzaWNNV3BBQSIs\n            InN0YXRlIjoiUzhOSjd1cWs1Zlk0RWpOdlBfR19GdHlKdTZwVXN2SDlqc1luaTlkT\n            UFKdyIsInRva2VuX3R5cGUiOiJiZWFyZXIiLCJleHBpcmVzX2luIjoiMzYwMCIsIn\n            Njb3BlIjoiZXhhbXBsZSJ9.bgHLOu2dlDjtCnvTLK7hTN_JNwoZXEBnbXQx5vd9z1\n            7v1HyzfMqz00Vi002T-SWf2JEs3IVSvAe1xWLIY0TeuaiegklJx_gvB59SQIhXX2i\n            fzRmqPoDdmJGaWZ3tnRyFWNnEogJDqGFCo2RHtk8fXkE5IEiBD0g-tN0GS_XnxlE\"/>\n        &#x3C;/form>\n    &#x3C;/body>\n&#x3C;/html>\n</code></pre>\n<p>The user agent auto submit sending an HTTP POST request to the callback</p>\n<pre><code>POST /cb HTTP/1.1\nHost: client.example.org\nContent-Type: application/x-www-form-urlencoded\nresponse=eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJodHRwczovL2\nFjY291bnRzLmV4YW1wbGUuY29tIiwiYXVkIjoiczZCaGRSa3F0MyIsImV4cCI6MTM\nxMTI4MTk3MCwiYWNjZXNzX3Rva2VuIjoiMllvdG5GWkZFanIxekNzaWNNV3BBQSIs\nInN0YXRlIjoiUzhOSjd1cWs1Zlk0RWpOdlBfR19GdHlKdTZwVXN2SDlqc1luaTlkT\nUFKdyIsInRva2VuX3R5cGUiOiJiZWFyZXIiLCJleHBpcmVzX2luIjoiMzYwMCIsIn\nNjb3BlIjoiZXhhbXBsZSJ9.bgHLOu2dlDjtCnvTLK7hTN_JNwoZXEBnbXQx5vd9z1\n7v1HyzfMqz00Vi002T-SWf2JEs3IVSvAe1xWLIY0TeuaiegklJx_gvB59SQIhXX2i\nfzRmqPoDdmJGaWZ3tnRyFWNnEogJDqGFCo2RHtk8fXkE5IEiBD0g-tN0GS_XnxlE\n</code></pre>\n<h3>4.3.4. Response Mode \"jwt\"</h3>\n<blockquote>\n<p>The response mode \"jwt\" is a shortcut and indicates the default redirect encoding (query, fragment) for the requested response type. The default for response type \"code\" is \"query.jwt\" whereas the default for \"token\" and the response types defined in [OIDM], except \"none\", is \"fragment.jwt\".</p>\n</blockquote>","frontmatter":{"title":"Financial-grade API: JWT Secured Authorization Response Mode for OAuth 2.0 (JARM)","language":"en-US","coverPath":null,"status":"Read","date":"2022-07-08"}}},{"node":{"html":"<h1>Introduction</h1>\n<blockquote>\n<p>This document is Part 2 of FAPI Security Profile 1.0 that specifies an advanced security profile of OAuth that is suitable to be used for protecting APIs with high inherent risk. Examples include APIs that give access to highly sensitive data or that can be used to trigger financial transactions (e.g., payment initiation).</p>\n</blockquote>\n<h1>5. Advanced security profile</h1>\n<h2>5.1. Introduction</h2>\n<blockquote>\n<p>For example, read and write access to a bank API has a higher financial risk than read-only access. As such, the security profiles of the authorization framework protecting these APIs are also different.</p>\n</blockquote>\n<blockquote>\n<p>This profile does not support public clients.</p>\n</blockquote>\n<blockquote>\n<p>Implementations can leverage OpenID Connect's Hybrid Flow that returns an ID Token in the authorization response or they can utilize the JWT Secured Authorization Response Mode for OAuth 2.0 (JARM) that returns and protects all authorization response parameters in a JWT.</p>\n</blockquote>","frontmatter":{"title":"Financial-grade API Security Profile 1.0 - Part 2: Advanced","language":"en-US","coverPath":null,"status":"Read","date":"2022-07-07"}}},{"node":{"html":"<h1>Introduction</h1>\n<blockquote>\n<p>The Financial-grade API is a highly secured OAuth profile that aims to provide specific implementation guidelines for security and interoperability.</p>\n</blockquote>\n<blockquote>\n<p>Among other security enhancements, this specification provides a secure alternative to screen scraping.</p>\n</blockquote>\n<blockquote>\n<p>Importantly, this profile does not provide non-repudiation (signing of authorization requests and responses) and sender-constrained access tokens.</p>\n</blockquote>\n<h1>5. Baseline security profile</h1>\n<h2>5.1. Introduction</h2>\n<blockquote>\n<p>The OIDF Financial-grade API (FAPI) security profile specifies security requirements for API resources protected by the OAuth 2.0 Authorization Framework</p>\n</blockquote>\n<blockquote>\n<p>FAPI Security Profile 1.0 - Part 1: Baseline and Part 2: Advanced specify different levels of security.</p>\n</blockquote>\n<h2>5.2.2</h2>","frontmatter":{"title":"Financial-grade API Security Profile 1.0 - Part 1: Baseline","language":"en-US","coverPath":null,"status":"Read","date":"2022-07-07"}}},{"node":{"html":"<h1>2 Kubernetes principles of operation</h1>\n<blockquote>\n<p>At the highest level, Kubernetes is two things:</p>\n<ul>\n<li>A cluster for running applications</li>\n<li>An orchestrator of cloud-native microservices apps</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>For production environments, multi-master high availability (HA) is a must have.</p>\n</blockquote>\n<blockquote>\n<p>Generally speaking, running 3 or 5 replicated masters in an HA (high availability) configuration is recommended.</p>\n</blockquote>\n<blockquote>\n<p>It's also considered a good practice not to run user applications on masters. This allows masters to concentrate entirely on managing the cluster.</p>\n</blockquote>\n<blockquote>\n<p>For an application to run on a Kubernetes cluster it needs to tick a few boxes. These include:</p>\n<ol>\n<li>Packaged as a container</li>\n<li>Wrapped in a Pod</li>\n<li>Deployed via a declarative manifest file</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>The declarative model and the concept of desired state are at the very heart of Kubernetes.</p>\n</blockquote>\n<blockquote>\n<p>containers must always run inside of Pods.</p>\n</blockquote>\n<blockquote>\n<p>The simplest model is to run a single container per POD. However, there are advanced use-cases that run multiple containers inside a single Pod.</p>\n</blockquote>\n<blockquote>\n<p>a Pod is a ring fenced environment to run containers. The pod itself doesn't actually run anything, it's just a sandbox for hosting containers.</p>\n</blockquote>\n<blockquote>\n<p>If you need to scale your app, you add or remove Pods.</p>\n</blockquote>\n<blockquote>\n<p>Services use labels and a label selector to know which set of pods to load balance traffic to. The service has a label selector that is a list of all the labels a Pod must posses in order for it to receive traffic from the Service.</p>\n</blockquote>\n<h2>Summary</h2>\n<p>K8 cluster is a bunch of nodes and a control plane. The control plane exposes an HTTP RESTful API, it assign work to nodes through the scheduler and records states in a persistent store. Nodes are responsible for running the applications.</p>\n<p><strong>Masters:</strong> Schedule decisions, perform monitoring, implement changes, respond to events, and more.</p>\n<ul>\n<li>\n<p>API Server: The grand Central Station of k8. </p>\n<ul>\n<li>All communication goes through here.</li>\n<li>Exposes the RESTful API</li>\n</ul>\n</li>\n<li>\n<p>Cluster Store: The heart of K8.</p>\n<ul>\n<li>The only stateful part of the control plane.</li>\n<li>Cluster store based on etcd.</li>\n<li>3-5 replicas of etcd for HA.</li>\n</ul>\n</li>\n<li>\n<p>Controller manager: Controller of Controllers.</p>\n<ul>\n<li>Applies control loops to ensure desired and current state are the same. Reconcile if they arent.</li>\n<li>Each control loop is very specialized.</li>\n</ul>\n</li>\n<li>\n<p>Scheduler: Watches for new work and assign it to workers.</p>\n<ul>\n<li>Applies Filter and ranking logic to the nodes</li>\n</ul>\n</li>\n<li>Cloud Controller manager: Integrates with cloud technologies</li>\n</ul>\n<p><strong>Nodes:</strong> Where applications runs.</p>\n<ol>\n<li>Watch the API Server for new work assignment</li>\n<li>Execute new work assignments</li>\n<li>Report back to the control plane</li>\n<li>\n<p>kubelet: Runs on every node in the cluster.</p>\n<ul>\n<li>Watches the API Server</li>\n</ul>\n</li>\n<li>\n<p>container runtime: Container related tasks</p>\n<ul>\n<li>Pull images, start and stop containers and so on ...</li>\n<li>Container Runtime Interface</li>\n<li>containerd is one of the popular options. It's the docker container runtime, which was donated by docker to cncf.</li>\n</ul>\n</li>\n<li>kub-proxy: Responsible for local networking.</li>\n</ol>\n<p>K8 clusters have an internal DNS. The DNS service has a static IP address that is hard-coded into every Pod on the cluster.</p>\n<p><strong>Pods</strong>: Atomic unit of K8.</p>\n<p><strong>Deployments:</strong> Higher level K8 object that wraps around a Pod and adds functionality.</p>\n<p><strong>Services</strong>: Provide reliable networking for a set of Pods.</p>\n<ul>\n<li>Consists of a stable DNS name , IP address, and port. </li>\n<li>Load balancer for pods</li>\n</ul>\n<h1>4 Working with Pods</h1>\n<blockquote>\n<p>The atomic unit of scheduling in the virtualization world is the Virtual Machine. This means <strong>deploying applications</strong> in the virtualization world means scheduling them on VMs.</p>\n</blockquote>\n<blockquote>\n<p>In the Docker world, the atomic unit is the container. This means <strong>deploying applications</strong> on Docker means deploying them inside of containers.</p>\n</blockquote>\n<blockquote>\n<p>In the Kubernetes world, the atomic unit is the Pod. Ergo, <strong>deploying applications</strong> on kubernetes means stamping them out in Pods.</p>\n</blockquote>\n<blockquote>\n<p>(About Pod) they're bigger than a container, but a lot smaller than a VM.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> Pod is a shared execution environment for one or more containers.</p>\n</blockquote>\n<blockquote>\n<p>By running the web server container and the file-sync container in the same Pod, we ensure they will always be deployed to the same code.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> <em>Shared execution environment</em> means that the Pod has a set of resources (IP, ports, hostname, memory, volumes, ...) that are shared by every container that is part of the Pod.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> a Pod is actually a special type of container called a pause container.</p>\n</blockquote>\n<blockquote>\n<p>the Pod (pause container) is just a collection of system resources that containers running inside of it will inherit and share. These system resources are kernel namespaces and include:</p>\n<ul>\n<li>Network namespace: IP address, port range, routing table, ...</li>\n<li>UTS namespace: Hostname</li>\n<li>IPC namespace: Unix domain sockets</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>This networking model makes <em>inter-Pod</em> communication really simple. Every Pod in the cluster has its own IP addresses that's fully routable on the Pod network.</p>\n</blockquote>\n<blockquote>\n<p> Because every Pod gets its own routable IP, every Pod on the Pod network can talk directly to every other Pod without messing around with things like nasty port mappings.</p>\n</blockquote>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> Control Groups (cgroups) prevent individual containers from consuming all of the available CPU, RAM and IOPS on a node.</p>\n</blockquote>\n<blockquote>\n<p>We could say that cgroups actively police resource usage.</p>\n</blockquote>\n<blockquote>\n<p>Individual containers have their own cgroup limits.</p>\n</blockquote>\n<blockquote>\n<p>Deploying a Pod is an atomic operation.</p>\n</blockquote>\n<blockquote>\n<p>Pods that are deployed via Pod manifest files are singletons - they are not replicated and have no self-healing capabilities. For this reason, we almost always deploy Pods via higher-level objects like Deployments and DaemonSets, as these can reschedule Pods when they fail.</p>\n</blockquote>\n<blockquote>\n<p>namespaces allow us to logically divide clusters for management purposes.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Deploying applications:</p>\n<ul>\n<li>VM -> scheduling a deploy in a VM</li>\n<li>Docker -> deploying a container</li>\n<li>Kubernetes -> deploying a Pod</li>\n</ul>\n<p><strong>What's a POD?</strong></p>\n<p>It's a shared execution environment for containers. It defines the system resources like IP, port ranges, host name, ... the containers will inherit. </p>\n<p>A pod is bigger than a container but smaller than a VM. Within a POD you can run multiple containers, and the will share the POD resources. </p>\n<p>In the docker language, a POD is a pause container, a collection of system resources that containers within it will inherit. That's why the shared execution environment.</p>\n<p>Network:</p>\n<ul>\n<li>intra POD: you can use localhost and the port. Useful for multi containers.</li>\n<li>inter POD: PODs can easily talk with other PODs due the POD network. </li>\n</ul>\n<p>To deploy a Pod, send the POST manifest to the API Server. The control plane stores the intent. And the scheduler deploys to a healthy node with enough resources.</p>\n<p>A best practice is to handle PODs as stateles due their ephemeral existence. Avoid storing states or relying on PODs IPs.</p>\n<p>Pods don't self-heal, they don't scale, and they don't allow for easy updates or rollbacks.</p>\n<p><strong>Pod lifecycle</strong></p>\n<p>Pending -> while download image and starts all the containers.</p>\n<p>Running -> Once everything is running.</p>\n<p>Succeeded -> All tasks were completed.</p>\n<p>Failed -> When a POD cannot start or if they break when running.</p>\n<p>Threat Pods as mortal, or cattle (pets vs cattle analogy). Once they die, replace with a new one instead of bringing the old one back.</p>\n<p><strong>What's a CGroup?</strong></p>\n<p>A control group avoid containers to use all resources from a node.</p>\n<p>The CGroup is applyed at container level. Having the CGroup applyed at container level, enables in a multi container POD to stablish resource boundaries (cgroups) for each container.</p>\n<h1>5 Kubernetes Deployment</h1>\n<blockquote>\n<p>It's important to know that a single Deployment can only manage a single type of Pod.</p>\n</blockquote>\n<blockquote>\n<p>Deployment can manage multiples replicas of the same Pod.</p>\n</blockquote>\n<blockquote>\n<p>behind-the-scene, Deployments leverage another object called a ReplicaSet.</p>\n</blockquote>\n<blockquote>\n<p>it's best practice that we don't directly manage ReplicaSets</p>\n</blockquote>\n<blockquote>\n<p>Deployments use ReplicaSets to provide self-healing and scalability</p>\n</blockquote>\n<blockquote>\n<p><em>Desired state</em> is what you want. <em>Current state</em> is what you have. If the two match, everybody's happy.</p>\n</blockquote>\n<blockquote>\n<p>A <em>Declarative model</em> is a way of telling Kubernetes what our desired state is, without having to get into the detail of <em>how</em> to implement it.</p>\n</blockquote>\n<blockquote>\n<p>Kubernetes supports both models (Declarative and Imperative), but strongly prefers the declarative model.</p>\n</blockquote>\n<blockquote>\n<p>Kubernetes is constantly making sure <em>current state</em> matches <em>desired state</em>.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p><strong>What's a Deployment?</strong></p>\n<p>A Deployment is a kubernetes object that adds capabilities to the Pod.</p>\n<p>Capabilities:</p>\n<ul>\n<li>Zero downtime</li>\n<li>Liveness Checks</li>\n<li>Readiness Checks</li>\n<li>Rolling updates (Through Deployment)</li>\n<li>Rollbacks (Through Deployment)</li>\n<li>Self-healing (Through ReplicaSet)</li>\n<li>Scalability (Through ReplicaSet)</li>\n</ul>\n<p>A Deployment wraps a ReplicaSet, which wraps a Pod. It's a best practice to not manage ReplicaSet, and to use Deployments.</p>\n<p><strong>What are the k8 states?</strong></p>\n<p>K8 uses 2 states the desired state (what you want) and the current state (what you got).</p>\n<p><strong>What's the reconciliation loop?</strong></p>\n<p>The reconciliation loops validate whether desire and current state match and if they don't triggers an action to make they match.</p>\n<p>The reconciliation loops is used for both self-healing and scalability.</p>\n<p><strong>How Deployment deal with updates?</strong></p>\n<p>Deployment deal with updates by adding another ReplicaSet with the new configuration. So, while the new ReplicaSet spins up all the Pods both ReplicaSet will be active. As soon, the ReplicaSet with the updated information is ready, the old ReplicaSet starts decreasing it's Pods (But the ReplicaSet is not deleted). Therefore, updates have a zero downtime.</p>\n<p><strong>How Deployment deal with Rollbacks?</strong></p>\n<p>Because the Deployment has the ReplicaSet with previous configuration, whenever you want to rollback your application you can spin up an old ReplicaSet and wind down the current ReplicaSet.</p>\n<h1>6 Kubernetes Services</h1>\n<blockquote>\n<p><img src=\"/images/concept.png\" alt=\"new concept\"> Kubernetes Services give us the networking we <strong>can</strong> rely on.</p>\n</blockquote>\n<blockquote>\n<p>a Kubernetes <strong>Service</strong> is an object in the API that we define in a manifest and POST to the API server.</p>\n</blockquote>\n<blockquote>\n<p>every Service gets its own stable IP address, its own stable DNS name, and its own stable port.</p>\n</blockquote>\n<blockquote>\n<p>Services use labels to dynamically select the Pods in the cluster they will sen traffic to.</p>\n</blockquote>\n<blockquote>\n<p>Think of Services as having a static front-end and a dynamic back-end. The front-end consists of the IP, DNS name, and port and never changes. The back-end consists of the Pods, which are fluid and can be constantly changing.</p>\n</blockquote>\n<blockquote>\n<p>Pods and Services are loosely coupled via labels and labels selectors.</p>\n</blockquote>\n<blockquote>\n<p>It also provides simple load-balancing</p>\n</blockquote>\n<blockquote>\n<p>The logic behind the selection process is a Boolean And.</p>\n</blockquote>\n<blockquote>\n<p>Each Service that is created, automatically gets an associated <em>Endpoint</em> object.</p>\n</blockquote>\n<blockquote>\n<p>Kubernetes supports several types of Services.</p>\n</blockquote>\n<blockquote>\n<p>A ClusterIP Service has a stable IP address and port that is only accessible from inside the cluster.</p>\n</blockquote>\n<blockquote>\n<p>Kubernetes has another type of Service called a NodePort Service. This builds on top of ClusterIP and enables access from outside of the cluster.</p>\n</blockquote>\n<blockquote>\n<p>There are other types of Services, such as LoadBalancer Services. These integrate with load-balancers from your cloud provider such as AWS, Azure, and GCP.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p><strong>What's a Service?</strong></p>\n<p>A Service is a K8 object that enables trusted networking for a Deployment or a set of Pods.</p>\n<p>IP address, DNS and Ports together with label selection is what enables Services deliver reliable networking.  A service will route traffic to Pods in the Endpoint that match their Label Selection.</p>\n<p>A service contains:</p>\n<ul>\n<li>IP addresses</li>\n<li>DNS name</li>\n<li>Ports</li>\n<li>(is associated an) Endpoint</li>\n<li>(provides) LoadBalancing</li>\n</ul>\n<p><strong>How does Service matches Pods?</strong></p>\n<p>The Service looks for Pods that contains all labels specified in Service's Label Selector, which means if a pod have an identical set or a super set of the Service's Label Selector it will be matched and it can receive traffic. In the case of a Pod having a super set of the Service's Label selectot, the additional labels will be ignored.</p>\n<p><strong>What's an Endpoint?</strong></p>\n<p>A dynamic list of healthy object in the cluster that matches the Service's label selector.</p>\n<p><strong>How does ClusterIP Service works?</strong></p>\n<p>The ClusterIP is a type of Service that is used for accessing services within the cluster. The ClusterIP Service name, IP address and port are registered in the cluster DNS service. Therefore, all Pods and Objects within the cluster are able to resolve the Service name.</p>\n<p>It's important to notice that the IP of a ClusterIP is longlived, which means will remains the same while the cluster lives.</p>\n<p><strong>How does NodePort Service works?</strong></p>\n<p>NodePort builts on top of ClusterIP and it enables access to outside of the cluster. The NodePort Service, in addition to the Name, IP and port it also adds the NodePort to the DNS.</p>\n<p>The service enables access from within cluster calling the first three initial parameters. Or they could also hit any cluster node on the NodePort specified.</p>\n<p>When a external client hits one of the cluster nodes with the NodePort, this request is redirect to the Service that will determine one Pod from the Endpoint list and route the traffic to it.</p>\n<p><strong>How to use K8 Service Discovery?</strong></p>\n<p>There are two ways. Using the DNS (preferred) or Environment Variables.</p>\n<p>The DNS is preferable because everytime a new Service is created the DNS is watching to register them in the DNS.</p>\n<p>Using Env Variables is not the best scenario, because pods only receive the env variable when they are created. Therefore, services created after the Pod creation won't be registered.</p>\n<h1>7 Kubernetes storage</h1>\n<blockquote>\n<p>Kubernetes has a mature and feature-rich storage subsystem called the <em>persistent volume subsystem</em>.</p>\n</blockquote>\n<blockquote>\n<p>All storage on a Kubernetes cluster is called a <em>volume</em>.</p>\n</blockquote>\n<blockquote>\n<p>plugins will be based on the Container Storage Interface (CSI) which is an open-standard aimed at providing a clean interface for plugins.</p>\n</blockquote>\n<blockquote>\n<p>At a high-level, Persistent Volumes (PV) are how we map external storage onto the cluster, and Persistent Volume Claims (PVC) are like tickets that authorize applications (Pods) to use a PV.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>There are rules safeguarding access to a single volume from multiple Pods( more on this later)</li>\n<li>A single external storage volume can only be used by a single PV. For example, you cannot have a 50GB external volume that has two 25GB Kuberbetes PVs using half of it.</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>.spec.accessModes defines how the PV can be mounted. Three options exist:</p>\n<ul>\n<li>ReadWriteOnce(RWO)</li>\n<li>ReadWriteMany(RWM)</li>\n<li>ReadOnlyMany(ROM)</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>(Reclaim Policy) Two policies currently exist:</p>\n<ul>\n<li>Delete</li>\n<li>Retain</li>\n</ul>\n</blockquote>\n<h2>My Summary</h2>\n<p><strong>What's the Container Storage Interface(CSI)?</strong></p>\n<p>The CSI for short is an open-standard aimed to abstract k8 internal storage details easing storage plugin development.</p>\n<p>CSI is the middlware between the Storage providers and the K8 internal storage.</p>\n<p>The CSI is a standard aimed to improve interoperability between storage vendors. Which means, the storage vendor implements the CSI and their plugin works on multiple orchestrators like Kubernetes and Docker Swarm.</p>\n<p><strong>Which are the three main resources in the Persistent Volume Subsystem?</strong></p>\n<p>The Persistent Volume (PV) - the storage in K8 (which linked to an external storage through CSI).</p>\n<p>The Persistent Volume Claims (PVC) - authorization to use a PV or a Storage class.</p>\n<p>The Storage Class (SC) - Allows us to define different classes, or tiers, of storage. It creates the PV dynamically.</p>\n<p><strong>What's the behavior of the Access Modes?</strong></p>\n<p>RWO the PV can only be mounted/bound as read and write by a single PVC. Usually a Block Storage.</p>\n<p>RWM the PV can only be bound as read and write by a multiple PVC. Usually a File or Object Store.</p>\n<p>ROM the PV can only be bound as read only by a multiple PVC.</p>\n<p><strong>What's the behavior of the PV Reclaim policies?</strong></p>\n<p>The delete policy is the default behavior for PVs created via <em>storage classes</em>. This policy deletes the PV and associaes resource on the external storage system. It could result in data loss.</p>\n<p>The retain policy will keep the PV associated to the cluster, but it will be retained (others PVC won't be able to use it).</p>\n<h1>8 Other important Kubernetes stuff</h1>\n<h2>My Summary</h2>","frontmatter":{"title":"The Kubernetes Book - 2019 edition","language":"en-US","coverPath":null,"status":"Reading","date":"2022-07-06"}}},{"node":{"html":"<h1>8 Ingress</h1>\n<blockquote>\n<p>Ingress is all about accessing multiple web applications through a single LoadBalancer Service.</p>\n</blockquote>\n<blockquote>\n<p>Ingress fixes this by exposing multiple Services through a single cloud load-balancer.</p>\n</blockquote>\n<blockquote>\n<p>It creates a LoadBalancer Service, on port 80 or 443, and uses host-based and path-based routing to send traffic\nto the correct backend Service.</p>\n</blockquote>\n<blockquote>\n<p>The object spec defines rules that govern traffic routing, and the controller implements the rules.</p>\n</blockquote>\n<blockquote>\n<p>Once you have an Ingress controller, you deploy Ingress objects with rules that govern how traffic hitting the\nIngress is routed.</p>\n</blockquote>\n<blockquote>\n<p>On the topic of routing, Ingress operates at layer 7 of the OSI model, also known as the “application layer”. This\nmeans it has awareness of HTTP headers, and can inspect them and forward traffic based on hostnames and\npaths.</p>\n</blockquote>\n<blockquote>\n<p>Ingress exposes multiple ClusterIP Services through a single cloud load-balancer.</p>\n</blockquote>\n<blockquote>\n<p>The way Kubernetes knows which Ingress controller to use when you deploy an Ingress object is via Ingress\nclasses. You create Ingress classes, and then tag Ingress objects with a particular class.</p>\n</blockquote>","frontmatter":{"title":"The Kubernetes Book - 2021 edition","language":"en-US","coverPath":null,"status":"Reading","date":"2022-07-06"}}},{"node":{"html":"<h1>Part 1 First Steps</h1>\n<blockquote>\n<p>A pache Camel is an open source integration framework that aims to make integrating systems easier.</p>\n</blockquote>\n<h2>Chapter 1 Meeting Camel</h2>\n<blockquote>\n<p>At the core of the Camel framework is a routing engine—or more precisely, a routing-engine builder. It allows you to define your own routing rules, decide from which sources to accept messages, and determine how to process and send those messages to other destinations.</p>\n</blockquote>\n<blockquote>\n<p>One of the fundamental principles of Camel is that it makes no assumptions about the type of data you need to process.</p>\n</blockquote>\n<blockquote>\n<p>Camel isn’t an enterprise service bus (ESB), although some call Camel a lightweight ESB because of its support for routing, transformation, orchestration, monitoring, and so forth.</p>\n</blockquote>\n<blockquote>\n<p>The core feature of Camel is its routing and mediation engine. A routing engine selectively moves a message around, based on the route’s configuration.</p>\n</blockquote>\n<blockquote>\n<p>Camel was designed not to be a server or ESB but instead to be embedded in whatever runtime you choose.</p>\n</blockquote>\n<blockquote>\n<p><code>org.apache.camel.Message</code>—The fundamental entity containing the data being carried and routed in Camel.</p>\n</blockquote>\n<blockquote>\n<p><code>org.apache.camel.Exchange</code>—The Camel abstraction for an exchange of messages. This exchange of messages has an <em>in</em> message, and as a reply, an <em>out</em> message.</p>\n</blockquote>\n<blockquote>\n<p>Messages have a body (a payload), headers, and optional attachments,</p>\n</blockquote>\n<blockquote>\n<p>During routing, messages are contained in an exchange.</p>\n</blockquote>\n<blockquote>\n<p>An exchange in Camel is the message’s container during routing.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>InOnly - A one-way message (also known as an event message). For example, JMS messaging is often one-way messaging.</li>\n<li>InOut - A request-response message. For example, HTTP-based transports are often request-rep</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>The exchange is the same for the entire lifecycle of routing, but the messages can change, for instance, if messages are transformed from one format to another.</p>\n</blockquote>\n<blockquote>\n<p>(About CamelContext) You can think of it as Camel’s runtime system, which keeps all the pieces together.</p>\n</blockquote>\n<blockquote>\n<p>The simplest way to define a route is as a chain of processors.</p>\n</blockquote>\n<blockquote>\n<p>The processor is a core Camel concept that represents a node capable of using, creating, or modifying an incoming exchange.</p>\n</blockquote>\n<blockquote>\n<p>(Component) they’re associated with a name that’s used in a URI, and they act as a factory of endpoints.</p>\n</blockquote>\n<blockquote>\n<p>An endpoint is the Camel abstraction that models the end of a channel through which a system can send or receive messages.</p>\n</blockquote>\n<blockquote>\n<p>A producer is the Camel abstraction that refers to an entity capable of sending a message to an endpoint.</p>\n</blockquote>\n<blockquote>\n<p>A consumer is the service that receives messages produced by some external system, wraps them in an exchange, and sends them to be processed. Consumers are the source of the exchanges being routed in Camel.</p>\n</blockquote>\n<blockquote>\n<p>(Event Driven Consumer) It’s also referred to as an asynchronous receiver in the EIP world. An event-driven consumer listens on a particular messaging channel, such as a TCP/IP port, JMS queue, Twitter handle, Amazon SQS queue, WebSocket, and so on. It then waits for a client to send messages to it. When a message arrives, the consumer wakes up and takes the message for processing.</p>\n</blockquote>\n<blockquote>\n<p>The polling consumer is also known as a synchronous receiver in EIP lingo, because it won’t poll for more messages until it’s finished processing the\ncurrent message.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Apache Camel is an OSS that aims to ease integrations. It provides a routing engine that contains several tools to extract, process and load data. The data can be of any type.</p>\n<p>Camel has some core concepts:</p>\n<ul>\n<li>Message: An entity that contains data and is routed by camel. A message has a body, headers and optional parameters. It's contained in an exchange.</li>\n<li>Exchange: An entity that contains in and out messages. It also supports in only message. It's the same during the lifecycle of a route.</li>\n<li>CamelContext: The runtime system. Keeps everything together.</li>\n<li>Route: Chain of processors.</li>\n<li>Processor: It can use, modify or create an incoming exchange.</li>\n<li>Component: Endpoint's factory.</li>\n<li>Endpoint: End of a channel that can receive or send messages.</li>\n<li>Producer: Sends messages to an endpoint.</li>\n<li>Consumer: Receives a message from an external system and wrap it in an exchange. Then, sends the exchange to be processed. Plays the role of endpoint.</li>\n</ul>\n<h2>Chapter 2 Routing With Camel</h2>\n<blockquote>\n<p>routing is the process by which a message is taken from an input queue and, based on a set of conditions, sent to one of several output queues</p>\n</blockquote>\n<blockquote>\n<p>(about messaging systems) The input and output queues are unaware of the conditions in between them. The conditional logic is decoupled from the message consumer and producer.</p>\n</blockquote>\n<blockquote>\n<p>With an endpoint URI, you can identify the component you want to use and the way that component is configured</p>\n</blockquote>\n<blockquote>\n<p>Java Message Service (JMS) is a Java API that allows you to create, send, receive, and read messages.</p>\n</blockquote>\n<blockquote>\n<p>In JMS, message consumers and producers talk to one another through an intermediary—a JMS destination</p>\n</blockquote>\n<blockquote>\n<p>Queues are strictly point-to-point; each message has only one consumer.</p>\n</blockquote>\n<blockquote>\n<p>Topics operate on a publish/subscribe scheme; a single message may be delivered to many consumers if they’ve subscribed to the topic.</p>\n</blockquote>\n<blockquote>\n<p>(about <code>jms:queue:incomingOrders</code>) The jms prefix indicates that you’re using the JMS component you configured before. By specifying queue, the JMS component knows to send to a queue named incomingOrders.</p>\n</blockquote>\n<blockquote>\n<p>The from method returns a RouteDefinition object, on which you can invoke various methods that implement EIPs and other messaging concepts.</p>\n</blockquote>\n<blockquote>\n<p>Another popular way of implementing internal DSLs is by using fluent interfaces (a.k.a. fluent builders). When using a fluent interface, you build up objects by chaining together method invocations. Methods of this type perform an operation and then return the current object instance.</p>\n</blockquote>\n<blockquote>\n<p>Each Java statement that starts with a from method in the RouteBuilder creates a new route.</p>\n</blockquote>\n<blockquote>\n<p>An IoC framework allows you to “wire” beans together to form applications. This wiring is typically done through an XML configuration file.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>The chapter starts speaking about routing in camel (the process to take a message as input and based on a set of conditions sent to one or several inputs that match conditions).</p>\n<p>Because, this chapter uses JMS and FTP, there's a quickly introduction on Messaging, but more specifically JMS and its queues and topics.</p>\n<p>Then the chapter introduces the Camel Route creation through Java DSL and XML DSL.</p>\n<p>In the XML DSL, there are several ways to import routes:</p>\n<ul>\n<li>Load the route <code>Bean</code> and pass the reference to <code>RouteBuilder</code></li>\n<li>Define the routes using XML DSL</li>\n<li>Use a <code>packageScan</code></li>\n<li>Tag the routes with <code>@Component</code> and use the Spring <code>ScanComponent</code></li>\n</ul>\n<p>It also explains how to create processors through the <code>Processor</code> interface.</p>\n<p>Then it shows the usage of camel with IoC and later with Spring.</p>\n<p>Use <code>toD</code> to dynamic endpoints. Together with the use of the Simple expression, for example <code>${headers.CamelFileName}</code>.</p>\n<p>Use <code>{{}}</code> to access Camel Properties. See the code to see how to setup.</p>\n<p>A lot of code in this chapter, it's a good idea to have a look at my implementation of the examples in the repository.</p>","frontmatter":{"title":"Camel In Action","language":"en-US","coverPath":"camel-in-action","status":"Reading","date":"2022-07-05"}}},{"node":{"html":"<h1>1. Introduction</h1>\n<blockquote>\n<p>This document describes an additional mechanism of client authentication utilizing mutual-TLS certificate-based authentication that provides better security characteristics than shared secrets.</p>\n</blockquote>\n<blockquote>\n<p>Mutual-TLS certificate-bound access tokens ensure that only the party in possession of the private key corresponding to the certificate can utilize the token to access the associated resources. </p>\n</blockquote>\n<blockquote>\n<p>Binding an access token to the client's certificate prevents the use of stolen access tokens or replay of access tokens by unauthorized parties.</p>\n</blockquote>\n<blockquote>\n<p>Mutual-TLS certificate-bound access tokens and mutual-TLS client authentication are distinct mechanisms that are complementary but don't necessarily need to be deployed or used together.</p>\n</blockquote>\n<h2>1.2 Terminology</h2>\n<blockquote>\n<p>Throughout this document the term \"mutual TLS\" refers to the process whereby, in addition to the normal TLS server authentication with a certificate, a client presents its X.509 certificate and proves possession of the corresponding private key to a server when negotiating a TLS session.</p>\n</blockquote>\n<h1>2. Mutual TLS for OAuth Client Authentication</h1>\n<blockquote>\n<p>For all requests to the authorization server utilizing mutual-TLS client authentication, the client include the client_id parameter</p>\n</blockquote>\n<blockquote>\n<p>In order to utilize TLS for OAuth client authentication, the TLS connection between the client and the authorization server have been established or re-established with mutual-TLS X.509 certificate authentication (i.e., the client Certificate and CertificateVerify messages are sent during the TLS handshake).</p>\n</blockquote>\n<blockquote>\n<p>The authorization server can locate the client configuration using the client identifier and check the certificate presented in the TLS handshake against the expected credentials for that client</p>\n</blockquote>\n<h2>2.1. PKI Mutual-TLS Method</h2>\n<blockquote>\n<p>It relies on a validated certificate chain and a single subject distinguished name (DN) or a single subject alternative name (SAN) to authenticate the client. Only one subject name value of any type is used for each client</p>\n</blockquote>\n<blockquote>\n<p>The TLS handshake is utilized to validate the client's possession of the private key corresponding to the public key in the certificate and to validate the corresponding certificate chain. The client is successfully authenticated if the subject information in the certificate matches the single expected subject configured or registered for that particular client</p>\n</blockquote>\n<h3>2.1.1 PKI Method Metadata Value</h3>\n<p>Adds the <code>tls_client_auth</code> HTTP Parameter.</p>\n<h3>2.1.2 Client Registration Metadata</h3>\n<blockquote>\n<p>A client using the tls<em>client</em>auth authentication method use exactly one of the below metadata parameters to indicate the certificate subject value that the authorization server is to expect when authenticating the respective client.</p>\n</blockquote>\n<p>During client registration:</p>\n<p><code>tls_client_auth_subject_dn</code> :  Subject distinguished name of the certificate</p>\n<p><code>tls_client_auth_san_dns</code>: dNSName SAN entry in the certificate</p>\n<p><code>tls_client_auth_san_uri</code>: uniformResourceIdentifier SAN entry in the certificate</p>\n<p><code>tls_client_auth_san_ip</code>: IP addres, either IPv4 or IPv6 y in the certificate</p>\n<p><code>tls_client_auth_san_email</code>: rfc822Name SAN entry in the certificate</p>\n<h2>2.2. Self-Signed Certificate Mutual-TLS Method</h2>","frontmatter":{"title":"0Auth 2.0 Mutual-TLS Client Authentication and Certificate-Bound Access Tokens","language":"en-US","coverPath":null,"status":"Reading","date":"2022-07-011"}}},{"node":{"html":"<h1>One - Fundamentals</h1>\n<h2>1.1 Basic Programming Model</h2>\n<blockquote>\n<p>The basis of our approach is the scientific method: we develop hypotheses about performance, create mathematical models, and run experiments to test them, repeating the process as necessary.</p>\n</blockquote>\n<blockquote>\n<p>The term algorithm is used in computer science to describe a finite, deterministic, and effective problem-solving method suitable for implementation as a computer program</p>\n</blockquote>\n<blockquote>\n<p>When developing a huge or complex computer program, a great deal of effort must go into understanding and defining the problem to be solved, managing its complexity, and decomposing it into smaller subtasks that can be implemented easily.</p>\n</blockquote>\n<blockquote>\n<p>We should not use an algorithm without having an idea of what resources it might consume, so we strive to be aware of how our algorithms might be expected to perform.</p>\n</blockquote>\n<blockquote>\n<p>There are three important rules of thumb in developing recursive programs:</p>\n<ul>\n<li>The recursion has a base case—we always include a conditional statement as the first statement in the program that has a return.</li>\n<li>Recursive calls must address subproblems that are smaller in some sense, so that recursive calls converge to the base case. In the code below, the difference between the values of the fourth and the third arguments always decreases.</li>\n<li>Recursive calls should not address subproblems that overlap. In the code below, the portions of the array referenced by the two subproblems are disjoint.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>The purpose of an API is to separate the client from the implementation: the client should know nothing about the implementation other than information given in the API, and the implementation should not take properties of any particular client into account. </p>\n</blockquote>\n<blockquote>\n<p>Accordingly, programmers normally think of the API as a contract between the client and the implementation that is a clear specification of what each method is to do. Our goal when developing an implementation is to honor the terms of the contract. </p>\n</blockquote>\n<h2>1.2 Data Abstraction</h2>\n<blockquote>\n<p>The primary difference is that we associate data with the function implementations and we hide the representation of the data from the client.</p>\n</blockquote>\n<blockquote>\n<p>Objects are characterized by three essential properties: state, identity, and behavior.</p>\n</blockquote>\n<blockquote>\n<p>Java nomenclature makes clear the distinction from primitive types (where variables are associated with values) by using the term reference types for nonprimitive types.</p>\n</blockquote>\n<blockquote>\n<p>This situation is known as aliasing: both variables refer to the same object.</p>\n</blockquote>\n<blockquote>\n<p>In other words, the convention is to pass the reference by value (make a copy of it) but to pass the object by reference.</p>\n</blockquote>\n<blockquote>\n<p>A data type is a set of values and a set of operations defined on those values. We implement data types in independent Java class modules and write client programs that use them</p>\n</blockquote>\n<blockquote>\n<p>An object is an entity that can take on a data-type value or an instance of a data type.</p>\n</blockquote>\n<blockquote>\n<p>Whenever you have data of different types that logically belong together, it is worthwhile to contemplate defining an ADT as in these examples.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Specify an API. The purpose of the API is to separate clients from implementations, to enable modular programming. We have two goals when specifying an API. First, we want to enable clear and correct client code. Indeed, it is a good idea to write some client code before finalizing the API to gain confidence that the specified data-type operations are the ones that clients need. Second, we want to be able to implement the operations. There is no point specifying operations that we have no idea how to implement.</li>\n<li>Implement a Java class that meets the API specifications. First we choose the instance variables, then we write constructors and the instance methods.</li>\n<li>Develop multiple test clients, to validate the design decisions made in the first two steps.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Encapsulation enables modular programming, allowing us to</p>\n<ul>\n<li>Independently develop of client and implementation code</li>\n<li>Substitute improved implementations without affecting clients</li>\n<li>Support programs not yet written (the API is a guide for any future client)</li>\n</ul>\n<p> Encapsulation also isolates data-type operations, which leads to the possibility of</p>\n<ul>\n<li>Limiting the potential for error</li>\n<li>Adding consistency checks and other debugging tools in implementations</li>\n<li>Clarifying client code</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>We do so by insisting on the API being the only point of dependence between client and implementation</p>\n</blockquote>\n<blockquote>\n<p>You do not need to know how a data type is implemented in order to use it and you can assume that a client knows nothing but the API when implementing a data type</p>\n</blockquote>\n<blockquote>\n<p>Articulating an API might seem to be overkill when writing a small program, but you should consider writing every program as though you will need to reuse the code someday</p>\n</blockquote>\n<blockquote>\n<p>Within this broad outline, there are numerous pitfalls that every API design is susceptible to:</p>\n<ul>\n<li>An API may be too hard to implement, implying implementations that are difficult or impossible to develop.</li>\n<li>An API may be too hard to use, leading to client code that is more complicated than it would be without the API.</li>\n<li>An API may be too narrow, omitting methods that clients need.</li>\n<li>An API may be too wide, including a large number of methods not needed by any client. This pitfall is perhaps the most common, and one of the most difficult to avoid. The size of an API tends to grow over time because it is not difficult to add methods to an existing API, but it is difficult to remove methods without breaking existing clients.</li>\n<li>An API may be too general, providing no useful abstractions.</li>\n<li>An API may be too specific, providing abstractions so detailed or so diffuse as to be useless.</li>\n<li>An API may be too dependent on a particular representation, therefore not serving the purpose of freeing client code from the details of using that representation. This pitfall is also difficult to avoid, because the representation is certainly central to the development of the implementation.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Generally, immutable types are easier to use and harder to misuse than mutable types because the scope of code that can change their values is far smaller.</p>\n</blockquote>\n<blockquote>\n<p>The downside of immutability is that a new object must be created for every value</p>\n</blockquote>\n<blockquote>\n<p>Another downside of immutability stems from the fact that, unfortunately, final guarantees immutability only when instance variables are primitive types, not reference types</p>\n</blockquote>\n<blockquote>\n<p>Assertions are for debugging: your program should not rely on assertions for normal operation since they may be disabled</p>\n</blockquote>\n<blockquote>\n<p>The designer of a data type expresses a precondition (the condition that the client promises to satisfy when calling a method), a postcondition (the condition that the implementation promises to achieve when returning from a method), and side effects (any other change in state that the method could cause). During development, these conditions can be tested with assertions. </p>\n</blockquote>\n<blockquote>\n<p>Subtyping makes modular programming more difficult for two reasons. First, any change in the superclass affects all subclasses. The subclass cannot be developed independently of the superclass; indeed, it is completely dependent on the superclass. This problem is known as the fragile base class problem.</p>\n</blockquote>\n<h2>1.3 Bags, Queues and Stacks</h2>\n<h3>Bags</h3>\n<blockquote>\n<p>A bag is a collection where removing items is not supported—its purpose is to provide clients with the ability to collect items and then to iterate through the collected items (the client can also test if a bag is empty and find its number of items).</p>\n</blockquote>\n<blockquote>\n<p>The order of iteration is unspecified and should be immaterial to the client.</p>\n</blockquote>\n<h3>FIFO queues</h3>\n<blockquote>\n<p>A FIFO queue (or just a queue) is a collection that is based on the firstin-first-out (FIFO) policy</p>\n</blockquote>\n<blockquote>\n<p>A typical reason to use a queue in an application is to save items in a collection while at the same time preserving their relative order : they come out in the same order in which they were put in.</p>\n</blockquote>\n<h3>Pushdown stacks</h3>\n<blockquote>\n<p>A pushdown stack (or just a stack) is a collection that is based on the last-in-first-out (LIFO) policy.</p>\n</blockquote>\n<h3>Loitering</h3>\n<blockquote>\n<p>Java’s garbage collection policy is to reclaim the memory associated with any objects that can no longer be accessed.</p>\n</blockquote>\n<blockquote>\n<p>This condition (holding a reference to an item that is no longer needed) is known as loitering</p>\n</blockquote>\n<blockquote>\n<p>performance goals for any collection implementation:</p>\n<ul>\n<li>Each operation should require time independent of the collection size.</li>\n<li>The space used should always be within a constant factor of the collection size.</li>\n</ul>\n</blockquote>\n<h3>Linked lists</h3>\n<blockquote>\n<p>A linked list is a recursive data structure that is either empty (null) or a reference to a node having a generic item and a reference to a linked list. </p>\n</blockquote>\n<blockquote>\n<p>The node in this definition is an abstract entity that might hold any kind of data, in addition to the node reference that characterizes its role in building linked lists.</p>\n</blockquote>\n<blockquote>\n<p>To emphasize that we are just using the Node class to structure the data, we define no methods and we refer directly to the instance variables in code: if first is a variable associated with an object of type Node, we can refer to the instance variables with the code first.item and first.next. Classes of this kind are sometimes called records.</p>\n</blockquote>\n<blockquote>\n<p>The standard solution to enable arbitrary insertions and deletions is to use a doubly-linked list, where each node has two links, one in each direction.</p>\n</blockquote>\n<blockquote>\n<p>(To go through the linked list elements) This process is known as traversing the list</p>\n</blockquote>\n<blockquote>\n<p>This implementation uses the same data structure as does Stack—a linked list—but it implements different algorithms for adding and removing items, which make the difference between LIFO and FIFO for the client. </p>\n</blockquote>\n<blockquote>\n<p>Linked lists are a fundamental alternative to arrays for structuring a collection of data</p>\n</blockquote>\n<blockquote>\n<p>These two alternatives, often referred to as sequential allocation and linked allocation, are fundamental.</p>\n</blockquote>\n<h2>1.4 Analysis of Algorithms</h2>\n<blockquote>\n<p>One of the key tenets of the scientific method is that the experiments we design must be reproducible, so that others can convince themselves of the validity of the hypothesis. Hypotheses must also be falsifiable, so that we can know for sure when a given hypothesis is wrong (and thus needs revision). As Einstein famously is reported to have said (“No amount of experimentation can ever prove me right; a single experiment can prove me wrong”), we can never know for sure that any hypothesis is absolutely correct; we can only validate that it is consistent with our observations.</p>\n</blockquote>\n<blockquote>\n<p>Our first qualitative observation about most programs is that there is a problem size that characterizes the difficulty of the computational task</p>\n</blockquote>\n<blockquote>\n<p>Intuitively, the running time should increase with problem size, but the question of by how much it increases naturally comes up every time we develop and run a program</p>\n</blockquote>\n<blockquote>\n<p>Knuth’s basic insight is simple: the total running time of a program is determined by two primary factors:</p>\n<ul>\n<li>The cost of executing each statement</li>\n<li>The frequency of execution of each statement</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>To allow us to ignore insignificant terms and therefore substantially simplify the mathematical formulas that we work with, we often use a mathematical device known as the tilde notation (~).</p>\n</blockquote>\n<blockquote>\n<p>This notation allows us to work with tilde approximations, where we throw away low-order terms that complicate formulas and represent a negligible contribution to values of interest</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>description</th>\n<th>function</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>constant</td>\n<td>1</td>\n</tr>\n<tr>\n<td>logarithmic</td>\n<td>log N</td>\n</tr>\n<tr>\n<td>linear</td>\n<td>N</td>\n</tr>\n<tr>\n<td>linearithmic</td>\n<td>N log N</td>\n</tr>\n<tr>\n<td>quadratic</td>\n<td>N^2</td>\n</tr>\n<tr>\n<td>cubic</td>\n<td>N^3</td>\n</tr>\n<tr>\n<td>exponential</td>\n<td>2^N</td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>A key observation from this exercise is to note that only the instructions that are executed the most frequently play a role in the final total—we refer to these instructions as the inner loop of the program.</p>\n</blockquote>\n<blockquote>\n<p>The algorithm that you are using (and sometimes the input model) determines the order of growth. Separating the algorithm from the implementation on a particular computer is a powerful concept because it allows us to develop knowledge about the performance of algorithms and then apply that knowledge to any computer.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Develop an input model, including a definition of the problem size.</li>\n<li>Identify the inner loop.</li>\n<li>Define a cost model that includes operations in the inner loop.</li>\n<li>Determine the frequency of execution of those operations for the given input</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>As it turns out, several important problems have natural solutions that are quadratic but clever algorithms that are linearithmic</p>\n</blockquote>\n<blockquote>\n<p>Typical computer systems are extremely complex and close analysis is best left for experts, but the same methods are effective for developing approximate estimates of the running time of any program.</p>\n</blockquote>\n<blockquote>\n<p>These guarantees are not absolute, but the chance that they are invalid is less than the chance your computer will be struck by lightning. Thus, such guarantees are as useful in practice as worst-case guarantees.</p>\n</blockquote>\n<blockquote>\n<p>when your program calls a method, the system allocates the memory needed for the method (for its local variables) from a special area of memory called the stack (a system pushdown stack), and when the method returns to the caller, the memory is returned to the stack</p>\n</blockquote>\n<blockquote>\n<p>When you create an object with new, the system allocates the memory needed for the object from another special area of memory known as the heap (not the same as the binary heap data structure we consider in Section 2.4), and you must remember that every object lives until no references to it remain, at which point a system process known as garbage collection reclaims its memory for the heap</p>\n</blockquote>\n<blockquote>\n<p>Perhaps the most common mistake made in programming is to pay too much attention to performance characteristics. Your first priority is to make your code clear and correct</p>\n</blockquote>\n<h2>1.5 Case Study: Union-Find</h2>\n<blockquote>\n<p>The size of a tree is its number of nodes. The depth of a node in a tree is the number of links on the path from it to the root. The height of a tree is the maximum depth among its nodes. </p>\n</blockquote>\n<h1>Two - Sorting</h1>\n<h2>2.1 Elementary Sorts</h2>\n<blockquote>\n<p>Sorting cost model. When studying sorting algorithms, we count compares and exchanges. For algorithms that do not use exchanges, we count array accesses.</p>\n</blockquote>\n<blockquote>\n<p>The sorting algorithms divide into two basic types: those that sort in place and use no extra memory except perhaps for a small function call stack or a constant number of instance variables, and those that need enough extra memory to hold another copy of the array to be sorted.</p>\n</blockquote>\n<h3>Selection Sort</h3>\n<blockquote>\n<p><strong>Selection sort.</strong> One of the simplest sorting algorithms works as follows: First, find the smallest item in the array and exchange it with the first entry (itself if the first entry is already the smallest). Then, find the next smallest item and exchange it with the second entry. Continue in this way until the entire array is sorted. This method is called selection sort because it works by repeatedly selecting the smallest remaining item.</p>\n</blockquote>\n<blockquote>\n<p>For example, the person using the sort client might be surprised to realize that it takes about as long to run selection sort for an array that is already in order or for an array with all keys equal as it does for a randomly-ordered array!</p>\n</blockquote>\n<h3>Insertion Sort</h3>\n<blockquote>\n<p><strong>Insertion sort</strong> The algorithm that people often use to sort bridge hands is to consider the cards one at a time, inserting each into its proper place among those already considered (keeping them sorted).</p>\n</blockquote>\n<blockquote>\n<p>consider what happens when you use insertion sort on an array that is already sorted. Each item is immediately determined to be in its proper place in the array, and the total running time is linear. </p>\n</blockquote>\n<blockquote>\n<p>If the number of inversions in an array is less than a constant multiple of the array size, we say that the array is partially sorted</p>\n</blockquote>\n<blockquote>\n<p>Indeed, when the number of inversions is low, insertion sort is likely to be faster than any sorting method that we consider in this chapter.</p>\n</blockquote>\n<blockquote>\n<p>In summary, insertion sort is an excellent method for partially sorted arrays and is also a fine method for tiny arrays</p>\n</blockquote>\n<h3>Shell Sort</h3>\n<blockquote>\n<p><strong>Shellsort</strong> is a simple extension of insertion sort that gains speed by allowing exchanges of array entries that are far apart, to produce partially sorted arrays that can be efficiently sorted, eventually by insertion sort.</p>\n</blockquote>\n<blockquote>\n<p>The idea is to rearrange the array to give it the property that taking every <em>h</em>th entry (starting anywhere) yields a sorted subsequence. Such an array is  said to be h-sorted. Put another way, an h-sorted array is h independent sorted subsequences, interleaved together.</p>\n</blockquote>\n<blockquote>\n<p>achieving speedups that enable the solution of problems that could not otherwise be solved is one of the prime reasons to study algorithm performance and design.</p>\n</blockquote>\n<h2>2.2 Mergesort</h2>\n<blockquote>\n<p>combining two ordered arrays to make one larger ordered array.</p>\n</blockquote>\n<blockquote>\n<p>to sort an array, divide it into two halves, sort the two halves (recursively), and then merge the results.</p>\n</blockquote>\n<blockquote>\n<p>most attractive properties is that it guarantees to sort any array of N items in time proportional to N log N. Its prime disadvantage is that it uses extra space proportional to N.</p>\n</blockquote>\n<blockquote>\n<p>In the merge, there are four conditions:</p>\n<ul>\n<li>left half exhausted (take from the right),</li>\n<li>right half exhausted (take from the left),</li>\n<li>current key on right less than current key on left (take from the right),</li>\n<li>current key on right greater than or equal to current key on left (take from the left).</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>The tree has precisely n levels.</p>\n</blockquote>\n<blockquote>\n<p>If space is at a premium, we need to consider another method.</p>\n</blockquote>\n<blockquote>\n<p>When addressing a new problem, your best bet is to use the simplest implementation with which you are comfortable and then refine it if it becomes a bottleneck.</p>\n</blockquote>\n<h2>2.3 Quicksort</h2>\n<blockquote>\n<p>The quicksort algorithm’s desirable features are that it is in-place (uses only a small auxiliary stack) and that it requires time proportional to N log N on the average to sort an array of length N.</p>\n</blockquote>\n<blockquote>\n<p>Quicksort is complementary to mergesort: for mergesort, we break the array into two subarrays to be sorted and then combine the ordered subarrays to make the whole ordered array; for quicksort, we rearrange the array such that, when the two subarrays are sorted, the whole array is ordered.</p>\n</blockquote>\n<blockquote>\n<p>Ultimately, the efficiency of the sort depends on how well the partitioning divides the array, which in turn depends on the value of the partitioning item’s key.</p>\n</blockquote>\n<blockquote>\n<p>The best case for quicksort is when each partitioning stage divides the array exactly in half.</p>\n</blockquote>\n<blockquote>\n<p>it can be extremely inefficient if the partitions are unbalanced</p>\n</blockquote>\n<blockquote>\n<p>Quicksort is slower than insertion sort for tiny subarrays.</p>\n</blockquote>\n<blockquote>\n<p>( Improving quicksort) It turns out that most of the available improvement comes from choosing a sample of size 3 and then partitioning on the middle item</p>\n</blockquote>\n<blockquote>\n<p>It was a classical programming exercise popularized by E. W. Dijkstra as the Dutch National Flag problem, because it is like sorting an array with three possible key values, which might correspond to the three colors on the flag.</p>\n</blockquote>\n<blockquote>\n<p>A carefully tuned version of quicksort is likely to run significantly faster on most computers for most applications than will any other compare-based sorting method.</p>\n</blockquote>\n<h2>2.4 Priority Queues</h2>\n<blockquote>\n<p>(computer running several apps) This effect is typically achieved by assigning a priority to events associated with applications, then always choosing to process next the highest-priority event.</p>\n</blockquote>\n<blockquote>\n<p>An appropriate data type in such an environment supports two operations: remove the maximum and insert. Such a data type is called a priority queue.</p>\n</blockquote>\n<blockquote>\n<p>Some important applications of priority queues include simulation systems, where the keys correspond to event times, to be processed in chronological order; job scheduling, where the keys correspond to priorities indicating which tasks are to be performed first; and numerical computations, where the keys represent computational errors, indicating in which order we should deal with them</p>\n</blockquote>\n<blockquote>\n<p>We can use any priority queue as the basis for a sorting algorithm by inserting a sequence of items, then successively removing the smallest to get them out, in order.</p>\n</blockquote>\n<blockquote>\n<p>Using <strong>unordered sequences</strong> is the prototypical lazy approach to this problem, where we defer doing work until necessary (to find the maximum);</p>\n</blockquote>\n<blockquote>\n<p>Using <strong>ordered sequences</strong> is the prototypical eager approach to the problem, where we do as much work as we can up front (keep the list sorted on insertion) to make later operations efficient.</p>\n</blockquote>\n<blockquote>\n<p>For stacks and queues, we were able to develop implementations of all the operations that take constant time; for priority queues, all of the elementary implementations just discussed have the property that either the insert or the remove the maximum operation takes linear time in the worst case. The heap data structure that we consider next enables implementations where both operations are guaranteed to be fast.</p>\n</blockquote>\n<blockquote>\n<p>A binary tree is <strong>heap-ordered</strong> if the key in each node is larger than or equal to the keys in that node’s two children (if any).</p>\n</blockquote>\n<blockquote>\n<p>(<strong>Complete binary tree</strong>) We draw such a structure by placing the root node and then proceeding down the page and from left to right,\ndrawing and connecting two nodes beneath each node on the previous level until we have drawn N nodes.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>binary heap</strong> is a collection of keys arranged in a complete heap-ordered binary tree, represented in level order in an array (not using the first entry).</p>\n</blockquote>\n<blockquote>\n<p>In a heap, the parent of the node in position k is in position ⎣k /2⎦ and, conversely, the two children of the node in position k are in positions 2k and 2k + 1.</p>\n</blockquote>\n<blockquote>\n<p>Complete binary trees represented as arrays (heaps) are rigid structures, but they have just enough flexibility to allow us to implement efficient priority-queue operations.</p>\n</blockquote>\n<blockquote>\n<p>(Travel up or down to restore the heap order) We refer to this process as <strong>reheapifying</strong>, or <strong>restoring heap order</strong></p>\n</blockquote>\n<blockquote>\n<p>Where elementary implementations using an ordered array or an unordered array require linear time for one of the operations, a heap-based implementation provides a guarantee that both operations complete in logarithmic time.</p>\n</blockquote>\n<blockquote>\n<p>(Multiway heaps) There is a tradeoff between the lower cost from the reduced tree height (log d N) and the higher cost of finding the largest of the <strong>d</strong> children at each node</p>\n</blockquote>\n<blockquote>\n<p>Immutability of keys. The priority queue contains objects that are created by clients but assumes that client code does not change the keys (which might invalidate the heap-order invariant)</p>\n</blockquote>\n<h3>Heapsort</h3>\n<blockquote>\n<p>We can use any priority queue to develop a sorting method. We insert all the items to be sorted into a minimum-oriented priority queue, then repeatedly use remove the minimum to remove them all in order.</p>\n</blockquote>\n<blockquote>\n<p>Using a priority queue represented as an unordered array in this way corresponds to doing a selection sort; using an ordered array corresponds to doing an insertion sort.</p>\n</blockquote>\n<blockquote>\n<p>Heapsort breaks into two phases: heap construction, where we reorganize the original array into a heap, and the sortdown, where we pull the items out of the heap in decreasing order to build the sorted result.</p>\n</blockquote>\n<blockquote>\n<p>This process is a bit like selection sort (taking the items in decreasing order instead of in increasing order), but it uses many fewer compares because the heap provides a much more efficient way to find the largest item in the unsorted part of the array.</p>\n</blockquote>\n<blockquote>\n<p>Although the loops in this program seem to do different tasks (the first constructs the heap, and the second destroys the heap for the sortdown), they are both built around the sink() method.</p>\n</blockquote>\n<blockquote>\n<p>Heapsort is significant in the study of the complexity of sorting (see page 279) because it is the only method that we have seen that is optimal (within a constant factor) in its use of both time and space—it is guaranteed to use ~2N lg N compares and constant extra space in the worst case.</p>\n</blockquote>\n<h2>2.5 Applications</h2>\n<blockquote>\n<p>It stands to reason that an array might not remain sorted if a client is allowed to change the values of keys after the sort.</p>\n</blockquote>\n<blockquote>\n<p>The reference approach makes the cost of an exchange roughly equal to the cost of a compare for general situations involving arbitrarily large items (at the cost of the extra space for the references).</p>\n</blockquote>\n<blockquote>\n<p>The Java Comparator interface allows us to build multiple orders within a single class. It has a single public method compare() that compares two objects.</p>\n</blockquote>\n<blockquote>\n<p>A sorting method is stable if it preserves the relative order of equal keys in the array. ... To begin, suppose that we store events in an array as they arrive, so they are in order of the timestamp in the array. Now suppose that the application requires that the transactions be separated out by location for further processing. One easy way to do so is to sort the array by location. If the sort is unstable, the transactions for each city may not necessarily be in order by timestamp after the sort.</p>\n</blockquote>\n<blockquote>\n<p>Some of the sorting methods that we have considered in this chapter are stable (insertion sort and mergesort); many are not (selection sort, shellsort, quicksort, and heapsort).</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>algorithm</th>\n<th>stable?</th>\n<th>running time</th>\n<th>extra space</th>\n<th>notes</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>selection sort</td>\n<td>no</td>\n<td>N²</td>\n<td>1</td>\n<td></td>\n</tr>\n<tr>\n<td>insertion sort</td>\n<td>yes</td>\n<td>between N and N²</td>\n<td>1</td>\n<td>depends on order of items</td>\n</tr>\n<tr>\n<td>shellsort</td>\n<td>no</td>\n<td>N log N / N ^ 6/5</td>\n<td>1</td>\n<td></td>\n</tr>\n<tr>\n<td>quicksort</td>\n<td>no</td>\n<td>N log N</td>\n<td>lg N</td>\n<td>probabilistic guarantee</td>\n</tr>\n<tr>\n<td>3-way quicksort</td>\n<td>no</td>\n<td>between N and N log N</td>\n<td>lg N</td>\n<td>probabilistic, also depends on distribution of input keys</td>\n</tr>\n<tr>\n<td>mergesort</td>\n<td>yes</td>\n<td>N log N</td>\n<td>N</td>\n<td></td>\n</tr>\n<tr>\n<td>heapsort</td>\n<td>no</td>\n<td>N log N</td>\n<td>1</td>\n<td></td>\n</tr>\n</tbody>\n</table>\n<blockquote>\n<p>Perhaps the best way to interpret Property T is as saying that you certainly should seriously consider using quicksort in any sort application where running time is important.</p>\n</blockquote>\n<blockquote>\n<p>Java’s systems programmers have chosen to use quicksort (with 3-way partitioning) to implement the primitive-type methods, and mergesort for reference-type methods. The primary practical implications of these choices are, as just discussed, to trade speed and memory usage (for primitive types) for stability (for reference types).</p>\n</blockquote>\n<blockquote>\n<p>A reduction is a situation where an algorithm developed for one problem is used to solve another.</p>\n</blockquote>\n<blockquote>\n<p>(Duplicates) first sort the array, then make a pass through the sorted array, taking note of duplicate keys that appear consecutively in the ordered array.</p>\n</blockquote>\n<blockquote>\n<p>An effective alternative to TopM when you have the items in an array is to just smallest values in the array are in the first k array positions for all k less than the array length.</p>\n</blockquote>\n<blockquote>\n<p>(Data sorted) You will also see that the same scheme makes it easy to quickly handle many other kinds of queries. How many items are smaller than a given item? Which items fall within a given range?</p>\n</blockquote>\n<h1>Three - Searching</h1>\n<blockquote>\n<p> We use the term <strong>symbol table</strong> to describe an abstract mechanism where we save information (a value) that we can later search for and retrieve by specifying a key.</p>\n</blockquote>\n<blockquote>\n<p>Symbol tables are sometimes called dictionaries, by analogy with the time-honored system of providing definitions for words by listing them alphabetically in a reference book.</p>\n</blockquote>\n<h2>3.1 Symbol Tables</h2>\n<blockquote>\n<p>Symbol tables are sometimes called dictionaries, by analogy with the time-honored system of providing definitions for words by listing them alphabetically in a reference book.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>symbol table</strong> is a data structure for key-value pairs that supports two operations: insert (put) a new pair into the table and search for (get) the value associated with a given key.</p>\n</blockquote>\n<blockquote>\n<p>These conventions define the associative array abstraction, where you can think of a symbol table as being just like an array, where keys are indices and values are array entries.</p>\n</blockquote>\n<h3>Ordered Symbol tables</h3>\n<blockquote>\n<p>More important, in such implementations, we can think of the symbol table as keeping the keys in order and consider a significantly expanded API that defines numerous natural and useful operations involving relative key order.</p>\n</blockquote>\n<blockquote>\n<p>When studying symbol-table implementations, we count compares (equality tests or key comparisons).</p>\n</blockquote>\n<blockquote>\n<p>For basic (unordered) implementations, the order of the keys in the output of this test client is not specified (it depends on characteristics of the implementation);</p>\n</blockquote>\n<p>FrequencyCounter</p>\n<h1>Four - Graphs</h1>\n<h2>4.1 Undirected Graphs</h2>\n<blockquote>\n<p>A <strong>graph</strong> is a set of vertices and a collection of edges that each connect a pair of vertices.</p>\n</blockquote>\n<blockquote>\n<p>Our definition allows two simple anomalies:</p>\n<ul>\n<li>A self-loop is an edge that connects a vertex to itself.</li>\n<li>Two edges that connect the same pair of vertices are parallel.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>When there is an edge connecting two vertices, we say that the vertices are <em>adjacent</em> to one another and that the edge is <em>incident</em> to both vertices.</p>\n</blockquote>\n<blockquote>\n<p>The <strong>degree</strong> of a vertex is the number of edges incident to it.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>path</strong> in a graph is a sequence of vertices connected by edges</p>\n</blockquote>\n<blockquote>\n<p>A <strong>simple path</strong> is one with no repeated vertices.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>cycle</strong> is a path with at least one edge whose first and last vertices are the same.</p>\n</blockquote>\n<blockquote>\n<p>when we want to allow repeated vertices, we refer to general paths and cycles.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>graph is connected</strong> if there is a path from every vertex to every other vertex in the graph. A <strong>graph that is not connected</strong> consists of a set of connected components, which are maximal connected subgraphs.</p>\n</blockquote>\n<blockquote>\n<p>An <strong>acyclic</strong> graph is a graph with no cycles.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>tree</strong> is an acyclic connected graph.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>spanning tree</strong> of a connected graph is a subgraph that contains all of that graph’s vertices and is a single tree.</p>\n</blockquote>\n<blockquote>\n<p>A disjoint set of trees is called a <strong>forest</strong>.</p>\n</blockquote>\n<blockquote>\n<p>For example, a graph G with V vertices is a tree if and only if it satisfies any of the following five conditions:</p>\n<ul>\n<li>G has V1 edges and no cycles.</li>\n<li>G has V1 edges and is connected.</li>\n<li>G is connected, but removing any edge disconnects it.</li>\n<li>G is acyclic, but adding any edge creates a cycle.</li>\n<li>Exactly one simple path connects each pair of vertices in G.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>A <strong>sparse</strong> graph has relatively few of the possible edges present; a <strong>dense</strong> graph has relatively few of the possible edges missing.</p>\n</blockquote>\n<blockquote>\n<p>A <strong>bipartite</strong> graph is a graph whose vertices we can divide into two sets such that all edges connect a vertex in one set with a vertex in the other set.</p>\n</blockquote>\n<blockquote>\n<p>An adjacency matrix, where we maintain a V-by-V boolean array, with the entry in row v and column w defined to be true if there is an edge adjacent to both vertex v and vertex w in the graph, and to be false otherwise.</p>\n</blockquote>\n<blockquote>\n<p>An array of edges, using an Edge class representations with two instance variables of type int.</p>\n</blockquote>\n<blockquote>\n<p>An array of adjacency lists, where we maintain a vertex-indexed array of lists of the vertices adjacent to each vertex.</p>\n</blockquote>\n<blockquote>\n<p><strong>Tremaux exploration</strong>. To explore all passages in a maze:</p>\n<ul>\n<li>Take any unmarked passage, unrolling a string behind you.</li>\n<li>Mark all intersections and passages when you first visit them.</li>\n<li>Retrace steps (using the string) when approaching a marked intersection.</li>\n<li>Retrace steps when no unvisited options remain at an intersection encountered while retracing steps.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>To visit a vertex:</p>\n<ul>\n<li>Mark it as having been visited.</li>\n<li>Visit (recursively) all the vertices that are adjacent to it and that have not yet been marked.</li>\n</ul>\n<p>This method is called <strong>depth-first search (DFS)</strong></p>\n</blockquote>\n<blockquote>\n<p>in DFS of an undirected graph, we either do a recursive drawing with both edges call when we encounter an edge v-w (if w is not marked) or skip the edge (if w is marked)</p>\n</blockquote>\n<blockquote>\n<p><strong>Connectivity</strong>. Given a graph, support queries of the form Are two given vertices connected ? and How many connected components does the graph have ?</p>\n</blockquote>\n<blockquote>\n<p><strong>Single-source paths</strong>. Given a graph and a source vertex s, support queries of the form Is there a path from s to a given target vertex v? If so, find such a path.</p>\n</blockquote>\n<blockquote>\n<p><strong>Single-source shortest paths.</strong> Given a graph and a source vertex s, support queries of the form Is there a path from s to a given target vertex v? If so, find a shortest such path (one with a minimal number of edges).</p>\n</blockquote>\n<blockquote>\n<p>DFS is analogous to one person exploring a maze. BFS is analogous to a group of searchers exploring by fanning out in all directions, each unrolling his or her own ball of string. When more than one passage needs to be explored, we imagine that the searchers split up to expore all of them;</p>\n</blockquote>\n<blockquote>\n<p>We put the source vertex on the data structure, then perform the following steps until the data structure is empty:</p>\n<ul>\n<li>Take the next vertex v from the data structure and mark it.</li>\n<li>Put onto the data structure all unmarked vertices that are adjacent to v.</li>\n</ul>\n<p>The algorithms differ only in the rule used to take the next vertex from the data structure (least recently added for BFS, most recently added for DFS).</p>\n</blockquote>\n<blockquote>\n<p><strong>Cycle detection.</strong> Support this query: Is a given graph acylic ?</p>\n</blockquote>\n<blockquote>\n<p><strong>Two-colorability.</strong> Support this query: Can the vertices of a given graph be assigned one of two colors in such a way that no edge connects vertices of the same color ? which is equivalent to this question: Is the graph bipartite ?</p>\n</blockquote>\n<blockquote>\n<p><strong>Symbol graphs</strong>. Typical applications involve processing graphs defined in files or on web pages, using strings, not integer indices, to define and refer to vertices.</p>\n</blockquote>","frontmatter":{"title":"Algorithms","language":"en-US","coverPath":"algorithm","status":"Reading","date":"2022-05-09"}}},{"node":{"html":"<h1>Foreword</h1>\n<blockquote>\n<p> success in modern technical endeavors absolutely requires multiple perspectives and expertise to collaborate.</p>\n</blockquote>\n<h1>Imagine a World Where Dev and Ops Become DevOps</h1>\n<blockquote>\n<p>Imagine a world where product owners, Development, QA, IT Operations, and Infosec work together, not only to help each other, but also to ensure that the overall organization succeeds. By working toward a common goal, they enable the fast flow of planned work into production (e.g., performing tens, hundreds, or even thousands of code deploys per day), while achieving world-class stability, reliability, availability, and security.</p>\n</blockquote>\n<blockquote>\n<p>In almost every IT organization, there is an inherent conflict between Development and IT Operations which creates a downward spiral, resulting in ever-slower time to market for new products and features, reduced quality, increased outages, and, worst of all, an ever-increasing amount of technical debt.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>There is a world where we don't have to fight our whole organization to deliver value for your organization. A world where skillful people work together toward a common goal. This is DevOps.</p>","frontmatter":{"title":"The DevOPS Handbook: How to Create World-Class Agility, Reliability, and Security in Technology Organizations","language":"en-US","coverPath":"the-devops-handbook","status":"Reading","date":"2022-03-22"}}},{"node":{"html":"<h1>1 Google, Meet OKRs</h1>\n<blockquote>\n<p>It is a collaborative goal-setting protocol for companies, teams, and individuals</p>\n</blockquote>\n<blockquote>\n<p>Now, OKRs are not a silver bullet. They cannot substitute for sound judgment, strong leadership, or a creative workplace culture. But if those fundamentals are in place, OKRs can guide you to the mountaintop.</p>\n</blockquote>\n<blockquote>\n<p>A management methodology that helps to ensure that the company focuses efforts on the same important issues throughout the organization.</p>\n</blockquote>\n<blockquote>\n<p>An <em>OBJECTIVE</em> , I explained, is simply <em>WHAT</em> is to be achieved, no more and no less. By definition, objectives are significant, concrete, action oriented, and (ideally) inspirational. When properly designed and deployed, they’re a vaccine against fuzzy thinking—and fuzzy execution.</p>\n</blockquote>\n<blockquote>\n<p><em>KEY RESULTS</em> benchmark and monitor <em>HOW</em> we get to the objective. Effective KRs are specific and time-bound, aggressive yet realistic. Most of all, they are measurable and verifiable. ( As prize pupil Marissa Mayer would say, “It’s not a key result unless it has a number.”) You either meet a key result’s requirements or you don’t; there is no gray area, no room for doubt</p>\n</blockquote>\n<blockquote>\n<p>First, said Edwin Locke, “hard goals” drive performance more effectively than easy goals. Second, <em>specific</em> hard goals “produce a higher level of output” than vaguely worded ones.</p>\n</blockquote>\n<blockquote>\n<p>In business, alienation isn’t an abstract, philosophical problem; it saps the bottom line. More highly engaged work groups generate more profit and less attrition.</p>\n</blockquote>\n<blockquote>\n<p>At smaller start-ups, where people absolutely need to be pulling in the same direction, OKRs are a survival tool. In the tech sector, in particular, young companies must grow quickly to get funding before their capital runs dry. </p>\n</blockquote>\n<blockquote>\n<p>At medium-size, rapidly scaling organizations, OKRs are a shared language for execution. They clarify expectations</p>\n</blockquote>\n<blockquote>\n<p>In larger enterprises, OKRs are neon-lit road signs. They demolish silos and cultivate connections among far-flung contributors.</p>\n</blockquote>\n<blockquote>\n<p>Acute focus, open sharing, exacting measurement, a license to shoot for the moon—these are the hallmarks of modern goal science.</p>\n</blockquote>\n<blockquote>\n<p>Then come the four OKR “superpowers”: focus, align, track, and stretch.</p>\n</blockquote>\n<blockquote>\n<p>Superpower #1—Focus and Commit to Priorities <em>(chapters 4, 5, and 6):</em></p>\n<p>High-performance organizations home in on work that’s important, and are equally clear on what <em>doesn’t</em> matter. OKRs impel leaders to make hard choices. They’re a precision communication tool for departments, teams, and individual contributors. By dispelling confusion, OKRs give us the focus needed to win.</p>\n</blockquote>\n<blockquote>\n<p>Superpower #2—Align and Connect for Teamwork <em>(chapters 7, 8, and 9):</em></p>\n<p>With OKR transparency, everyone’s goals—from the CEO down—are openly shared. Individuals link their objectives to the company’s game plan, identify cross-dependencies, and coordinate with other teams. By connecting each contributor to the organization’s success, top-down alignment brings meaning to work. By deepening people’s sense of ownership, bottom-up OKRs foster engagement and innovation.</p>\n</blockquote>\n<blockquote>\n<p>Superpower #3—Track for Accountability <em>(chapters 10 and 11):</em></p>\n<p>OKRs are driven by data. They are animated by periodic check-ins, objective grading, and continuous reassessment—all in a spirit of no-judgment accountability. An endangered key result triggers action to get it back on track, or to revise or replace it if warranted.</p>\n</blockquote>\n<blockquote>\n<p>Superpower #4—Stretch for Amazing <em>(chapters 12, 13, and 14):</em></p>\n<p>OKRs motivate us to excel by doing more than we’d thought possible. By testing our limits and affording the freedom to fail, they release our most creative, ambitious selves.</p>\n</blockquote>\n<h1>2 The Father of OKRs</h1>\n<blockquote>\n<p>Now, the two key phrases . . . are objectives and the key result. And they match the two purposes. The objective is the direction: “We want to dominate the mid-range microcomputer component business.” That’s an objective. That’s where we’re going to go. Key results for this quarter: “Win ten new designs for the 8085” is one key result. It’s a milestone. The two are not the same. . . .</p>\n<p>The key result has to be measurable. But at the end you can look, and without any arguments: Did I do that or did I not do it? Yes? No? Simple. No judgments in it.</p>\n<p>Now, did we dominate the mid-range microcomputer business? That’s for us to argue in the years to come, but over the next quarter we’ll know whether we’ve won ten new designs or not.</p>\n</blockquote>\n<blockquote>\n<p>“activity trap”: “[S]tressing output is the key to increasing productivity, while looking to increase activity can result in just the opposite.”</p>\n</blockquote>\n<blockquote>\n<p>How can we define and measure output by knowledge workers? And what can be done to increase it?</p>\n</blockquote>\n<blockquote>\n<p><em>Less is more.</em> “A few extremely well-chosen objectives,” Grove wrote, “impart a clear message about what we say ‘yes’ to and what we say ‘no’ to.” A limit of three to five OKRs per cycle leads companies, teams, and individuals to choose what matters most. In general, each objective should be tied to five or fewer key results. (See chapter 4, “Superpower #1: Focus and Commit to Priorities.”)</p>\n</blockquote>\n<blockquote>\n<p><em>Set goals from the bottom up.</em> To promote engagement, teams and individuals should be encouraged to create roughly half of their own OKRs, in consultation with managers. When all goals are set top-down, motivation is corroded. (See chapter 7, “Superpower #2: Align and Connect for Teamwork.”)</p>\n</blockquote>\n<blockquote>\n<p><em>No dictating.</em> OKRs are a cooperative social contract to establish priorities and define how progress will be measured. Even after company objectives are closed to debate, their key results continue to be negotiated. Collective agreement is essential to maximum goal achievement. (See chapter 7, “Superpower #2: Align and Connect for Teamwork.”)</p>\n</blockquote>\n<blockquote>\n<p><em>Stay flexible.</em> If the climate has changed and an objective no longer seems practical or relevant as written, key results can be modified or even discarded mid-cycle. (See chapter 10, “Superpower #3: Track for Accountability.”)</p>\n</blockquote>\n<blockquote>\n<p><em>Dare to fail.</em> “Output will tend to be greater,” Grove wrote, “when everybody strives for a level of achievement beyond [their] immediate grasp. . . . Such goal-setting is extremely important if what you want is peak performance from yourself and your subordinates.” While certain operational objectives must be met in full, aspirational OKRs should be uncomfortable and possibly unattainable. “Stretched goals,” as Grove called them, push organizations to new heights. (See chapter 12, “Superpower #4: Stretch for Amazing.”)</p>\n</blockquote>\n<blockquote>\n<p><em>A tool, not a weapon.</em> The OKR system, Grove wrote, “is meant to pace a person—to put a stopwatch in his own hand so he can gauge his own performance. It is not a legal document upon which to base a performance review.” To encourage risk taking and prevent sandbagging, OKRs and bonuses are best kept separate. (See chapter 15, “Continuous Performance Management: OKRs and CFRs.”)</p>\n</blockquote>\n<blockquote>\n<p><em>Be patient; be resolute.</em> Every process requires trial and error. As Grove told his iOPEC students, Intel “stumbled a lot of times” after adopting OKRs: “We didn’t fully understand the principal purpose of it. And we are kind of doing better with it as time goes on.” An organization may need up to four or five quarterly cycles to fully embrace the system, and even more than that to build mature goal muscle.</p>\n</blockquote>\n<h1>3 Operation Crush: An Intel Story</h1>\n<blockquote>\n<p>\"...If you tell everybody to go to the center of Europe, and some start marching off to France, and some to Germany, and some to Italy, that’s no good—not if you want them all going to Switzerland. If the vectors point in different directions, they add up to zero. But if you get everybody pointing in the same direction, you maximize the results. ...\"</p>\n</blockquote>\n<blockquote>\n<p> You can tell people to clean up a mess, but should you be telling them which broom to use?</p>\n</blockquote>\n<blockquote>\n<p>A field engineer tells his general manager, “You turkeys don’t understand what’s happening in the market,” and within two weeks, the whole company is realigned, top to bottom. Everyone’s agreed: “The whistleblower is right. We’ve got to act differently.” It was terribly important that Don Buckout and Casey Powell felt they could speak their minds without retribution.</p>\n</blockquote>\n<blockquote>\n<p>“Bad companies,” Andy wrote, “are destroyed by crisis. Good companies survive them. Great companies are improved by them.”</p>\n</blockquote>\n<h1>4 Superpower #1: Focus and Commit to Priorities</h1>\n<blockquote>\n<p>Successful organizations <em>focus</em> on the handful of initiatives that can make a real difference, deferring less urgent ones.</p>\n</blockquote>\n<blockquote>\n<p>While paring back a list of goals is invariably a challenge, it is well worth the effort. As any seasoned leader will tell you, no one individual—or company—can “do it all.” With a select set of OKRs, we can highlight a few things—the vital things—that must get done, as planned and on time.</p>\n</blockquote>\n<blockquote>\n<p>OKRs require a public commitment by leadership, in word and deed</p>\n</blockquote>\n<blockquote>\n<p>Their people need more than milestones for motivation. They are thirsting for meaning, to understand how their goals relate to the mission.</p>\n</blockquote>\n<blockquote>\n<p>Objectives are the stuff of inspiration and far horizons. Key results are more earthbound and metric-driven</p>\n</blockquote>\n<blockquote>\n<p>If an objective is well framed, three to five KRs will usually be adequate to reach it</p>\n</blockquote>\n<blockquote>\n<p>each key result should be a challenge in its own right.</p>\n</blockquote>\n<blockquote>\n<p>The best practice may be a parallel, dual cadence, with short-horizon OKRs (for the here and now) supporting annual OKRs and longer-term strategies</p>\n</blockquote>\n<blockquote>\n<p>To safeguard quality while pushing for quantitative deliverables, one solution is to pair key results—to measure “both effect and counter-effect,”</p>\n</blockquote>\n<blockquote>\n<p>Key results should be succinct, specific, and measurable. . A mix of outputs and inputs is helpful. Finally, completion of all key results <em>must</em> result in attainment of the objective. If not, it’s not an OKR.</p>\n</blockquote>\n<blockquote>\n<p>We must realize—and act on the realization—that if we try to focus on everything, we focus on nothing.</p>\n</blockquote>\n<blockquote>\n<p>OKRs are neither a catchall wish list nor the sum of a team’s mundane tasks. They’re a set of stringently curated goals that merit special attention and will move people forward in the here and now.</p>\n</blockquote>\n<h1>5 Focus: The Remind Story</h1>\n<blockquote>\n<p>We learned the three watchwords for entrepreneurs:</p>\n<ul>\n<li>Solve a problem</li>\n<li>Build a simple product</li>\n<li>Talk to your users</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>In my view, you can only do one big thing at a time really well, and so you better know what that one thing is.</p>\n</blockquote>\n<blockquote>\n<p>Our objectives were clear and quantified, and we were teacher-obsessed from the start.</p>\n</blockquote>\n<h1>6 Commit: The Nuna Story</h1>\n<blockquote>\n<p>Alongside focus, commitment is a core element of our first superpower. In implementing OKRs, leaders must publicly commit to their objectives and stay steadfast.</p>\n</blockquote>\n<blockquote>\n<p>Until your executives are fully on board, you can’t expect contributors to follow suit—especially when a company’s OKRs are aspirational. The more challenging an objective, the more tempting it may be to abandon it.</p>\n</blockquote>\n<blockquote>\n<p>Our whole team needs sharper focus and clearer priorities, the prerequisites for deeper commitment. OKRs have forced a bunch of conversations in the company that otherwise would not have happened.</p>\n</blockquote>\n<h1>7 Superpower #2: Align and Connect for Teamwork</h1>\n<blockquote>\n<p>Transparency seeds collaboration. Say Employee A is struggling to reach a quarterly objective. Because she has publicly tracked her progress, colleagues can see she needs help. They jump in, posting comments and offering support. The work improves. Equally important, work relationships are deepened, even transformed.</p>\n</blockquote>\n<blockquote>\n<p>Once top-line objectives are set, the real work begins. As they shift from planning to execution, managers and contributors alike tie their day-to-day activities to the organization’s vision. The term for this linkage is <em>alignment,</em> and its value cannot be overstated. According to the <em>Harvard Business Review</em> , companies with highly aligned employees are more than twice as likely to be top performers.</p>\n</blockquote>\n<blockquote>\n<p>In moderation, cascading makes an operation more coherent. But when <em>all</em> objectives are cascaded, the process can degrade into a mechanical, color-by-numbers exercise, with four adverse effects:</p>\n<ul>\n<li><em>A loss of agility.</em> Even medium-size companies can have six or seven reporting levels. As everyone waits for the waterfall to trickle down from above, and meetings and reviews sprout like weeds, each goal cycle can take weeks or even months to administer. Tightly cascading organizations tend to resist fast and frequent goal setting. Implementation is so cumbersome that quarterly OKRs may prove impractical.</li>\n<li><em>A lack of flexibility.</em> Since it takes so much effort to formulate cascaded goals, people are reluctant to revise them mid-cycle. Even minor updates can burden those downstream, who are scrambling to keep their goals aligned. Over time, the system grows onerous to maintain.</li>\n<li><em>Marginalized contributors.</em> Rigidly cascaded systems tend to shut out input from frontline employees. In a top-down ecosystem, contributors will hesitate to share goal-related concerns or promising ideas.</li>\n<li><em>One-dimensional linkages.</em> While cascading locks in vertical alignment, it’s less effective in connecting peers horizontally, across departmental lines.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>A healthy OKR environment strikes a balance between alignment and autonomy, common purpose and creative latitude.</p>\n</blockquote>\n<blockquote>\n<p>Precisely because OKRs are transparent, they can be shared without cascading them in lockstep</p>\n</blockquote>\n<blockquote>\n<p>Rather than laddering down from the CEO to a VP to a director to a manager (and then to the manager’s reports), an objective might jump from the CEO straight to a manager, or from a director to an individual contributor. Or the company’s leadership might present its OKRs to everyone at once and trust people to say, “Okay, now I see where we’re going, and I’ll adapt my goals to that.”</p>\n</blockquote>\n<blockquote>\n<p>An optimal OKR system frees contributors to set at least some of their own objectives and most or all of their key results.</p>\n</blockquote>\n<blockquote>\n<p>In business, I have found, there is rarely a single right answer. By loosening the reins and backing people to find <em>their</em> right answers, we help everybody win.</p>\n</blockquote>\n<blockquote>\n<p> When leaders are attuned to the fluctuating needs of both the business and their employees, the mix of top-down and bottom-up goals generally settles at around half-and-half. Which sounds about right to me.</p>\n</blockquote>\n<blockquote>\n<p>When goals are public and visible to all, a “team of teams” can attack trouble spots wherever they surface.</p>\n</blockquote>\n<h1>8 Align: The MyFitnessPal Story</h1>\n<blockquote>\n<p>OKRs are not islands. To the contrary, they create networks—vertical, horizontal, diagonal—to connect an organization’s most vital work. When employees align with a company’s top-line goals, their impact is amplified. </p>\n</blockquote>\n<blockquote>\n<p>It’s not easy to predict the market for the conceptually new; we’d wildly beat our metric or wildly miss. So we switched it up. We began pinning our key results to deadlines instead of revenue or projected users. (Example: “Launch MFP Premium by 5/1/15.”) After a feature launched and some real data came back, we’d be in a stronger position to assess its impact and potential. Then our next round of OKRs could be more realistically keyed (or stretched) to projected outputs.</p>\n</blockquote>\n<blockquote>\n<p>“If we take this one off the road map this quarter, what happens? Would it really affect the user experience?” More often than not, the feature in question wouldn’t make a big difference. These calls are not subjective; we have metrics to measure impact. We’re making tougher, sharper choices about where to place our bets these days, and they all stem from the OKR process.</p>\n</blockquote>\n<blockquote>\n<p>Beyond making objectives more consistent within a company, alignment contains a deeper meaning. It’s about keeping your goals true to your North Star values. Connected Fitness is deliberately aligned with Under Armour’s mission “to make all athletes better.” At the same time, we still live by the old MyFitnessPal mantra: <em>When our customers succeed at reaching their health and fitness goals, we succeed as a company.</em></p>\n</blockquote>\n<h1>9 Connect: The Intuit Story</h1>\n<blockquote>\n<p>People can’t connect with what they cannot see; networks cannot blossom in silos. By definition, OKRs are open and visible to all parts of an organization, to each level of every department. As a result, companies that stick with them become more coherent.</p>\n</blockquote>\n<blockquote>\n<p>In IT, we’re always juggling the needs of internal partners with the demands of our end users. We bridge technology and business outcomes. Maybe toughest of all, we must balance the task of making systems work perfectly today (as our people expect) with our mandate to invest in the future.</p>\n</blockquote>\n<blockquote>\n<p>Now, with horizontally transparent OKRs, our engineers intentionally connect as they link to each other’s objectives. Quarter by quarter, they iterate against the department’s objectives while devising how best to coordinate with their peers.</p>\n</blockquote>\n<h1>10 Superpower #3: Track for Accountability</h1>","frontmatter":{"title":"Measure What Matters","language":"en-US","coverPath":"measure-what-matters","status":"Reading","date":"2022-03-14"}}},{"node":{"html":"<h1>Foreword</h1>\n<blockquote>\n<p>They want to improve. They know they need to improve. They usually understand the theory of how strong teams work. But they just don’t have the hands-on experience and knowledge to be able to provide the coaching their people need.</p>\n</blockquote>\n<h1>Part I What is Continuous Discovery</h1>\n<h2>Chapter One The What and Why of Continuous Discovery</h2>\n<blockquote>\n<p>I’ll refer to the work that you do to decide what to build as discovery and the work that you do to build and ship a product as delivery</p>\n</blockquote>\n<blockquote>\n<p>they focus on whether you shipped what you said you would on time and on budget—while under-investing in discovery, forgetting to assess if you built the right stuff.</p>\n</blockquote>\n<h3>The Evolution of Modern Product Discovery</h3>\n<blockquote>\n<p>In the early days of software, business leaders owned discovery—they decided what to build. Discovery happened once a year in an annual budgeting process, where projects with fixed timelines were assigned to specific engineering teams.</p>\n</blockquote>\n<blockquote>\n<p>The authors of the Agile manifesto advocated for shorter cycles with more frequent customer feedback. Second, they proposed working at a pace that could be sustained continuously, rather than furiously scurrying from one milestone to another. Third, they advocated for maximum flexibility—having the ability to adapt to customer feedback quickly and easily. And fourth, they advocated for simplicity.</p>\n</blockquote>\n<blockquote>\n<p>Leaders struggled to give up ownership of discovery. Even with shorter cycles and more customer feedback, business stakeholders still clung to their original ideas. Most teams weren’t very good at estimating unpredictable work (who is?), and their shorter cycles, aptly named sprints in Scrum, truly became biweekly sprints, killing any chance of finding a continuously sustainable pace. The rest of the business continued operating on an annual budgeting cycle, making true flexibility nearly impossible. When teams learned something wouldn’t work, they were still expected to deliver it on time and under budget. Usability testing was often done too late in the process, making it hard to address the substantial issues that were so often uncovered. User research was often outsourced to design agencies who did project-based research. And finally, teams continued to be measured by what they delivered, not whether anyone used it or if it created any value for the customer or the business.</p>\n</blockquote>\n<h3>Who This Book Is for</h3>\n<blockquote>\n<p>Throughout the book, the term “product trio” will refer to a product manager, a designer, and a software engineer working together to develop products for their customers.</p>\n</blockquote>\n<blockquote>\n<p>Each team needs to define the right “trio” on their team to adopt these habits.</p>\n</blockquote>\n<h3>The pre-requisite mindset</h3>\n<blockquote>\n<p>That means rather than defining your success by the code that you ship (your output), you define success as the value that code creates for your customers and for your business (the outcomes).</p>\n</blockquote>\n<blockquote>\n<p>We elevate customer needs to be on par with business needs and focus on creating customer value as well as business value.</p>\n</blockquote>\n<blockquote>\n<p>Rather than the product manager decides, the designer designs, and the engineer codes, we embrace a model where we make team decisions while leveraging the expertise and knowledge that we each bring to those decisions.</p>\n</blockquote>\n<blockquote>\n<p>The habits in this book will encourage you to draw, to externalize your thinking, and to map what you know.</p>\n</blockquote>\n<blockquote>\n<p>to do discovery well, we need to learn to think like scientists identifying assumptions and gathering evidence.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This first chapter introduces talks about the evolution of product / software development over time. Starting with a more project driven, based on the decisions of few, then going through the agile movement and companies trying to adapt but failing to change the way they work. And at last, with the evolution of product instrumentation companies finding the right spot and culture to achieve better results.</p>\n<p>The \"product trio\" is composed by a product manager, a designer and a software engineer. This is not a static structure each company needs to find the right fit for their product trio. </p>\n<p>There are companies failing to achieve the expected results, because they try to copy a framework or methodology without understanding the the whys behind the practices. It's required to cultivate 6 mindsets in order to achieve the continuous delivery:</p>\n<ol>\n<li>outcome-oriented</li>\n<li>customer-centric</li>\n<li>collaborative</li>\n<li>visual</li>\n<li>experimental</li>\n<li>continuous</li>\n</ol>\n<p>At last the author differentiates companies who sporadically uses modern discovery practices to companies who have achieve a continuous discovery, then it proposes the following definition:</p>\n<blockquote>\n<p>At a minimum, weekly touch points with customers</p>\n<p>By the team building the product</p>\n<p>Where they conduct small research activities</p>\n<p>In pursuit of a desired outcome</p>\n</blockquote>\n<h2>Chapter Two A Common Framework for Continuous Discovery</h2>\n<blockquote>\n<p>The company rightly started with a desired outcome: To increase the average number of accounts per customer. However, they didn’t pair this outcome mindset with a customer-centric mindset, that is critical for long-term product success.</p>\n</blockquote>\n<blockquote>\n<p>by arguing that serving customers is how we generate profit.</p>\n</blockquote>\n<h3>Begin With the End in Mind</h3>\n<blockquote>\n<p>Rather than obsessing about features (outputs), we are shifting our focus to the impact those features have on both our customers and our business (outcomes).</p>\n</blockquote>\n<blockquote>\n<p>When a product trio is tasked with delivering an outcome, the business is clearly communicating what value the team can create for the business. And when the business leaves it up to the team to explore the best outputs that might drive that outcome, they are giving the team the latitude they need to create value for the customer.</p>\n</blockquote>\n<h3>The Challenge of Driving Outcomes</h3>\n<blockquote>\n<p>So, when we shift from an output mindset to an outcome mindset, we have to relearn how to do our jobs.</p>\n</blockquote>\n<blockquote>\n<p>Ill-structured problems are defined by having many solutions. There are no right or wrong answers, only better or worse ones.</p>\n</blockquote>\n<blockquote>\n<p>I’ll refer to customer needs, pain points, and desires collectively as “opportunities”—they represent opportunities to intervene in our customers’ lives in a positive way.</p>\n</blockquote>\n<blockquote>\n<p>To reach their desired outcome, a product trio must discover and explore the opportunity space. The opportunity space, however, is infinite.</p>\n</blockquote>\n<blockquote>\n<p>two of the most important steps for reaching our desired outcome are first, how we map out and structure the opportunity space, and second, how we select which opportunities to pursue.</p>\n</blockquote>\n<blockquote>\n<p>We do have to get to solutions—shipping code is how we ship value to our customers and create value for our business. But the right problem framing will help to ensure that we explore and ultimately ship better solutions.</p>\n</blockquote>\n<h3>The Underlying Structure of Discovery</h3>\n<blockquote>\n<p>It starts with defining a clear outcome—one that sets the scope for discovery. From there, we must discover and map out the opportunity space—this is what gives structure to the ill-structured problem of reaching our desired outcome. It’s the all-important problem framing that opens up the solution space. And finally, we need to discover the solutions that will address those opportunities and thus drive our desired outcome.</p>\n</blockquote>\n<blockquote>\n<p>opportunity solution tree (OST).</p>\n</blockquote>\n<h3>OSTs Resolve the Tension Between Business Needs and Customer Needs</h3>\n<blockquote>\n<p>You start by prioritizing your business need—creating value for your business is what ensures that your team can serve your customer over time.</p>\n</blockquote>\n<blockquote>\n<p>Next, the team should explore the customer needs, pain points and desires that, if addressed, would drive that outcome.</p>\n</blockquote>\n<h3>OSTs Help Build and Maintain a Shared Understanding Across Your Trio</h3>\n<blockquote>\n<p>When a team takes the time to visualize their options, they build a shared understanding of how they might reach their desired outcomes.</p>\n</blockquote>\n<h3>OSTs Help Product Trios Adopt a Continuous Mindset</h3>\n<blockquote>\n<p>We tend to take our six-month-long waterfall project, carve it up into a series of two-week sprints, and call it \"Agile\".</p>\n</blockquote>\n<blockquote>\n<p>With time, as they address a series of smaller opportunities, these solutions start to address the bigger opportunity.</p>\n</blockquote>\n<h3>OSTs Unlock Better Decision-Making</h3>\n<blockquote>\n<p>When you bounce from tactic to tactic, it's easy to forget what you've learned and what decisions you need to make next.</p>\n</blockquote>\n<blockquote>\n<p>The first villain is looking too narrowly at a problem.</p>\n</blockquote>\n<blockquote>\n<p>The second villain is looking for evidence that confirms our beliefs.</p>\n</blockquote>\n<blockquote>\n<p>The third villain is letting our short-term emotions affect our decisions.</p>\n</blockquote>\n<blockquote>\n<p>The fourth villain is overconfidence.</p>\n</blockquote>\n<blockquote>\n<p>But one tactic we'll rely on over and over again throughout this book is their advice to avoid \"whether or not\" decisions.</p>\n</blockquote>\n<blockquote>\n<p>Good discovery doesn't prevent us from failing; it simply reduces the chance of failures.</p>\n</blockquote>\n<blockquote>\n<p>However, most of the decisions that we make in discovery are reversible decisions. If we do the necessary work to test our decisions, we can quickly correct course when we find that we made the wrong decision.</p>\n</blockquote>\n<h3>OSTs Unlock Faster Learning Cycles</h3>\n<blockquote>\n<p>As they explore potential solutions, they learn more about the problem, and, as they learn more about the problem, new solutions become possible.</p>\n</blockquote>\n<h3>OSTs Build Confidence in Knowing What to Do Next</h3>\n<blockquote>\n<p>The depth and breadth of the opportunity space reflects the team's current understanding of their target customer.</p>\n</blockquote>\n<h3>OSTs Unlock Simpler Stakeholder Management</h3>\n<blockquote>\n<p>As a result, it's not enough for a product trio to make evidence-based decisions about what to build; they also need to justify those decisions to key stakeholders along the way</p>\n</blockquote>\n<blockquote>\n<p>When sharing your discovery work with stakeholders, you can use your tree to first remind them of your desired outcome. Next, you can share what you've learned about your customer, by walking them through the opportunity space.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Chapter two starts talking about the conflict between businesses needs and customer needs and uses the Wells Fargo company which was fined $185 million by the Consumer Financial Protection Bureau, because they didn't balance the outcome mindset with the customer centric mindset. Then  proceeds arguing what generates profit is the customers being served correctly.</p>\n<p>You should focus on outcomes not outputs. You should focus on what's the impact that your deliveries causes. The outcome is a business responsibility and the product trio explore the best opportunities that will become outputs, which will drive the outcome.</p>\n<p>When shifting for a culture of outputs to outcome is common that you'll face difficulties. You should look for ill-structured problems, which is a problem with many solutions. There are no right or wrong, only better or worse. To find a ill structured problem, the most difficult part is to frame the problem itself, what we call opportunity spaces.</p>\n<p>Opportunities is a better way to refer to customer needs, pain points and desires. Problems infer that you need to fix something. so it doesn't encompass desires neither customer needs.</p>\n<p>Because the opportunity space is infinite structuring the opportunity space is so important as well as choosing the opportunities from the opportunity space.</p>\n<p>Discovery Structure:</p>\n<ol>\n<li>Define a clear outcome</li>\n<li>Discover and map the opportunity space (ill structured problem)</li>\n<li>Discover solutions that will address and this drive our desired outcome</li>\n</ol>\n<p>Opportunity solution tree is a visual representation tool to help to achieve your outcomes:</p>\n<ul>\n<li>Resolve the tension between business needs and customer needs</li>\n<li>Build and maintain a shared understanding of how they might reach their desired outcome</li>\n<li>\n<p>Adopt a continuous mindset</p>\n<ul>\n<li>Solving smaller opportunities eventually solves bigger opportunities</li>\n</ul>\n</li>\n<li>\n<p>Unlock better decision-making</p>\n<ul>\n<li>Avoid whether or not decisions. Use compare and contrast mindset.</li>\n<li>Avoid the analysis paralysis by doing reversible decisions in discovery.</li>\n</ul>\n</li>\n<li>\n<p>Unlock faster learning cycles</p>\n<ul>\n<li>The problem space and the solution space are intrinsically intertwined</li>\n</ul>\n</li>\n<li>Build confidence in knowing what to do next</li>\n<li>\n<p>Unlock simpler stakeholder management</p>\n<ul>\n<li>Use the OST to communicate learning and help to align with stakeholders.</li>\n</ul>\n</li>\n</ul>\n<h1>Part II The Continuous Discovery Habits</h1>\n<h2>Chapter Three Focusing on Outcomes Over Outputs</h2>\n<blockquote>\n<p>Teams tasked with a new outcome often have no idea how to measure that outcome, how to impact it, or even if it's the right outcome to be pursuing.</p>\n</blockquote>\n<blockquote>\n<p>Lagging indicators like 90-day retention make it hard to measure the impact of fast experiment cycles.</p>\n</blockquote>\n<blockquote>\n<p>Product teams often have to do some discovery work to identify the connections between product outcomes (the metrics they can influence) and business outcomes (the metrics that drive the business).</p>\n</blockquote>\n<h3>Why Outcomes?</h3>\n<blockquote>\n<p>When we manage by outcomes, we give our teams the autonomy, responsibility, and ownership to chart their own path.</p>\n</blockquote>\n<blockquote>\n<p>The key distinction with this strategy over traditional roadmaps is that we are giving the team autonomy to find the best solution.</p>\n</blockquote>\n<blockquote>\n<p>A fixed roadmap communicates false ceratinty</p>\n</blockquote>\n<blockquote>\n<p>We know we need this problem solved, but we don't know the best way to solve it.</p>\n</blockquote>\n<h3>Exploring Different Types of Outcomes</h3>\n<blockquote>\n<p>If we choose the wrong outcomes, we'll still get the wrong results</p>\n</blockquote>\n<blockquote>\n<p>A business outcome measures how well the business is progressing.</p>\n<p>A product outcome measures how well the product is moving the business forward.</p>\n<p>A traction metric measures usage of a specific feature or workflow in the product.</p>\n</blockquote>\n<blockquote>\n<p>(about lagging indicators) They measure something after it has happened.</p>\n</blockquote>\n<blockquote>\n<p>We want to identify leading indicators that predict the direction of the lagging indicator.</p>\n</blockquote>\n<blockquote>\n<p>As a general rule, product trios will make more progress on a product outcome rather than a business outcome.</p>\n</blockquote>\n<blockquote>\n<p>we can increase the accountability of each team by assigning a metric that is relevant to their own work.</p>\n</blockquote>\n<blockquote>\n<p>When multiple teams are assigned the same outcome, it's easy to shift blame for lack of progress.</p>\n</blockquote>\n<blockquote>\n<p>(first situation where appropriate to assign traction metrics) assign traction metrics to more junior product trios. Improving a traction metric is more of an optimization challenge than a wide-open discovery challenge and is a great way for a junior team to get some experience with discovery methods before giving the more responsibility.</p>\n</blockquote>\n<blockquote>\n<p>(second situation where appropriate to assign traction metrics) If you have a mature product and you have a traction metric that you know is critical to your company's success, it makes sense to align this traction metric to an optimization team.</p>\n</blockquote>\n<blockquote>\n<p>The key is to use a traction metrics only when you are optimizing a solution and not when the intent is to discover new solutions.</p>\n</blockquote>\n<h3>Outcomes Are the Results of a Two-Way Negotiation</h3>\n<blockquote>\n<p>The key is that the leader should not narrow the scope so much that the team is tasked with a traction metric.</p>\n</blockquote>\n<blockquote>\n<p>The trio should not be required to communicate what solutions they will build at this time, as this should emerge from discovery.</p>\n</blockquote>\n<blockquote>\n<p>the product leader will need to understand that more ambitious outcomes carry more risk.</p>\n</blockquote>\n<blockquote>\n<p>the product trio will need some time to learn what might move the metric. This is why a stable product trio focused on the same outcome over time is so critical. Every time we mix up the team or change the outcome, we take a learning tax as the team gets up to speed.</p>\n</blockquote>\n<blockquote>\n<p>teams who participated in the setting of their own outcomes took more initiative and thus performed better than colleagues who were not involved in setting their outcomes.</p>\n</blockquote>\n<h3>Do You Need S.M.A.R.T Goals?</h3>\n<blockquote>\n<p>It's perfectly fine to start with a learning goal and work your way toward a S.M.A.R.T performance goal.</p>\n</blockquote>\n<h3>A Guide for Product Trios</h3>\n<blockquote>\n<p>When your product leader assigns a new initiatives to your product trio, ask your leader to share more of the business context with you.</p>\n</blockquote>\n<blockquote>\n<p>Why do you think that initiative will drive that outcome?</p>\n</blockquote>\n<h3>Avoid These Common Anti-Patterns</h3>\n<blockquote>\n<p>Pursuing too many outcomes at once</p>\n</blockquote>\n<blockquote>\n<p>The goal is for the product trio to collaborate to achieve product outcomes that drive business outcomes.</p>\n</blockquote>\n<blockquote>\n<p>a team needs to monitor health metrics to ensure they aren't causing detrimental effects elsewhere.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>When changing to use outcome based product development, the initial step is going to be hard. How to define the right outcome? How can you measure it? can you impact it? is this the right outcome? Even harder when using lagging indicators like 90 day retention, which takes long time to measure.</p>\n<p>Road maps:</p>\n<ul>\n<li>Specifies a fixed list of features and dates which should be released</li>\n<li>Conveys a false sense of certainty about knowing the future</li>\n</ul>\n<p>Outcomes:</p>\n<ul>\n<li>Gives the team responsibility, autonomy and ownership to define how to achieve the outcome</li>\n<li>We acknowledge that we know what is the problem we want to solve, but not the best way to do it</li>\n</ul>\n<p>Different types of outcomes:</p>\n<p><strong>Business outcome</strong>: Measures business value. Financial (revenue, grow, costs), strategic (increase sales in a region). Usually a lagging indicator. Example is retention.</p>\n<p><strong>Product outcome</strong>: Measures how well the product moves the business forward. It's within the span of control of the product trio.</p>\n<p><strong>Traction metric</strong>: Measures usage of a specific feature or workflow in the product.</p>\n<p>Type of indicator:</p>\n<ul>\n<li>Lagging Indicators - Already happened.</li>\n<li>Leading Indicators - Predict the direction of the lagging indicator.</li>\n</ul>\n<p>Teams should avoid use lagging indicator as a guide, because put them in react mode and depending on the time period doesn't allow short cycles of experimentation (90 days retention). Therefore, is required to look for leading indicators that predict lagging indicators.</p>\n<p>Teams should receive a product metric rather than a business outcome. A business outcome might be out of control of the team to improve it. It's better to assign for each team a product outcome so they can create a sense of responsibility and ownership.</p>\n<p>There are two cases where it might be a good idea to assign a traction metric to the team</p>\n<ol>\n<li>Junior product trio. So they can focus on optimization while gaining experience in the discovery process</li>\n<li>Mature product trio and the traction metric is key to the company success.</li>\n</ol>\n<p>Traction metrics are good for optimization, not for discovering new solutions.</p>\n<p>Setting a metric is a two way negotiation with the CPO and the product trio. The product leader identifies which product outcome derived from the company outcomes the product trio can work on. Then the product trio brings customer and technology to determine how much they can move the metric.</p>\n<p>When faced with a new outcome, it's okay to first have a learning goal, and then once you learned it the performance goal.</p>\n<p>Anti-patterns to avoid:</p>\n<ul>\n<li>Pursuing too many outcomes at once.</li>\n<li>Ping-ponging from one outcome to another.</li>\n<li>Setting individual outcomes instead of product-trio outcomes.</li>\n<li>Choosing an output as an outcome</li>\n<li>Focusing on one outcome to the detriment of all else</li>\n</ul>\n<h2>Discovering Opportunities</h2>\n<h2>Chapter Four Visualizing What You Know</h2>\n<h2>Chapter Five Continuous Interviewing</h2>\n<h2>Chapter Six Mapping the opportunity space</h2>\n<h2>Chapter Seven Prioritizing Opportunities, Not Solutions</h2>\n<h2>Discovering Problems</h2>","frontmatter":{"title":"Continuous Discovery Habits","language":"en-US","coverPath":"continuous-discovery-habits","status":"Reading","date":"2021-09-13"}}},{"node":{"html":"<h1>Part 1 - Learning kanban</h1>\n<h2>1 Team Kanbaneros gets started</h2>\n<blockquote>\n<p>Take Scrum or Rational Unified Process (RUP) , for example; they prescribe what roles you should have, what meetings you should run, even how you should run them, and so on. Kanban, on the other hand, starts where you are, helps you understand your current situation, and helps you identify the next step to improve it.</p>\n</blockquote>\n<blockquote>\n<p>Visualizing work</p>\n<ul>\n<li>\n<p>Makes hidden work apparent</p>\n<ul>\n<li>Can be as easy as a sticky for each work item</li>\n</ul>\n</li>\n<li>\n<p>Helps you see:</p>\n<ul>\n<li>Who’s working on what</li>\n<li>What you’re working on</li>\n<li>How much is going on</li>\n</ul>\n</li>\n<li>Visible work radiates information to people seeing it</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>They decided to start the workflow when the work was registered in JIRA and to end it when the work was in production.</p>\n</blockquote>\n<blockquote>\n<p>When you visualize your pain and gather data about it, it’s much easier to get the stakeholders’ and other teams’ understanding. It’s not you nagging, it’s data.</p>\n</blockquote>\n<blockquote>\n<p>Map your workflow to the board</p>\n<ul>\n<li>Identify all the stages, from work entering to work leaving the team</li>\n<li>\n<p>Don’t strive for perfection</p>\n<ul>\n<li>Inspect and adapt</li>\n</ul>\n</li>\n<li>Work isn’t done until it’s producing value to the customer</li>\n<li>\n<p>With a visualized workflow you can see:</p>\n<ul>\n<li>Status of work</li>\n<li>Potential problems such as work not progressing and piling up in a stage</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Any short description that reminds you of what you’re doing will do. It’s good if it’s apparent what the item is about without having to get up close to the board and read for a while</p>\n</blockquote>\n<blockquote>\n<p>We want the board and everything on it to radiate information to us. That information will tell us how our work works so that we can learn from it. The real gain isn’t seeing the status of each work item, great as that is. The real gain is to help us make decisions and to improve our process as we learn from how it works.</p>\n</blockquote>\n<blockquote>\n<p>What goes on the card?</p>\n<ul>\n<li>\n<p>From the card you should:</p>\n<ul>\n<li>See the status of the work item</li>\n<li>Be able to form a decision about what to do next with it</li>\n</ul>\n</li>\n<li>\n<p>Common attributes are:</p>\n<ul>\n<li>Description of the work item</li>\n<li>ID in electronic systems</li>\n<li>Deadlines</li>\n<li>Who’s working on the item</li>\n<li>Type of work (bug or normal, for example)</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>to measure the total lead time, the time it takes from start to finish for the work to flow through our workflow,</p>\n</blockquote>\n<blockquote>\n<p>What we’ve shown you with this simple exercise is that when you decrease the number of concurrent or simultaneous ongoing work items, the lead time decreases,</p>\n</blockquote>\n<blockquote>\n<p>Notice that we’re doing the same amount of work, but working in a different way—with smaller batches, with less work in process at the same time</p>\n</blockquote>\n<blockquote>\n<p>Work in process (WIP) is the number of work items you have going at the same time. Less work in process leads to quicker flow through your process: shorter lead time.</p>\n</blockquote>\n<blockquote>\n<p>with less work in process, will both give you better total speed and let you become more agile, because you can deliver the small, important stuff first.</p>\n</blockquote>\n<blockquote>\n<p>Limit work in process</p>\n<ul>\n<li>Strive to work with fewer items at the same time</li>\n<li>Smaller batches means shorter lead times</li>\n<li>Resource efficiency decreases while flow efficiency increases</li>\n<li>Games/simulations like Pass the Pennies can be a great way to teach people abstract concepts</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>you could all agree to stop starting and start finishing.</p>\n</blockquote>\n<blockquote>\n<p>When stuff starts to come out of your system on a regular basis, the demand for accurate estimates and predictions will diminish, in our experience. Second, you won’t feel swamped with work because you now have a limit on how many things you’ll work on at the same time. If someone wants to add a new work item, they’ll have to also decide what gets taken out.</p>\n</blockquote>\n<blockquote>\n<p>You need to balance those two against each other: fast flow versus people having work to work on. You want a low work-in-process limit, but probably not a limit of ‘one,’</p>\n</blockquote>\n<blockquote>\n<p>It’s not that important where the flow slows down or stops. The important thing is what you do about it.</p>\n</blockquote>\n<blockquote>\n<p>The WIP limit isn’t a strict rule; it’s a trigger for discussions.</p>\n</blockquote>\n<blockquote>\n<p>Limit your work in process</p>\n<ul>\n<li>Start with: stop starting and start finishing</li>\n<li>Limiting WIP will surface improvement opportunities</li>\n<li>Acting on them leads to better flow</li>\n<li>There’s no one right WIP limit for a team</li>\n<li>\n<p>A lower WIP is generally better. As a rule of thumb:</p>\n<ul>\n<li>Too-high WIP leaves work idle</li>\n<li>Too-low WIP leaves people idle</li>\n</ul>\n</li>\n<li>WIP limits are not rules—they are triggers for discussions</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>An expedite item should only be used for urgent cases and is not to be used as a fast lane to cheat the system. You’ll settle on a limit for how many expedite items you allow per month</p>\n</blockquote>\n<blockquote>\n<p>Expedite lane</p>\n<ul>\n<li>\n<p>Common way to handle special cases</p>\n<ul>\n<li>Such as work that is urgent</li>\n</ul>\n</li>\n<li>Often visualized as a separate lane on the\nboard</li>\n<li>\n<p>Policies around that lane might be:</p>\n<ul>\n<li>Only one item can be in the lane at the time</li>\n<li>Max one expedite item per week</li>\n<li>Don’t count the expedite lane against the WIP limit</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>These are metrics by you, for you, to help you to find areas in which to improve</p>\n</blockquote>\n<blockquote>\n<p>With the board in place, you’ve set yourselves up to easily track at least two simple and powerful metrics: lead time and throughput.</p>\n</blockquote>\n<blockquote>\n<p>Lead time is the time it takes for a work item to go from start to finish—from the first column to the last.</p>\n</blockquote>\n<blockquote>\n<p>Starting to track lead times can be as simple as writing down the date of the sticky as it enters the Todo column and then writing down the date when it enters In Production. Plot that out in a simple diagram, and you’ll have a pretty good idea of what your average lead time is.</p>\n</blockquote>\n<blockquote>\n<p>Throughput, the rate at which you complete work, is even easier to track. In Production Count the number of items you finish for a given period of time</p>\n</blockquote>\n<blockquote>\n<p>Metrics</p>\n<ul>\n<li>There to help the team improve</li>\n<li>Let the team choose their own metrics; do not use them for performance review</li>\n<li>\n<p>Two common and useful metrics are:</p>\n<ul>\n<li>Lead time—the time for the whole workflow</li>\n<li>Throughput—how much or how many work items you complete over a period of time</li>\n</ul>\n</li>\n</ul>\n</blockquote>\n<h1>Part 2 - Understanding kanban</h1>\n<h2>2 Kanban principles</h2>\n<blockquote>\n<p>When working with this principle, you can find inspiration in Lean Thinking to help your work flow more smoothly by removing waste in your process. You can also take a look at the Theory of Constraints 3 and identify, exploit, and alleviate the bottlenecks in your system.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Visualize</li>\n<li>Limit work in process</li>\n<li>Manage flow</li>\n<li>Make process policies explicit - With explicit policies, you can start to have discussions around your process that are based on objective data instead of on what you think, feel, and have anecdotal evidence for.</li>\n<li>Implement feedback loops - This practice puts a focus on getting feedback from your process: for example, in what is called an operations review, which is a kind of retrospective for the process itself.</li>\n<li>Improve collaboratively, evolve experimentally (using models and the scientific method) - This practice encourages you to use models such as the Theory of Constraints or Lean to push your team toward further improvements.</li>\n</ol>\n</blockquote>\n<h2>3 Visualizing your work</h2>\n<blockquote>\n<p>When Japanese use the term visualization, or mieruka in Japanese ( 見える化 ), they often mean not only presenting things in an easily understandable visual form, but also the goal of greater transparency and information sharing among employees and stakeholders in order to increase the organization’s effectiveness.</p>\n</blockquote>\n<blockquote>\n<p>Remember that the policies are only that:\npolicies—not rules that must be followed. They can, and should, be broken from time to time, but the decision to do so should be made\nintentionally and often with careful consideration from the whole team.</p>\n</blockquote>\n<blockquote>\n<p>Information radiators</p>\n<ul>\n<li>Big, visible displays</li>\n<li>For you and other interested parties</li>\n<li>Keep it easy to update</li>\n<li>Keep it big</li>\n<li>Use it or lose it</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Which criteria need to be met in order for me to move a work item to the next column?</p>\n</blockquote>\n<h2>4 Work items</h2>\n<blockquote>\n<p>First and foremost, you want the design and information on the card to facilitate decision making in the team.</p>\n</blockquote>\n<blockquote>\n<p>the description needs to be terse, to the point, and easy for everyone on the team to understand.</p>\n</blockquote>\n<blockquote>\n<p>in your mind, put “The one where” before the description on the card.</p>\n</blockquote>\n<blockquote>\n<p>Deadlines are risk-management information and help the team prioritize and self-organize; you don’t want to miss seeing them, so make sure they’re clearly visible.</p>\n</blockquote>\n<blockquote>\n<p>Although keeping a separate “parking lot” for blocked items might seem like a good idea, we advise against it. It’s basically the same thing as saying that it’s OK to be blocked—“Look, we even have a dedicated area on the board for it!”</p>\n</blockquote>\n<blockquote>\n<p>o accomplish this, many teams attach another sticky on top of the blocked work item. This is a good idea because you also can write the reason why the item is blocked on the blocker sticky. In this way, you get not only a signal that the work is blocked, but also some information about why it’s blocked, which in turn helps the team focus on resolving the blockage.</p>\n</blockquote>\n<blockquote>\n<p>These types can also be used to set up policies on how work should be treated, commonly referred to as classes of service</p>\n</blockquote>\n<blockquote>\n<p>A progress indicator is a simple tool that helps you track this information and shows “how much done” the item is.</p>\n</blockquote>\n<h2>5 Work in process</h2>\n<blockquote>\n<p>It doesn’t mean you should do less work, but that you should do less work at the same time</p>\n</blockquote>\n<blockquote>\n<p>Little’s law often comes up. The law is a mathematical proof by John D.C. Little that says that the more things you have going at the same time, the longer each thing will take</p>\n</blockquote>\n<blockquote>\n<p>That’s exactly what you experience when doing context switching in knowledge work. You lose time and focus for every task you’re trying to keep in your head at the same time.</p>\n</blockquote>\n<blockquote>\n<p>One study 2 showed that as much as 10% of your working time per project is lost to context switching.</p>\n</blockquote>\n<blockquote>\n<p>The more quickly you can get feedback, the more quickly you can change a bad process into a slightly better one.</p>\n</blockquote>\n<h2>6 Limiting work in process</h2>\n<blockquote>\n<p>A lower WIP limit is generally better than a higher one because you want to limit the number of items you work on as much as possible.</p>\n</blockquote>\n<blockquote>\n<p>If your WIP limit is too high, work will become idle.</p>\n</blockquote>\n<blockquote>\n<p>With a WIP limit that is too low, people will become idle.</p>\n</blockquote>\n<h2>7 Managing flow</h2>","frontmatter":{"title":"Kanban in action","language":"en-US","coverPath":"kanban-in-action","status":"Reading","date":"2021-08-29"}}},{"node":{"html":"","frontmatter":{"title":"Ágil do jeito certo - Transformação sem caos","language":"pt-BR","coverPath":"agil-do-jeito-certo","status":"Read","date":"2021-08-25"}}},{"node":{"html":"","frontmatter":{"title":"Team Topologies","language":"en-US","coverPath":null,"status":"Reading","date":"2021-06-21"}}},{"node":{"html":"<h1>Praise for <em>Extreme Programming Explained, Second Edition</em></h1>\n<blockquote>\n<p>My only beef is that our profession has gotten to a point where such common-sense ideas are labeled 'extreme'...</p>\n</blockquote>\n<blockquote>\n<p>Do not be frightened by the name, it is not that extreme at all. It is mostly good old recipes ad common sense, nicely integrated together, getting rid of all the fat that has accumulated over the years.</p>\n</blockquote>\n<h1>The XP Series</h1>\n<blockquote>\n<p>(XP) is a discipline of the business of software development that focuses the whole team on common, reachable goals.</p>\n</blockquote>\n<blockquote>\n<p>XP practices are chosen for their encouragement of human creativity and their acceptance of human frailty.</p>\n</blockquote>\n<blockquote>\n<p>XP teams produce quality software at a sustainable pace.</p>\n</blockquote>\n<blockquote>\n<p>XP aims to achieve these goals (accountability, transparency and outstanding results) by celebrating and serving the human needs of everyone touched by software development sponsors, managers, testers, users, and programmers.</p>\n</blockquote>\n<h1>Foreword to the First Edition</h1>\n<blockquote>\n<p>Delivering software is hard, and delivering quality software in time is even harder. To make it work requires the disciplined use of additional best practices.</p>\n</blockquote>\n<h1>Preface</h1>\n<blockquote>\n<p>The goal of Extreme Programming (XP) is outstanding software development.</p>\n</blockquote>\n<blockquote>\n<p>Software can be developed at lower cost, with fewer defects, with higher productivity, and with much higher return on investment.</p>\n</blockquote>\n<h1>Chapter 1. What is XP?</h1>\n<blockquote>\n<p>Extreme Programming (XP) is about social change.</p>\n</blockquote>\n<blockquote>\n<p>It is about letting go of habits an patterns that were adaptive in the past, but now get in the way of us doing our best work.</p>\n</blockquote>\n<blockquote>\n<p>It is about being open about what we are capable of doing and then doing it.</p>\n</blockquote>\n<blockquote>\n<p>You need both technique and good relationships to be successful. XP addresses both.</p>\n</blockquote>\n<blockquote>\n<p>XP is a style of software development focusing on excellent application of programming techniques, clear communication and teamwork.</p>\n</blockquote>\n<blockquote>\n<p>XP includes:</p>\n<ul>\n<li>A philosophy of software development based on the values of communication, feedback, simplicity, courage and respect.</li>\n<li>A body of practices proven useful in improving software development. The practices complement each other, amplifying their effects. They are chosen as expressions of the values.</li>\n<li>A set of complementary principles, intellectual techniques for translating the values into practice, useful when there isn't a practice handy for your particular problem.</li>\n<li>A community that shares these values and many of the same practices</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>It is  distinguished from other methodologies by:</p>\n<ul>\n<li>Its short development cycles, resulting in early, concrete and continuing feedback.</li>\n<li>Its incremental planning approach, which quickly comes up with an overall plan that is expected to evolve through the life of the project.</li>\n<li>Its ability to flexibly schedule the implementation of functionality, responding to changing business needs.</li>\n<li>Its reliance on automated tests written by programmers, customers, and testers to monitor the progress of development, to allow the system to evolve, and to catch defects early.</li>\n<li>Its reliance on oral communication, tests, and source code to communicate system structure and intent.</li>\n<li>Its reliance on an evolutionary design process that last as as long as the system lasts.</li>\n<li>Its reliance on the close collaboration of actively engaged individuals with ordinary talent.</li>\n<li>Its reliance on practices that work with both the short-term instincts of the team members an the long-term interests of the project.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>XP can be described this way:</p>\n<ul>\n<li>XP is lightweight. In XP you only do what you need to do to create value for the customers.</li>\n<li>XP is a methodology based on addressing constraints in software development. ... Methodology is often interpreted to mean \"a set of rules to follow that guarantee success.\" Methodologies don't work like programs. People aren't computers. Every team dos XP differently with varying degrees of success.</li>\n<li>The values and principles behind XP are applicable at any scale.</li>\n<li>XP adapts to vague or rapidly changing requirements.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>XP is my attempt to reconcile humanity and productivity in my own practice of software development and to share that reconciliation.</p>\n</blockquote>\n<blockquote>\n<p>The pursuit of excellence in technique is critical in a social style of development.</p>\n</blockquote>\n<blockquote>\n<p>If you can accurately estimate your work, deliver quality the first time, and create rapid feedback loops; then you can be a trustworthy partner.</p>\n</blockquote>\n<blockquote>\n<p>XP teams play full out to win and accept responsibility for the consequences.</p>\n</blockquote>\n<blockquote>\n<p>but it is always best to act as if there is going to be enough.</p>\n</blockquote>\n<blockquote>\n<p>You can do your best work even when there are constraints. Fussing about the constraints distracts you from your goal. Your clean self does the best work no matter what the constraints are.</p>\n</blockquote>\n<blockquote>\n<p>It's not my job to \"manage\" someone else's expectations. It's their job to manage their own expectations. It's my job to do my best and to communicate clearly.</p>\n</blockquote>\n<blockquote>\n<p>How does XP address the risks in the development process?</p>\n<ul>\n<li>Schedule slips -  XP calls for short release cycles, a few months at most, so the scope of any slip is limited. Within a release, XP uses one-week iterations of customer-requested features to create fine-grained feedback about progress. Within an iteration, XP plans with short tasks, so the team can solve problems during the cycle. Finally, XP calls for implementing the highest priority features first, so any features that slip past the release will be of lower value.</li>\n<li>XP always keeps the system in deployable condition.</li>\n<li>Defect Rate XP tests from the perspective of both programmers writing tests function-by-function and customers writing tests program-feature-by-program-feature.</li>\n<li>XP calls for business-oriented people to be first-class members of the team.</li>\n<li>XP shortens the release cycle, so there is less change during the development of a single release.</li>\n<li>XP insists that only the highest priority tasks are addressed.</li>\n<li>Staff turnover XP asks programmers to accept responsibility for estimating and completing their own work, gives them feedback about the actual time taken so their estimates can improve, and respects those estimates. The rules for who can make and change estimates are clear. Thus, there is less chance for a programmer to get frustrated by being asked to do the obviously impossible... New team members are  encouraged to gradually accept more and more responsibility, and are assisted along the way by each other and by existing programmers.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>XP assumes that you see yourself as part of a team, ideally one with clear goals and a plan of execution. XP assumes that you want to work together. XP assumes that change can be made inexpensive using this method. XP assumes that you want to grow, to improve your skills, and to improve your relationships. XP assumes you are willing to make changes to meet those goals.</p>\n</blockquote>\n<blockquote>\n<p>What is XP?</p>\n<ul>\n<li>XP is giving up old, ineffective technical and social habits in favor of new ones that work.</li>\n<li>XP is fully appreciating yourself for total effort today.</li>\n<li>XP is striving to do better tomorrow.</li>\n<li>XP is evaluating yourself by your contribution to the team's shared goals.</li>\n<li>XP is asking to get some of your human needs met through software development.</li>\n</ul>\n</blockquote>\n<h2>My Summary</h2>\n<p>The XP is presented as a change of the practices applied in software engineering at the time of the book's publication. It proposes not only technical but also relationships lead to good businesses. </p>\n<p>XP includes values, principles, practices and a community.</p>\n<p>It's distinguished by waterfall methodologies due to its short cycles, incremental planning, responding to business changes, automated tests, evolutionary systems and close collaboration. To say only a few.</p>\n<p>XP is lightweight and focused to do the simplest that delivers value to customers. It's a methodology that addresses constraints in software development. It's not a recipe, rather each team will apply XP differently with varying degrees of success. XP can be applied at any scale due to its values and principles. Even though XP works in situations where requirements don't change it shines in contexts with rapidly changing requirements.</p>\n<p>It tries to boost productivity by recognizing the importance of humanity. Of course, the pursuit of technical excellence is critical in a <em>social style of development</em>. It proposes that if you can apply techniques that will accurately estimate work, deliver software with quality and create rapid feedback loops, and ultimately business value you'll be considered a trustworthy partner.</p>\n<p>When applying XP you apply 100% of what you got, you act to achieve your best and you are able to achieve your best even when there are constraints.</p>\n<p>XP addresses risk at all levels of software development:</p>\n<ul>\n<li>XP proposes release cycles ( a few months at most) to reduce time to market and get feedback earlier. Within a release the every week the team gets feedback about their progress. Short release cycles also reduce the changes during development and increase the flexibility for business changes.</li>\n<li>XP teams also prioritize work and focus on the highest value.</li>\n<li>The business is considered first-class member of the development to seek for the highest value delivers and help to refine the business.</li>\n<li>XP focus on quality to reduce defect rate and in automation to ensure the system is always in a ready to deploy state.</li>\n<li>XP teams are responsible for doing the estimate and for completing their work. This reduces frustration from missed impossible deadlines assigned top down to developers. Also, it's expected that a new team member will gradually accept more and more responsibilities.</li>\n</ul>\n<p>XP assumes that you believe at teamwork, that changes shouldn't be expensive, that you are seeking technical excellence and to improve your relationships. You'll have to change to achieve this goals.</p>\n<h1>Section 1: Exploring XP</h1>\n<h1>Chapter 2. nos livros da lista, mas achei os títulos com Learning to Drive</h1>\n<blockquote>\n<p>\"...Driving is about constantly paying attention, making a little correction this way, a little correction that way\".</p>\n</blockquote>\n<blockquote>\n<p>This is the paradigm for XP. Stay aware. Adapt. Change.</p>\n</blockquote>\n<blockquote>\n<p>Everything in software changes. The requirements change. The design changes. The business changes. the technology changes. The team changes. The team members change. The problem isn't change, because change is going to happen; the problem rather, is our inability to cope with change.</p>\n</blockquote>\n<blockquote>\n<p>Customer dive the content of the system. The whole team drives the development process. XP lets you adapt by making frequent, small corrections; moving towards your goal with deployed software at short intervals. You don't wait a long time to find out if you were going the wrong way.</p>\n</blockquote>\n<blockquote>\n<p>Customers (internal or external) start with a general idea of what problems the system need to solve.</p>\n</blockquote>\n<blockquote>\n<p>The customers on the team need to keep in mind where on the horizon they want to go even as they decide, week-by-week, where the software should go <em>next</em>.</p>\n</blockquote>\n<blockquote>\n<p>As development continues, the team becomes aware of which of their practices enhance and which of their practices detract from their goals.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Change will occur no matter what. We have to be ready to work with it. The author uses driving as an analogy for XP and changes. Stay aware. Adapt. Change.</p>\n<p>This analogy works in two different levels:</p>\n<ul>\n<li>Customer: You start with an idea of what you want to build, and you adapt and apply small corrections in order to come closer to your goal.</li>\n<li>Development: Similarly for development, you start with a set of practices, and as the project evolves you improve these practices by understanding what you have to enhance in these practices.</li>\n</ul>\n<h1>Chapter 3. Values, Principles and Practices</h1>\n<blockquote>\n<p>Practices are the things you do day-to-day. Specifying practices is useful because they are clear and objective. You either write a test before you change code or you don't.</p>\n</blockquote>\n<blockquote>\n<p>Call this level of knowledge and understanding values. Values are the roots of the things we like and don't like in a situation.</p>\n</blockquote>\n<blockquote>\n<p>Making values explicit is important because without values, practices quickly become rote, activities performed for their own sake but lacking any purpose or direction.</p>\n</blockquote>\n<blockquote>\n<p>The defect itself might be a failure of technique, but the reluctance to learn from the defect shows that the programmer doesn't actually value learning and elf-improvement as much as something else.</p>\n</blockquote>\n<blockquote>\n<p>Values bring purpose to practices.</p>\n</blockquote>\n<blockquote>\n<p>Just as values bring purpose to practices, practices bring accountability to values.</p>\n</blockquote>\n<blockquote>\n<p>Principles are domain-specific guidelines for life.</p>\n</blockquote>\n<blockquote>\n<p>It is a start but it isn't enough for you to master XP.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Third chapter starts with an analogy about learning the basics about a skill and being a gardening master.</p>\n<p>Level of understanding:</p>\n<ul>\n<li>Practice: Clear and objective. Either you do or you don't. A place to start. Value's accountability.</li>\n<li>Principles: The bridge between practice and value. It's domain specific.</li>\n<li>Values: The roots of the things we like and don't like in a situation. A highly developed sense of good and bad. Experience. Practice's purpose.</li>\n</ul>\n<p>You are not an extreme programmer master because you read a book.</p>\n<h1>Chapter 4. Values</h1>\n<blockquote>\n<p>He knows in his bones what matters and what doesn't.</p>\n</blockquote>\n<blockquote>\n<p>The difference between what I think is valuable and what is really valuable creates waste.</p>\n</blockquote>\n<blockquote>\n<p>What actually matters is not how any given person behaves as much as how the individuals behave as part of a team and as part of an organization.</p>\n</blockquote>\n<blockquote>\n<p>XP embraces five values to guide development: communication, simplicity, feedback, courage and respect.</p>\n</blockquote>\n<h2>Communication</h2>\n<blockquote>\n<p>What matters most in team software development is communication.</p>\n</blockquote>\n<blockquote>\n<p>When problems arise in development, most often someone already knows the solution; but that knowledge doesn't get through to someone with the power to make the change.</p>\n</blockquote>\n<blockquote>\n<p>However, motion without communication is not progress.</p>\n</blockquote>\n<blockquote>\n<p>When you encounter a problem, ask yourselves if the problem was caused by a lack of communication.</p>\n</blockquote>\n<blockquote>\n<p>Communication is important for creating a sense of team and effective cooperation.</p>\n</blockquote>\n<h2>Simplicity</h2>\n<blockquote>\n<p>To make a system simple enough to gracefully solve only today's problem is hard work.</p>\n</blockquote>\n<blockquote>\n<p>Simplicity only makes sense in context.</p>\n</blockquote>\n<blockquote>\n<p>Improving communication helps achieve simplicity enabling unneeded or deferrable requirements from today's concerns.</p>\n</blockquote>\n<h2>Feedback</h2>\n<blockquote>\n<p>Directions set in advance of experience have an especially short half-life. Change is inevitable, but change creates the need for feedback.</p>\n</blockquote>\n<blockquote>\n<p>Being satisfied with improvement rather than expecting instant perfection, we use feedback to get closer and closer to our goals.</p>\n</blockquote>\n<blockquote>\n<p>XP teams strive to generate as much feedback as they can handle as quickly as possible.</p>\n</blockquote>\n<blockquote>\n<p>The sooner you know, the sooner you can adapt.</p>\n</blockquote>\n<blockquote>\n<p>For example, suppose you move to quarterly releases and suddenly have more defect reports than you can respond to before the next quarter's release. Slow down releases until you can handle the defect reports and still develop new functionalities.</p>\n</blockquote>\n<h2>Courage</h2>\n<blockquote>\n<p>Courage is effective action in the face of fear.</p>\n</blockquote>\n<blockquote>\n<p>It's how they handle their fear that dictates whether they are working as an effective part of a team.</p>\n</blockquote>\n<blockquote>\n<p>If you know what the problem is, do something about it. Sometimes courage manifests as patience. If you know there is a problem but you don't know what it is, it takes courage to wait for the real problem to emerge distinctly.</p>\n</blockquote>\n<blockquote>\n<p>Doing something without regard for the consequences is not effective teamwork.</p>\n</blockquote>\n<blockquote>\n<p>The courage to speak truths, pleasant or unpleasant, fosters communication and trust. The courage to discard failing solutions and seek new ones encourages simplicity. The courage to eek real, concrete answers creates feedback.</p>\n</blockquote>\n<h2>Respect</h2>\n<blockquote>\n<p>If members of a team don't care about each other and what they are doing, XP won't work. If members of a team don't care about a project, nothing can save it.</p>\n</blockquote>\n<blockquote>\n<p>For software development to simultaneously improve in humanity and productivity, the contributions of each person on the team need to be respected.</p>\n</blockquote>\n<blockquote>\n<p>Values don't provide concrete advice about what to do in software development. Because of the distance between values and practices, we need a way to bridge the gap between them. Principles are the tool we need.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Communication is key in software development. The sum of knowledge of all the people in a project is a great solution/problem catalog.</p>\n<p>If the problem is about not having the knowledge, then you can either learn with people that had similar problems or avoid that it become a recurring problem. Other than that there is nothing you can do.</p>\n<p>Simplicity as the opposite of complexity. Do only what you need to do. It is different from taking shortcuts. </p>\n<p>Our plans will change, feedback avoid generating too much waste. It helps to do small corrections in the direction of the right goal.</p>\n<p>When building a solution:</p>\n<ul>\n<li>You might not know what the right is. Or there might be several right solutions.</li>\n<li>Time changes business requirements, or even technical requirements.</li>\n<li>Requirements change too often, at a certain point that would invalidate a right solution even before it's done.</li>\n</ul>\n<p>Feedback is opinions, the implementation, if the code is testable, if tests pass, the solution in production. We seek as much feedback as we can handle. The shorter the feedback cycle the better, as long we can handle it.</p>\n<p>Courage can be either acting in the face of fear, or taking action to wait for a real problem to emerge. It shouldn't be reckless. It's power together with the other values.</p>\n<p>Respect lies below the surface of the other values. Without respect there isn't a team.</p>\n<h1>Chapter 5. Principles</h1>\n<h2>Humanity</h2>\n<blockquote>\n<p>This isn't good for business either, with the costs and disruption of high turnover and missed opportunities for creative action.</p>\n</blockquote>\n<blockquote>\n<p><em>Belonging</em> the ability to identify with a group from which they receive validation and accountability and contribute to its shared goals.</p>\n</blockquote>\n<blockquote>\n<p><em>Growth</em> the opportunity to expand their skills and perspective</p>\n</blockquote>\n<blockquote>\n<p>Limiting work hours allows time for these other human needs and enhances each person's contribution while he is with the team.</p>\n</blockquote>\n<blockquote>\n<p>Part of the challenge of team software development is balancing the needs of the individuals with the needs of the team.</p>\n</blockquote>\n<blockquote>\n<p>The magic of great teams is that after the team members develop trust they find that they are free to be <em>more</em> themselves as a result of their work together.</p>\n</blockquote>\n<h2>Economics</h2>\n<blockquote>\n<p>Make sure what you are doing has business value, meets business goals and serves business needs.</p>\n</blockquote>\n<blockquote>\n<p>The time value of money says that a dollar today is worth more than a dollar tomorrow.</p>\n</blockquote>\n<blockquote>\n<p>Incremental design explicitly defers design investment until the last responsible moment in an effort to spend money later. Pay-per-use provides a way of realizing revenue from features as soon as they are deployed.</p>\n</blockquote>\n<blockquote>\n<p>Another source of economic value in software development is its value as options for the future.</p>\n</blockquote>\n<h2>Mutual Benefits</h2>\n<blockquote>\n<p>Mutual benefit is the most important XP principle and the most difficult to adhere to.</p>\n</blockquote>\n<blockquote>\n<p>There are always solutions to any problem that cost one person while benefiting another.</p>\n</blockquote>\n<blockquote>\n<p>Mutual benefit in XP is searching for practices that benefit me now, me later, and my customer as well.</p>\n</blockquote>\n<h2>Self-Similarity</h2>\n<blockquote>\n<p>Try coping the structure of one solution into a new context, even at different scales.</p>\n</blockquote>\n<blockquote>\n<p>Just because you copy a structure that works in one context doesn't mean it will work in another. It is good place to start, though. Likewise, just because a solution is unique doesn't mean it's bad.</p>\n</blockquote>\n<h2>Improvement</h2>\n<blockquote>\n<p>In software development, \"perfect\" is verb, not an adjective. There is no perfect process. There is no perfect design. There are no perfect stories. You can, however, prefect your process, your design, and our stories.</p>\n</blockquote>\n<blockquote>\n<p>\"Best is the enemy of good enough\" suggests that mediocrity is preferable to waiting.</p>\n</blockquote>\n<blockquote>\n<p>The cycle is to do the best you can today, striving for the awareness and understanding necessary to do better tomorrow.</p>\n</blockquote>\n<blockquote>\n<p>The history of software development technology shows us gradually eliminating wasted effort.</p>\n</blockquote>\n<blockquote>\n<p>Find a starting place, get started, and improve from there.</p>\n</blockquote>\n<h2>Diversity</h2>\n<blockquote>\n<p>Teams need to bring together a variety of skills, attitudes, and perspectives to see problems and pitfalls, to think of multiple ways to solve problems, and to implement the solutions. Teams need diversity.</p>\n</blockquote>\n<blockquote>\n<p>Conflict is the inevitable companion of diversity.</p>\n</blockquote>\n<blockquote>\n<p>Two ideas about a design present an opportunity, not a problem.</p>\n</blockquote>\n<blockquote>\n<p>Every team has conflict. The question is whether they resolve it productively. Respecting others and maintaining myself smooths communication in time of stress.</p>\n</blockquote>\n<blockquote>\n<p>Diversity is expressed in the practice of Whole Team, where you bring together on the team people with a variety of skills and perspectives.</p>\n</blockquote>\n<h2>Reflection</h2>\n<blockquote>\n<p>Good teams don't just do their work, they think about <em>how</em> they are working and <em>why</em> they are working.</p>\n</blockquote>\n<blockquote>\n<p>Reflection comes after action. Learning is action reflected. To maximize feedback, reflection in XP teams is mixed with doing.</p>\n</blockquote>\n<h2>Flow</h2>\n<blockquote>\n<p>Flow in software development is delivering a steady flow of valuable software by engaging in all the activities of development simultaneously.</p>\n</blockquote>\n<h2>Opportunity</h2>\n<blockquote>\n<p>To reach excellence, problems need to turn into opportunities for learning and improvement, not just survival.</p>\n</blockquote>\n<blockquote>\n<p>Part of being extreme is consciously choosing to transform each problem into an opportunity: an opportunity for personal growth, deepening relationships, and improved software.</p>\n</blockquote>\n<h2>Redundancy</h2>\n<blockquote>\n<p>The cost of the redundancy is more than paid for by the savings from not having the disaster.</p>\n</blockquote>\n<blockquote>\n<p>While redundancy can be wasteful, be careful not to remove redundancy that servers a valid purpose.</p>\n</blockquote>\n<h2>Failure</h2>\n<blockquote>\n<p>If you're having trouble succeeding, fail.</p>\n</blockquote>\n<blockquote>\n<p>(about waste) No, not if it imparts knowledge. Knowledge is valuable and sometimes hard to come by. Failure may not be avoidable waste.</p>\n</blockquote>\n<blockquote>\n<p>(About the team the thought too much) They only used the time a couple of time, but they kept it around as a reminder to fail instead of talk.</p>\n</blockquote>\n<blockquote>\n<p>When you don't know what to do though, risking failure can be the shortest, surest road to success.</p>\n</blockquote>\n<h2>Quality</h2>\n<blockquote>\n<p>Sacrificing quality is not effective as a means of control. Quality is not a control variable. Projects don't go faster bu accepting lower quality. They don't go slower by demanding high quality. Pushing quality higher often results in faster delivery; while lowering quality standards often results in later, less predictable delivery.</p>\n</blockquote>\n<blockquote>\n<p>Time and cost are most often fixed. XP chooses scope as the primary means of planning, tracking, and steering projects. Since scope is never known precisely in advance, it makes a good lever.</p>\n</blockquote>\n<blockquote>\n<p>If you don't know a clean way to do a job that has to be done, do it the best way you can. If you know a clean way but it wold take too long, do the job as well as you have time for now.</p>\n</blockquote>\n<blockquote>\n<p>This often occurs during architectural evolution, where you have to live with two architecture solving the same problem while you transition from one to the other.</p>\n</blockquote>\n<h2>Baby Steps</h2>\n<blockquote>\n<p>Under the right conditions, people and teams can take many small steps so rapidly that they appear to be leaping.</p>\n</blockquote>\n<h2>Accepted Responsibility</h2>\n<blockquote>\n<p>Responsibility cannot be assigned; it can only be accepted</p>\n</blockquote>\n<blockquote>\n<p>Similarly, the person responsible for implementing a story ultimately responsible for the design, implementation, and testing of the story.</p>\n</blockquote>\n<blockquote>\n<p>You will create new practices occasionally to fill you specific need.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Humanity, contemporaneously might be called as Developer Experience. The companies or teams that don't have humanity will have a high turnover and it will impact business. A developer needs belonging and growth to be a improve as developer. The developer shouldn't sacrifice its career for the company or the team. It should be a balance.</p>\n<p>Economics, is about knowing that your company has bills to pay. You should spend their money wisely and focus on the highest business value as soon as possible. Incremental design, deferring technical decision until the last moment in a effort to spend money latter and Pay-per-use realizing revenue as early as possible.</p>\n<p>Mutual Benefits, seeks to find win win situations, where both the XP team and its customer will gain. Mutual benefits seeks for you and your customers benefits now and later.</p>\n<p>Self-Similarity, is about applying patterns that you found in a context in similar problems at different scales. It's not because a solution is rare that is bad, neither because it's widely used that is good (though is a good starting place).</p>\n<p>Improvement, is about doing the best you can everyday, and bringing improvements over time. There's always place to improve, perfect is a verb not a noun. Good enough is preferable instead of waiting.</p>\n<p>Diversity, the more different attitudes, perspectives the easier to see pitfalls. Diversity also brings conflicts which is an opportunity to improvement.</p>\n<p>Reflection, XP teams consider the <em>how</em> and the <em>why</em> they work that way. Reflection comes after action. The best way to learn is to start doing something.</p>\n<p>Flow, is about a continuous delivering value to customers, we do that by starting all the activities to deliver software together. The more frequent or delivers more feedback, more improvements and better the flow. We must avoid doing something that is problematic only because it has a problem, it's actually the opposite, we need to fix the problem.</p>\n<p>Opportunity, is about turning problems into learning and improvement. It's about the attitude you face your challenges.</p>\n<p>Redundancy, is important to prevent disaster when the first solution fails. The cost of redundancy pays off from not having the disaster.</p>\n<p>Failure, it's best to try and fail than stagnation. If failure imparts knowledge, then it is valuable. Avoid over thinking when you can try and get real feedback. Accept failure.</p>\n<p>Quality, projects don't go faster when sacrificing quality. When projects have time and cost fixed, use scope as a control mechanism.</p>\n<p>Baby Steps, smalls steps are less wasteful. Test-first, continuous integration, constantly going to production are examples of XP baby steps.</p>\n<p>Accepted Responsibility, responsibility cannot be assigned only accepted. It's about building ownership.</p>\n<h1>Chapter 6. Practices</h1>\n<blockquote>\n<p>Unless given purpose by values, they become rote.</p>\n</blockquote>\n<blockquote>\n<p>The practices are a vector from where you are to where you can be with XP.</p>\n</blockquote>\n<blockquote>\n<p>I have divided the practices into two chapters: \"Primary Practices,\" Chapter 7, and \"Corollary Practices,\" Chapter 9. The primary practices are useful independent of what else you are doing. They each can give you immediate improvement. You can start safely with any of them. The corollary practices are likely to be difficult without first mastering the primary practices.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Practices need to be together with values otherwise they are empty. Only applying the practices without the values is going to be meaningless.</p>\n<p>Practices are the XP vector that display where you are.</p>\n<p>There are two types of practices: Primary and Corollary. Both of them are important, but you should start with the primary, since they are more independent and can provide a immediate improvement boost.</p>\n<h1>Chapter 7. Primary Practices</h1>\n<h2>Sit Together</h2>\n<blockquote>\n<p>Develop in an open space big enough for the whole team. Meet the need for privacy and \"owned\" space by having small private spaces nearby or by limiting work hours so team members can get their privacy needs met elsewhere.</p>\n</blockquote>\n<blockquote>\n<p>\"Sit together\" predicts that the more face time you have, the more humane and productive the project.</p>\n</blockquote>\n<h2>Whole Team</h2>\n<blockquote>\n<p>Include on the team people with all the skills and perspectives necessary for the project to succeed. This is really nothing more than the old idea of cross-functional team.</p>\n</blockquote>\n<blockquote>\n<p>An issue that often arises is ideal team size. <em>The Tipping Point</em> by Malcolm Gladwell describes two discontinuities in team size: 12 and 150.</p>\n</blockquote>\n<blockquote>\n<p>Some organizations try to have teams with fractional people: \"You'll spend 40% of your time working for these customers and 6% work for those customers\". In this case, so much time is wasted on task-switching that you can see immediate improvement by grouping the programmers into teams. The team responds to the customers' needs.</p>\n</blockquote>\n<h2>Informative Workspace</h2>\n<blockquote>\n<p>Make your workspace about your work. An interested observer should be able to walk into the team space and get a general idea of how the project is going in fifteen seconds.</p>\n</blockquote>\n<blockquote>\n<p>Use your space for important, active information.</p>\n</blockquote>\n<h2>Energized Work</h2>\n<blockquote>\n<p>Work only as many hour as you can be productive and only as many hours you can sustain. Burning yourself out unproductively today and spoiling the next two days' work isn't good for you or the team.</p>\n</blockquote>\n<blockquote>\n<p>Software development is a game of insight, and insight comes to the prepared, rested, relaxed mind.</p>\n</blockquote>\n<h2>Pair Programming</h2>\n<blockquote>\n<p>Write all production programs with two people sitting at one machine. Set up the machine so the partners can sit comfortably side-by-side. Move the keyboard and mouse back and forth so you are comfortable.</p>\n</blockquote>\n<blockquote>\n<p>Pair programming is a dialog between two people simultaneously programming (and analyzing and designing and testing) and trying to program better.</p>\n</blockquote>\n<blockquote>\n<p>If you need to work on an idea alone, go do it. Then come back and check with your team.</p>\n</blockquote>\n<h2>Stories</h2>\n<blockquote>\n<p>Plan using units of customer-visible functionality. \"Handle five times the traffic with the same response time.\", \"Provide a two-click way for user to dial frequently used numbers.\". As soon as a story is written, try to estimate the development effort necessary to implement it.</p>\n</blockquote>\n<blockquote>\n<p>Early estimation is a key difference between stories an other requirements practices. Estimation gives the business and technical perspectives a chance to interact, which creates value early, when an idea has the most potential.</p>\n</blockquote>\n<blockquote>\n<p>One feature of XP style planning is that stories are estimated very early in their life. This gets everyone thinking about how to get the greatest return from the smallest investment.</p>\n</blockquote>\n<h2>Weekly Cycle</h2>\n<blockquote>\n<p>Plan work a week at a time. Have a meeting at the beginning of every week.</p>\n</blockquote>\n<blockquote>\n<p>Planning is a form of necessary waste. It doesn't create much value all by itself.</p>\n</blockquote>\n<h2>Quarterly Cycle</h2>\n<blockquote>\n<p>Plan work a quarter at a time. Once a quarter reflect on the team, the project, its progress, and its alignment with larger goals.</p>\n</blockquote>\n<h2>Slack</h2>\n<blockquote>\n<p>In any plan, include some minor tasks that can be dropped if you get behind. You can always add more stories later and deliver more than you promised. It's important in an atmosphere of distrust and broken promises to meet your commitments.</p>\n</blockquote>\n<blockquote>\n<p>I suggested to the middle manager that he encourage his teams to only sign up for what they were confident they could actually do.</p>\n</blockquote>\n<blockquote>\n<p>You can structure slack in many ways. One week in eight could be \"Geek Week\". Twenty percent of the weekly budget could go to programmer-chosen tasks.</p>\n</blockquote>\n<h2>Ten-Minute Build</h2>\n<blockquote>\n<p>A shorter build doesn't give you time to drink coffee.</p>\n</blockquote>\n<h2>Continuous Integration</h2>\n<blockquote>\n<p>Integrate and test changes after no more than a couple of hours.</p>\n</blockquote>\n<blockquote>\n<p>Integrate and build a complete product.</p>\n</blockquote>\n<h2>Test-First Programming</h2>\n<blockquote>\n<p>Test-first programming addresses many problems at once:</p>\n<ul>\n<li>Scope-creep: It's easy to get carried away programming and put in code \"just in case\". By stating explicitly and objectively what the program is supposed to do, you give yourself a focus for your coding.</li>\n<li>Coupling and Cohesion: If it's hard to write a test, it's a signal that you have a design problem, not a testing problem. Loosely coupled, higly cohesive code is easy to test.</li>\n<li>Trust: It's hard to trust the author of code that doesn't work.</li>\n<li>Rhythm: It's easy to get lost for hours when you are coding. When programming test-first, it's clearer what to do next: either write another test or make the broken test work.</li>\n</ul>\n</blockquote>\n<h2>Incremental Design</h2>\n<blockquote>\n<p>When your understanding of the best possible design leaps forward, work gradually but persistently to bring the design back into alignment with your understanding.</p>\n</blockquote>\n<blockquote>\n<p>XP teams work hard to create conditions under which the cost of changing the software doesn't rise catastrophically. The automated tests, the continual practice of improving the design , and the explicit social process all contribute to keep the cost of changes low.</p>\n</blockquote>\n<blockquote>\n<p>They piled story on story as quickly as possible with the least possible investment in design. Without daily attention to design , the cost of changes does skyrocket. The result is poorly designed, brittle, hard-to-change systems.</p>\n</blockquote>\n<blockquote>\n<p>As a direction for improvement, incremental design doesn't say that designing in advance of experience is horrible. It says that design done close to when it is used is more efficient.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Sit Together, the whole team should sit together. The longer the team stays together more productive it will become.</p>\n<p>Whole Team, same as cross functional teams. Teams are composed by a set of different skills from different people.</p>\n<p>Informative Workspace, use space to conveys information about the project.</p>\n<p>Energized Work, work must be sustainable. Work too many hours don't improve productivity. Software development is an insight game, the more tired you are less insight you have.</p>\n<p>Pair Programming, is a dialogue. Programming, testing and designing between two persons. You can for sure spend some alone, and then regroup with your pair.</p>\n<p>Stories, instead of planning each layer of development, focus on the functionality for the customer. Stories are short and convey the customer need. Estimate it after you understand the scope.</p>\n<p>Weekly Cycle, at beginning of the week you plan the whole week. Focus on what delivers more value to customers.</p>\n<p>Quarterly Cycle, similar to the weekly cycle, but quarterly. It's important to have a more high level understanding where you are going. Otherwise, you could a tunnel vision of the week by week delivers.</p>\n<p>Slack, tasks that you can drop to avoid breaking not delivering what you promised. Could also be time for the developers to choose their highest problem and fix it.</p>\n<p>Ten-Minute Build, building your software must be fast enough so you don't have time to get a coffee. The faster the build, the faster the developer feedback loop.</p>\n<p>Continuous Integration, integrate your code frequently with the main. Avoid big chunks of code.</p>\n<p>Test-First Programming, addresses scope creep, coupling and cohesion, trust, rhythm. Start to develop the your stories by creating the test. Then develop it's functionality.</p>\n<p>Incremental Design, try to do the design as close as you need it, so you have the most knowledge about the solution. You should pay constant attention to the costs of change. Tests helps to maintain it under control.</p>\n<h1>Chapter 8. Getting Started</h1>\n<blockquote>\n<p>XP works best when it is done all together, but you need a starting place.</p>\n</blockquote>\n<blockquote>\n<p>How do you decide what to change first? Look at what you are doing and what you want to achieve. Choose the first practice on that path.</p>\n</blockquote>\n<blockquote>\n<p>Change begins with awareness. Awareness of the need for change comes from feelings, instincts, facts or feedback from outsiders. Feelings, while valuable, need to be cross-checked with facts or trusted opinions.</p>\n</blockquote>\n<blockquote>\n<p>Metrics can lead to awareness. Trends in metrics can point to the need for change for change before the consequence of the trend becomes painful.</p>\n</blockquote>\n<blockquote>\n<p>Change always starts at home. The only person you can actually change is yourself. No matter how functional or dysfunctional your organization, you can begin applying XP for yourself.</p>\n</blockquote>\n<blockquote>\n<p>If there is one message I would like to communicate, whatever your job title and however your work is touched by software development it is this: software development is capable of much, much more than it is currently delivering.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>The change always start with yourself, the developer. Once you want to change, try one practice at time. Try, fail, improve until you have master it. Then, go to next practice.</p>\n<p>To start to change you have to be aware. Metrics can help with that. Either way be conscious about your choices.</p>\n<h1>Chapter 9. Corollary Practices</h1>\n<blockquote>\n<p>The practices in this chapter seem to me to be difficult or dangerous to implement before completing the preliminary work of the primary practices.</p>\n</blockquote>\n<h2>Real Customer Involvement</h2>\n<blockquote>\n<p>Make people whose lives and business are affected by your system part of the team.</p>\n</blockquote>\n<blockquote>\n<p>The point of customer involvement is to reduce wasted effort by putting the people with the needs in direct contact with the people who can fill those needs.</p>\n</blockquote>\n<blockquote>\n<p>Generally, the closer customer needs and development capabilities are, the more valuable development becomes.</p>\n</blockquote>\n<blockquote>\n<p>When you act trustworthy and have nothing to hide, you are more productive.</p>\n</blockquote>\n<h2>Incremental Deployment</h2>\n<blockquote>\n<p>When replacing a legacy system, gradually take over its workload beginning very early in the project.</p>\n</blockquote>\n<h2>Team Continuity</h2>\n<blockquote>\n<p>Value in software is create not just by what people know and do but also by their relationship and what they accomplish together.</p>\n</blockquote>\n<h2>Shrinking Teams</h2>\n<blockquote>\n<p>As a team grows in capability, keep its workload constant but gradually reduce its size. This frees people to form more teams. When the team has too few members, merge it with another too-small team.</p>\n</blockquote>\n<h2>Root-Cause Analysis</h2>\n<blockquote>\n<p>Every time a defect is found after development, eliminate the defect and its cause. The goal is not just that this one defect won't ever recur, but that the team will ever make the same kind of mistake again.</p>\n</blockquote>\n<h2>Shared Code</h2>\n<blockquote>\n<p>Anyone on the team can improve any part of the system at any time. If something is wrong with the system and fixing it is not out of scope for what I'm doing right now, I should go ahead and fix it.</p>\n</blockquote>\n<blockquote>\n<p>There are other models of teamwork besides \"every man for himself\". The team members can collectively assume responsibility not just for the quality of what they deliver to users but also for the pride they take in their work along the way.</p>\n</blockquote>\n<h2>Code and Tests</h2>\n<blockquote>\n<p>Customer pay for the what the system does today and what the team can make the system to tomorrow.</p>\n</blockquote>\n<blockquote>\n<p>What are we going to do? What aren't we going to do? and How are we going to do what we do ? Bringing those decisions together so they can feed each other smooths the flow of value.</p>\n</blockquote>\n<h2>Single Code Base</h2>\n<blockquote>\n<p>There is only one code stream. You can develop in a temporary branch, but never let it live longer than a few hours.</p>\n</blockquote>\n<h2>Daily Deployment</h2>\n<blockquote>\n<p>Put new software into production every night. Any gap between what is on a programmer's desk and what is in production is a risk.</p>\n</blockquote>\n<blockquote>\n<p>As long as you don't change the user's experience of the system, you can deploy all the rest of that work. On the last day you put the \"keystone\", the change to the user interface, in place.</p>\n</blockquote>\n<h2>Negotiated Scope Contract</h2>\n<blockquote>\n<p>Write contracts for software development that fix time, costs and quality but call for an ongoing negotiation of the precise scope of the system. Reduce risk by signing a sequence of short contracts instead of one long one.</p>\n</blockquote>\n<h2>Pay-Per-Use</h2>\n<blockquote>\n<p>Connecting money flow directly to software development provides accurate, timely information with which to drive improvement.</p>\n</blockquote>\n<h2>Conclusion</h2>\n<blockquote>\n<p>They are, however, what my observations lead me to believe are the core of excellence for software development teams.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Real customer involvement, bringing real customer closer to development eases to build the right product. It reduces the chances of building something no one will use. Be transparent with the customer. Don't push away the customer because you think your product will too specific, or because you think they won't trust you.</p>\n<p>Incremental Deployment, instead of a big bang, gradually replace a legacy system.</p>\n<p>Team Continuity, keep effective teams together. Software is created not only by what people do, but also because of their relationship and how well they know each other.</p>\n<p>Shrinking Teams, the team should focus on reduce waste so that they can reduce the team's size to create another teams. If eventually the team reduced so many waste that is too little, you can also regroup it.</p>\n<p>Root-Cause Analysis, eliminate the defects in its root. Not only the defect should happen again but the mistake that created the defect as well. Apply the 5 whys to find the root cause analysis?</p>\n<p>Shared Code, everyone is owner of the code, you be able to contribute to any part of the system. The team is collectively responsible for the quality of the system.</p>\n<p>Code and Tests, code and test are the only artifacts. Ceremonies and anything else is a form of waste.</p>\n<p>Single Code Base, use only one mainline of code. Avoid split code based on different versions simultaneously.</p>\n<p>Daily Deployment, the faster you put in production earlier you get feedback. Avoid big diffs of code, they are riskier, deploy are harder and you don't get feedback. You can deploy daily, and only in the last day you can change the switch that impacts the customer.</p>\n<p>Negotiated Scope Contract, projects have fixed time, cost and quality. you negotiate its scope. Use it to shape the project to deliver the best value.</p>\n<p>Pay-per-Use, it's a great model that benefits both user and provider. The features more used are the one that you get more out of it.</p>\n<p>These techniques are better once you have mastered primary practices.</p>\n<h1>Chapter 10. The Whole XP Team</h1>\n<blockquote>\n<p>The XP practice Whole Team suggests that a variety of people work together in interlinking ways to make a project more effective. They have to work together as a group for each to be successful.</p>\n</blockquote>\n<blockquote>\n<p>The principle of flow suggests that more value is created in a smooth, steady stream of software than in occasional large deployments.</p>\n</blockquote>\n<blockquote>\n<p>There was no happy ending to this story. You can't be convinced against your will.</p>\n</blockquote>\n<blockquote>\n<p>It didn't really matter who was first . What the whole team was missing was a sense that they were roped together.</p>\n</blockquote>\n<h2>Testers</h2>\n<blockquote>\n<p>Testers on an XP team help customers choose and write automated system-level tests in advance of implementation and coach programmers on testing techniques.</p>\n</blockquote>\n<blockquote>\n<p>The role of testers shifts to early in development, helping define and specify what will constitute acceptable functioning of the system before the functionality as been implemented.</p>\n</blockquote>\n<h2>Interaction Designers</h2>\n<blockquote>\n<p>Addressing the concerns of eventual users is a priority for the team. The tools of interaction design, such as personas and goals, help the team analyze and make sense of the world of the user, although they are no substitute for conversation with real people.</p>\n</blockquote>\n<h2>Architects</h2>\n<blockquote>\n<p>Making big architectural changes in small, safe steps is one of the challenges for an XP team.</p>\n</blockquote>\n<blockquote>\n<p>The principle of the alignment of authority and responsibility suggests that it is a bad idea to give one person the power to make decisions that others have to follow without having to personally live with the consequences.</p>\n</blockquote>\n<blockquote>\n<p>Another task for architects on an XP team is partitioning systems. Partitioning isn't an up-front, once-and-for-all task, though. Rather than divide and conquer, an XP team conquers and divides.</p>\n</blockquote>\n<h2>Project Managers</h2>\n<blockquote>\n<p>To remain accurate, the information changes frequently; which gives project managers the challenge of communicating changes helpfully.</p>\n</blockquote>\n<blockquote>\n<p>Planning in XP is an activity, not a phase. Project managers are responsible for keeping plans synchronized with reality.</p>\n</blockquote>\n<blockquote>\n<p>To facilitate communication, they introduce the right person on the team to the right person outside the team as needed, rather than act as a communication bottleneck.</p>\n</blockquote>\n<h2>Product Manager</h2>\n<blockquote>\n<p>A plan in XP is an example of what <em>could</em> happen, not a prediction of what <em>will</em> happen.</p>\n</blockquote>\n<blockquote>\n<p>Product managers encourage communication between customers and programmers, making sure the most important customer concerns are heard and acted on by the team.</p>\n</blockquote>\n<h2>Executives</h2>\n<blockquote>\n<p>Another job for executives sponsoring or overseeing XP teams is monitoring, encouraging and facilitating improvement.</p>\n</blockquote>\n<blockquote>\n<p>Executives should expect honesty and clear explanations of options from the team in any decision-making process.</p>\n</blockquote>\n<blockquote>\n<p>I trust two metrics to measure the health of XP teams. The first is the number of defects found after development. An XP team should have dramatically fewer defects in its first deployment and make rapid progress from there.</p>\n</blockquote>\n<blockquote>\n<p>The second metric I use is the time lag between the beginning of investment in an idea and when the idea first generates revenue.</p>\n</blockquote>\n<blockquote>\n<p>Part of the executive's job is presenting the team positively to the rest of the organization.</p>\n</blockquote>\n<h2>Technical Writers</h2>\n<blockquote>\n<p>Explaining the system in prose and pictures is an important link in the chain that creates feedback for the team.</p>\n</blockquote>\n<blockquote>\n<p>What would be the perfect documentation? It would be done exactly when the features it documents are done. It would be trivial to update when it was found in need of improvement. It would be cheap. It wouldn't add any time to the basic development cycle. It would be valuable to the users. The writing of it would be valuable to the team.</p>\n</blockquote>\n<blockquote>\n<p>If users never look at a certain kind of documentation, stop writing it. You can find better ways to invest that time.</p>\n</blockquote>\n<h2>Human Resources</h2>\n<blockquote>\n<p>The problem with reviews is that most reviews and raises are based on individual goals and achievements, but XP focuses on team performance.</p>\n</blockquote>\n<blockquote>\n<p>Hiring for XP teams can differ from existing hiring practices. XP teams put much more emphasis on teamwork and social skills. Given the choice between an extremely skilled loner and a competent-but-social programmer, XP teams consistently choose the more social candidate.</p>\n</blockquote>\n<h2>Roles</h2>\n<blockquote>\n<p>The goal is not for people to fill abstract roles, but for each team member to contribute all he can to the team.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>The whole team must collaborate and work together in order to achieve the better results. A whole team is composed by a set of people with different skills each.</p>\n<p>Testers, act together with the customers to identify scenarios and the expected behavior. They also help the team to achieve better quality in software.</p>\n<p>Interaction designers, identify gaps in the system and try to improve it. Applying techniques that map the customer to personas, this role acts together with customer.</p>\n<p>The role of the architect is to drive the architecture of the system. The architect must not be a wizard in the top of its white tower taking decisions. It must for sure include the team in the process of evolving the system. Having one person responsible for all the decisions is a bad idea. Also, keep with small changes rather than big changes.</p>\n<p>The project manager acts as a facilitator as well as a the responsible for communicating the progress of the project. It's important they are together with the team to synchronize the plans with the reality.</p>\n<p>The product manager encourages the team to talk to the customers. The product manager does the priorization based on what will deliver more value in the current iteration. The items to be worked are created with focus on the business, so you have since first week of development working software.</p>\n<p>Executive's job is to sponsor XP for the whole organization. The role helps in the decision making and monitors the organization through the number of defects and the time between concept to production.</p>\n<p>Technical writers are responsible for documenting the system. The documentation cannot be expensive to create and maintain, and will be available as soon as the software is out. Just enough documentation.</p>\n<p>HR focuses in hiring and the performance review. Hiring must focus on the social part and the performance review must consider the whole team, not individual performance.</p>\n<p>There are other roles, these are some few to demonstrate what are the roles in a whole team. The important part is to each team identify which role are more required to achieve better results.</p>\n<h1>Chapter 11. The Theory of Constraints</h1>\n<blockquote>\n<p>XP isn't intended to solve marketing, sales or management problems. Its not that non-software bottlenecks aren't important, it's just that XP doesn't spread that thin.</p>\n</blockquote>\n<blockquote>\n<p>Theory of constraints says that in any system there is one constraint at a time (occasionally two). To improve the overall system throughput you have to first find the constraint; make sure it is working full speed; then find ways of either increasing the capacity of the constraint, offloading some of the work onto non-constraints, or eliminating the constraint entirely.</p>\n</blockquote>\n<blockquote>\n<p>How do you find the constraint in a system? Work piles up in front of the constraint.</p>\n</blockquote>\n<blockquote>\n<p>Micro-optimizations is never enough. To improve our results we must look at the whole situation before deciding what to change.</p>\n</blockquote>\n<blockquote>\n<p>XP uses a \"pull\" model. Stories are specified in detail immediately before they are implemented. The tests are pulled from the specification. The programming interface is designed to match the needs of the test. The code is written to match the tests and the interface. The design is refined to match the needs of the code as written.</p>\n</blockquote>\n<blockquote>\n<p>The theory of constraints shares with other theories of organizational change the assumption that the whole organization is focused on overall throughput, not on micro-optimizations.</p>\n</blockquote>\n<blockquote>\n<p>If the bottleneck exists outside of software development, the answer must come from outside of software development.</p>\n</blockquote>\n<blockquote>\n<p>Executive sponsorship and strong relationships with people outside the team are crucial to applying XP, precisely because applying XP will shift the structure of work in the rest of the organization as soon as software development gets its act together. If you don't have executive sponsorship, be prepared to do a better job yourself without recognition or protection.</p>\n</blockquote>\n<h2>My Summary</h2>\n<h1>Chapter 12. Planning: Managing Scope</h1>\n<blockquote>\n<p>Planning in XP starts with putting the current goals, assumptions, and facts on the table. With current, explicit information, you can work toward agreement about what's in scope, what's out of scope, and what to do next.</p>\n</blockquote>\n<blockquote>\n<p>Planning is complicated because the estimates of the cost and value of stories are uncertain. The information on which you base these decisions changes.</p>\n</blockquote>\n<blockquote>\n<p>Plans are not prediction of the future. At best, they express everything you know today about what might happen tomorrow. Their uncertainty doesn't negate their value.</p>\n</blockquote>\n<blockquote>\n<p>I learned three variables by which to manage projects: speed, quality and price. The sponsor gets to fix two of these variables and the team gets to estimate the third. If the plan is unacceptable, the negotiating starts.</p>\n<p>This model doesn't work well in practice. Time and costs are generally set outside the project. That leaves quality as the only variable you can manipulate. Lowering the quality of your work doesn't eliminate work, it just shifts it later so delays are not clearly your responsibility.</p>\n</blockquote>\n<blockquote>\n<p>Planning is something we do together. It requires cooperation. Planning is an exercise in listening, speaking and aligning goals for a specific time period.</p>\n</blockquote>\n<blockquote>\n<p>To estimate a story, imagine, given everything you know about similar stories, how many hours or days it will take a pair to complete the story. \"Complete\" means ready for deployment; including all the testing, implementation, refactoring, and discussions with the users.</p>\n</blockquote>\n<blockquote>\n<p>Estimates based on experience are more accurate.</p>\n</blockquote>\n<blockquote>\n<p>When things aren't going well is when we most need to adhere to our values and principles and modify our practices to remain as effective as possible. Inaccurate estimates are a failure of information, not of values or principles.</p>\n</blockquote>\n<blockquote>\n<p>It is the interaction around the stories that makes them valuable. The cards are a tool.</p>\n</blockquote>\n<h2>My Summary</h2>\n<h1>Chapter 13. Testing: Early, Often and Automated</h1>","frontmatter":{"title":"Extreme Programming Explained 2nd","language":"en-US","coverPath":"extreme-programming-explained-2nd","status":"Read","date":"2021-06-20"}}},{"node":{"html":"<h1>Foreword</h1>\n<blockquote>\n<p>Tools were only components in processes, working alongside chains of software, people, and data. Nothing here tells us how to solve problems universally, but that is the point. Stories like these are far more valuable than the code or designs they resulted in. Implementations are ephemeral, but the documented reasoning is priceless. Rarely do we have access to this kind of insight.</p>\n</blockquote>\n<h1>Preface</h1>\n<blockquote>\n<p>We apply the principles of computer science and engineering to the design and development of computing systems: generally, large distributed ones. Sometimes, our task is writing the software for those systems alongside our product development counterparts; sometimes, our task is building all the additional pieces those systems need, like backups or load balancing, ideally so they can be reused across systems; and sometimes, our task is figuring out how to apply existing solutions to new problems.</p>\n</blockquote>\n<blockquote>\n<p>a system isn’t very useful if nobody can use it! Because reliability is so critical, SREs are focused on finding ways to improve the design and operation of systems to make them more scalable, more reliable, and more efficient.</p>\n</blockquote>\n<blockquote>\n<p> it’s still worth putting lightweight reliability support in place early on, because it’s less costly to expand a structure later on than it is to introduce one that is not present.</p>\n</blockquote>\n<h1>Part I - Introduction</h1>\n<h2>Chapter 1 - Introduction</h2>\n<h3>The Sysadmin Approach to Service Management</h3>\n<blockquote>\n<p>Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.</p>\n</blockquote>\n<blockquote>\n<p>At their core, the development teams want to launch new features and see them adopted by users. At <em>their</em> core, the ops teams want to make sure the service doesn’t break while they are holding the pager. Because most outages are caused by some kind of change—a new configuration, a new feature launch, or a new type of user traffic—the two teams’ goals are fundamentally in tension.</p>\n</blockquote>\n<blockquote>\n<p>The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.</p>\n</blockquote>\n<h3>Google’s Approach to Service Management: Site Reliability Engineering</h3>\n<blockquote>\n<p>Site Reliability Engineering teams focus on hiring software engineers to run our products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.</p>\n</blockquote>\n<blockquote>\n<p>SRE is what happens when you ask a software engineer to design an operations team.</p>\n</blockquote>\n<blockquote>\n<p>Common to all SREs is the belief in and aptitude for developing software systems to solve complex problems.</p>\n</blockquote>\n<blockquote>\n<p>The result of our approach to hiring for SRE is that we end up with a team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.</p>\n</blockquote>\n<blockquote>\n<p>By design, it is crucial that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload.</p>\n</blockquote>\n<blockquote>\n<p>Google places <em>a 50% cap on the aggregate \"ops\" work for all SREs</em>—tickets, on-call, manual tasks, etc.</p>\n</blockquote>\n<blockquote>\n<p>Because SREs are directly modifying code in their pursuit of making Google’s systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. Such teams are relatively inexpensive—supporting the same service with an ops-oriented team would require a significantly larger number of people. </p>\n</blockquote>\n<h3>DevOps or SRE?</h3>\n<blockquote>\n<p>One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.</p>\n</blockquote>\n<h3>Tenets of SRE</h3>\n<blockquote>\n<p>SRE team is responsible for the <em>availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning</em> of their service(s).</p>\n</blockquote>\n<h4>Ensuring a Durable Focus on Engineering</h4>\n<blockquote>\n<p> by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on.</p>\n</blockquote>\n<blockquote>\n<p>Postmortems should be written for all significant incidents, regardless of whether or not they paged</p>\n</blockquote>\n<blockquote>\n<p>This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time</p>\n</blockquote>\n<blockquote>\n<p>Google operates under a <em>blame-free postmortem culture</em>, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.</p>\n</blockquote>\n<h4>Pursuing Maximum Change Velocity Without Violating a Service’s SLO</h4>\n<blockquote>\n<p>thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>What level of availability will the users be happy with, given how they use the product?</li>\n<li>What alternatives are available to users who are dissatisfied with the product’s availability?</li>\n<li>What happens to users’ usage of the product at different availability levels?</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Once that target is established, the error budget is one minus the availability target.</p>\n</blockquote>\n<blockquote>\n<p>An outage is no longer a \"bad\" thing—it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear.</p>\n</blockquote>\n<h4>Monitoring</h4>\n<blockquote>\n<p>Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.</p>\n</blockquote>\n<h4>Alerts</h4>\n<blockquote>\n<p>Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.</p>\n</blockquote>\n<h4>Tickets</h4>\n<blockquote>\n<p>Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.</p>\n</blockquote>\n<h4>Logging</h4>\n<blockquote>\n<p>No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.</p>\n</blockquote>\n<h4>Emergency Response</h4>\n<blockquote>\n<p> Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). The most relevant metric in evaluating the effectiveness of emergency response is how quickly the response team can bring the system back to health—that is, the MTTR.</p>\n</blockquote>\n<blockquote>\n<p>Humans add latency. Even if a given system experiences more <em>actual</em> failures, a system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention</p>\n</blockquote>\n<blockquote>\n<p>When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a \"playbook\" produces roughly a 3x improvement in MTTR as compared to the strategy of \"winging it.\"</p>\n</blockquote>\n<h4>Change Management</h4>\n<blockquote>\n<p>SRE has found that roughly 70% of outages are due to changes in a live system</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Implementing progressive rollouts</li>\n<li>Quickly and accurately detecting problems</li>\n<li>Rolling back changes safely when problems arise</li>\n</ul>\n</blockquote>\n<h4>Demand Forecasting and Capacity Planning</h4>\n<blockquote>\n<p>Capacity planning should take both organic growth (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.</p>\n</blockquote>\n<h4>Provisioning</h4>\n<blockquote>\n<p>Adding new capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results. Thus, it is a riskier operation than load shifting, which is often done multiple times per hour, and must be treated with a corresponding degree of extra caution.</p>\n</blockquote>\n<h4>Efficiency and Performance</h4>\n<blockquote>\n<p>Resource use is a function of demand (load), capacity, and software efficiency.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Historically, companies use the sysadmin model. Which is responsible for dividing the Developers and Operations. At its core developers and operations have different goals. Developers want to deliver code to production as fast as possible. Operations wants to have a reliable system. Since, changes might cause outage, there is a tension between developers and operations goals. This model is also expensive due to the need to add more people as the software scales.</p>\n<p>Google's SRE teams are composed of software engineers hired to do operations. These teams have the skill to automate the process and can be rapidly bored by repetitive tasks. Therefore, creating a great combination for continuous improvement.</p>\n<p>The SRE skills are very similar to developers skills, UNIX and networking being the main differentiators.</p>\n<p>It's important that these teams focus on engineering. Otherwise, it will be required to increase the staff of the teams. To achieve the focus on engineering, Google provides a cap of 50% of work for engineering. If they workload on operations exceeds this number, bugs, on-call and so on are redirected to the development team.</p>\n<p>SRE can be considered a Google's implementation of DevOps.</p>\n<p>SRE team is responsible for the:</p>\n<ul>\n<li>\n<p>Availability</p>\n<p>There's no gain to have a 100% reliable system, because there are other players into play with lower reliability (WiFi, ISP, power grid, ...)</p>\n<p>The availability is a product decision. The error budget is value of permitted unavailability. Developers may use this budget to speed up releases. The aim is not to have zero outages, but to not exceed the error budget.</p>\n<p>Postmortem it's important to expose faults and improve systems. A culture of fixing errors instead of hiding or minimizing them.</p>\n</li>\n<li>Latency</li>\n<li>Efficiency and Performance</li>\n<li>\n<p>Change Management:</p>\n<ul>\n<li>Implementing progressive rollouts</li>\n<li>Quickly and accurately detecting problems</li>\n<li>Rolling back changes safely when problems arise</li>\n</ul>\n</li>\n<li>\n<p>Monitoring: </p>\n<p>Keep track of a system’s health and availability. Software monitors human are notified when need to take action.</p>\n<ul>\n<li>Alerts: Human needs to take an action immediately in response to something that is either happening or about to happen.</li>\n<li>Tickets: a human needs to take action, but not immediately. If a human takes action in a few days, no damage will result</li>\n<li>Logging: Recorded for diagnostic or forensic purposes. People read the logs when something prompts them to do so.</li>\n</ul>\n</li>\n<li>\n<p>Emergency Response:</p>\n<p>How quickly the response team can bring the system back to health. Adding a \"playbook\" produces roughly a 3x improvement.</p>\n<ul>\n<li>MTTR (Repair)</li>\n<li>MTTF (Failure)</li>\n</ul>\n</li>\n<li>\n<p>Capacity Planning: </p>\n<p>There is sufficient capacity and redundancy to serve projected future demand with the required availability</p>\n<ul>\n<li>Organic growth (natural adoption)</li>\n<li>Inorganic growth (feature launches, marketing campaign, ...)</li>\n</ul>\n</li>\n</ul>\n<h2>Chapter 2 - The Production Environment at Google, from the Viewpoint of an SRE</h2>\n<h3>System Software That \"Organizes\" the Hardware</h3>\n<blockquote>\n<p>Given the large number of hardware components in a cluster, hardware failures occur quite frequently. In a single cluster in a typical year, thousands of machines fail and thousands of hard disks break; when multiplied by the number of clusters we operate globally, these numbers become somewhat breathtaking.</p>\n</blockquote>\n<h4>Networking</h4>\n<blockquote>\n<p>In order to minimize latency for globally distributed services, we want to direct users to the closest datacenter with available capacity.</p>\n</blockquote>\n<h3>Our Development Environment</h3>\n<blockquote>\n<p>When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This chapter they describe Google's Environment.</p>\n<p>The cluster operating system Borg handles resource allocation. Borg is the precedent of Kubernetes. r</p>\n<h1>Part II - Principles</h1>\n<h2>Chapter 3 - Embracing Risk</h2>\n<blockquote>\n<p>It turns out that past a certain point, however, increasing reliability is worse for a service (and its users) rather than better! Extreme reliability comes at a cost: maximizing stability limits how fast new features can be developed and how quickly products can be delivered to users, and dramatically increases their cost, which in turn reduces the numbers of features a team can afford to offer</p>\n</blockquote>\n<blockquote>\n<p> Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient service operations, so that users’ overall happiness—with features, service, and performance—is optimized.</p>\n</blockquote>\n<h3>Managing Risks</h3>\n<blockquote>\n<p>We strive to make a service reliable enough, but no <em>more</em> reliable than it needs to be.</p>\n</blockquote>\n<blockquote>\n<p>That is, when we set an availability target of 99.99%,we want to exceed it, but not by much: that would waste opportunities to add features to the system, clean up technical debt, or reduce its operational costs.</p>\n</blockquote>\n<h3>Measuring Service Risk</h3>\n<blockquote>\n<p>As standard practice at Google, we are often best served by identifying an objective metric to represent the property of a system we want to optimize. By setting a target, we can assess our current performance and track improvements or degradations over time.</p>\n</blockquote>\n<blockquote>\n<p>For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime.</p>\n</blockquote>\n<h4>Time-based availability</h4>\n<blockquote>\n<p>Availability = uptime / (uptime + downtime)</p>\n</blockquote>\n<h4>Aggregate availability</h4>\n<blockquote>\n<p> Availability = successful request / total requests</p>\n</blockquote>\n<blockquote>\n<p>Most often, we set quarterly availability targets for a service and track our performance against those targets on a weekly, or even daily, basis. This strategy lets us manage the service to a high-level availability objective by looking for, tracking down, and fixing meaningful deviations as they inevitably arise</p>\n</blockquote>\n<h3>Risk Tolerance of Services</h3>\n<blockquote>\n<p>SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer. In this case, the business goals we’re concerned about have a direct impact on the performance and reliability of the service offered.</p>\n</blockquote>\n<h4>Identifying the Risk Tolerance of Consumer Services</h4>\n<blockquote>\n<ul>\n<li>What level of availability is required?</li>\n<li>Do different types of failures have different effects on the service?</li>\n<li>How can we use the service cost to help locate a service on the risk continuum?</li>\n<li>What other service metrics are important to take into account?</li>\n</ul>\n</blockquote>\n<h4>Target level of availability</h4>\n<blockquote>\n<ul>\n<li>What level of service will the users expect?</li>\n<li>Does this service tie directly to revenue (either our revenue, or our customers’ revenue)?</li>\n<li>Is this a paid service, or is it free?</li>\n<li>If there are competitors in the marketplace, what level of service do those competitors provide?</li>\n<li>Is this service targeted at consumers, or at enterprises?</li>\n</ul>\n</blockquote>\n<h4>Types of failures</h4>\n<blockquote>\n<p>Which is worse for the service: a constant low rate of failures, or an occasional full-site outage? Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.</p>\n</blockquote>\n<blockquote>\n<p>Because most of this work takes place during normal business hours, we determined that occasional, regular, scheduled outages in the form of maintenance windows would be acceptable, and we counted these scheduled outages as planned downtime, not unplanned downtime.</p>\n</blockquote>\n<h4>Cost</h4>\n<blockquote>\n<p>It may be harder to set these targets when we do not have a simple translation function between reliability and revenue.</p>\n</blockquote>\n<h3>Identifying the Risk Tolerance of Infrastructure Services</h3>\n<blockquote>\n<p>A fundamental difference is that, by definition, infrastructure components have multiple clients, often with varying needs.</p>\n</blockquote>\n<h3>Motivation for Error Budgets</h3>\n<blockquote>\n<p>The product developers have more visibility into the time and effort involved in writing and releasing their code, while the SREs have more visibility into the service’s reliability (and the state of production in general).</p>\n</blockquote>\n<blockquote>\n<p>Instead, our goal is to define an objective metric, agreed upon by both sides, that can be used to guide the negotiations in a reproducible way.</p>\n</blockquote>\n<h4>Forming Your Error Budget</h4>\n<blockquote>\n<p>The error budget provides a clear, objective metric that determines how unreliable the service is allowed to be within a single quarter.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Product Management defines an SLO, which sets an expectation of how much uptime the service should have per quarter.</li>\n<li>The actual uptime is measured by a neutral third party: our monitoring system.</li>\n<li>The difference between these two numbers is the \"budget\" of how much \"unreliability\" is remaining for the quarter.</li>\n<li>As long as the uptime measured is above the SLO—in other words, as long as there is error budget remaining—new releases can be pushed.</li>\n</ul>\n</blockquote>\n<h3>Benefits</h3>\n<blockquote>\n<p>The main benefit of an error budget is that it provides a common incentive that allows both product development and SRE to focus on finding the right balance between innovation and reliability.</p>\n</blockquote>\n<blockquote>\n<p>If SLO violations occur frequently enough to expend the error budget, releases are temporarily halted while additional resources are invested in system testing and development to make the system more resilient, improve its performance, and so on.</p>\n</blockquote>\n<blockquote>\n<p> If the team is having trouble launching new features, they may elect to loosen the SLO (thus increasing the error budget) in order to increase innovation.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>The cost:</p>\n<ul>\n<li>The redundant compute resources</li>\n<li>The opportunity cost</li>\n</ul>\n<p>Typical tension:</p>\n<ul>\n<li>Software fault tolerance</li>\n<li>Testing</li>\n<li>Push frequency</li>\n<li>Canary duration and size</li>\n</ul>\n<h2>Chapter 4 - Service Level Objectives</h2>\n<blockquote>\n<p>We use intuition, experience, and an understanding of what users want to define <em>service level indicators</em> (SLIs), <em>objectives</em> (SLOs), and <em>agreements</em> (SLAs). These measurements describe basic properties of metrics that matter, what values we want those metrics to have, and how we’ll react if we can’t provide the expected service.</p>\n</blockquote>\n<h4>Indicators</h4>\n<blockquote>\n<p>An SLI is a service level <em>indicator</em>—a carefully defined quantitative measure of some aspect of the level of service that is provided.</p>\n</blockquote>\n<blockquote>\n<p>Most services consider <em>request latency</em>—how long it takes to return a response to a request—as a key SLI. Other common SLIs include the <em>error rate</em>, often expressed as a fraction of all requests received, and <em>system throughput</em>, typically measured in requests per second.</p>\n</blockquote>\n<blockquote>\n<p>Another kind of SLI important to SREs is <em>availability</em>, or the fraction of the time that a service is usable. It is often defined in terms of the fraction of well-formed requests that succeed, sometimes called <em>yield</em>.</p>\n</blockquote>\n<blockquote>\n<p>Although 100% availability is impossible, near-100% availability is often readily achievable, and the industry commonly expresses high-availability values in terms of the number of \"nines\" in the availability percentage.</p>\n</blockquote>\n<h4>Objectives</h4>\n<blockquote>\n<p>An SLO is a <em>service level objective</em>: a target value or range of values for a service level that is measured by an SLI.</p>\n</blockquote>\n<blockquote>\n<p><em>SLI ≤ target</em>, or <em>lower bound ≤ SLI ≤ upper bound</em>.</p>\n</blockquote>\n<blockquote>\n<p>Choosing and publishing SLOs to users sets expectations about how a service will perform. This strategy can reduce unfounded complaints to service owners about, for example, the service being slow.</p>\n</blockquote>\n<h4>Agreements</h4>\n<blockquote>\n<p>Finally, SLAs are service level <em>agreements</em>: an explicit or implicit contract with your users that includes consequences of meeting (or missing) the SLOs they contain.</p>\n</blockquote>\n<blockquote>\n<p>SRE does, however, get involved in helping to avoid triggering the consequences of missed SLOs</p>\n</blockquote>\n<h3>Indicators in Practice</h3>\n<h4>What Do You and Your Users Care About?</h4>\n<blockquote>\n<p>Choosing too many indicators makes it hard to pay the right level of attention to the indicators that matter, while choosing too few may leave significant behaviors of your system unexamined</p>\n</blockquote>\n<h4>Collecting Indicators</h4>\n<blockquote>\n<p>However, some systems should be instrumented with <em>client</em>-side collection, because not measuring behavior at the client can miss a range of problems that affect users but don’t affect server-side metrics.</p>\n</blockquote>\n<h4>Aggregation</h4>\n<blockquote>\n<p>Most metrics are better thought of as <em>distributions</em> rather than averages. </p>\n</blockquote>\n<blockquote>\n<p>Using percentiles for indicators allows you to consider the shape of the distribution and its differing attributes: a high-order percentile, such as the 99th or 99.9th, shows you a plausible worst-case value, while using the 50th percentile (also known as the median) emphasizes the typical case.</p>\n</blockquote>\n<blockquote>\n<p>User studies have shown that people typically prefer a slightly slower system to one with high variance in response time, so some SRE teams focus only on high percentile values, on the grounds that if the 99.9th percentile behavior is good, then the typical experience is certainly going to be.</p>\n</blockquote>\n<blockquote>\n<p>We recommend that you standardize on common definitions for SLIs so that you don’t have to reason about them from first principles each time.</p>\n</blockquote>\n<h3>Objectives in Practice</h3>\n<blockquote>\n<p>Start by thinking about (or finding out!) what your users care about, not what you can measure. Often, what your users care about is difficult or impossible to measure, so you’ll end up approximating users’ needs in some way.</p>\n</blockquote>\n<h4>Defining Objectives</h4>\n<blockquote>\n<p>SLOs should specify how they’re measured and the conditions under which they’re valid.</p>\n</blockquote>\n<blockquote>\n<p>It’s both unrealistic and undesirable to insist that SLOs will be met 100% of the time: doing so can reduce the rate of innovation and deployment, require expensive, overly conservative solutions, or both. Instead, it is better to allow an error budget—a rate at which the SLOs can be missed—and track that on a daily or weekly basis. Upper management will probably want a monthly or quarterly assessment, too.</p>\n</blockquote>\n<h4>Have as few SLOs as possible</h4>\n<blockquote>\n<p>Choose just enough SLOs to provide good coverage of your system’s attributes.</p>\n</blockquote>\n<h4>Perfection can wait</h4>\n<blockquote>\n<p>You can always refine SLO definitions and targets over time as you learn about a system’s behavior</p>\n</blockquote>\n<blockquote>\n<p>SLOs can—and should—be a major driver in prioritizing work for SREs and product developers, because they reflect what users care abou</p>\n</blockquote>\n<h4>Control Measure</h4>\n<blockquote>\n<ol>\n<li>Monitor and measure the system’s SLIs.</li>\n<li>Compare the SLIs to the SLOs, and decide whether or not action is needed.</li>\n<li>If action is needed, figure out <em>what</em> needs to happen in order to meet the target.</li>\n<li>Take that action.</li>\n</ol>\n</blockquote>\n<h4>Keep a safety margin</h4>\n<blockquote>\n<p>Using a tighter internal SLO than the SLO advertised to users gives you room to respond to chronic problems before they become visible externally. </p>\n</blockquote>\n<h2>My Summary</h2>\n<ul>\n<li><em>User-facing serving systems</em>: <em>availability</em>, <em>latency</em>, and <em>throughput</em>.</li>\n<li><em>Storage systems</em>: <em>latency</em>, <em>availability</em>, and <em>durability</em>.</li>\n<li><em>Big data systems</em>: <em>Throughput</em> and <em>end-to-end latency</em></li>\n</ul>\n<h2>Chapter 5 - Eliminating Toil</h2>\n<blockquote>\n<p>In SRE, we want to spend time on long-term engineering project work instead of operational work.</p>\n</blockquote>\n<h3>Toil Defined</h3>\n<blockquote>\n<p>Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.</p>\n</blockquote>\n<blockquote>\n<p> If you’re solving a novel problem or inventing a new solution, this work is not toil.</p>\n</blockquote>\n<blockquote>\n<p>Toil is interrupt-driven and reactive, rather than strategy-driven and proactive.</p>\n</blockquote>\n<h3>Why Less Toil Is Better</h3>\n<blockquote>\n<p>At least 50% of each SRE’s time should be spent on engineering project work that will either reduce future toil or add service features.</p>\n</blockquote>\n<blockquote>\n<p>The work of reducing toil and scaling up services is the \"Engineering\" in Site Reliability Engineering.</p>\n</blockquote>\n<h3>What Qualifies as Engineering?</h3>\n<blockquote>\n<p>It produces a permanent improvement in your service, and is guided by a strategy. It is frequently creative and innovative, taking a design-driven approach to solving a problem—the more generalized, the better.</p>\n</blockquote>\n<h3>Is Toil Always Bad?</h3>\n<blockquote>\n<p>Toil doesn’t make everyone unhappy all the time, especially in small amounts.</p>\n</blockquote>\n<blockquote>\n<p>Toil becomes toxic when experienced in large quantities.</p>\n</blockquote>\n<h3>My Summary</h3>\n<ul>\n<li>Overhead:  Administrative tasks such team meetings, setting and grading goals, HR paperwork</li>\n<li>Grungy work: An example is cleaning the entire alert configuration ( manual, and most likely boring ), yet, it adds value.</li>\n<li>\n<p>Toil: Work tied to a running a production service that is manual, repetitive, automatable, tactical, devoid of enduring value, and scales linearly as service grows.</p>\n<ul>\n<li>Manual: Running scripts manually.</li>\n<li>Repetitive: Work you do over and over. If you’re solving a novel problem or inventing a new solution, this work is not toil.</li>\n<li>Automatable: Either a machine could execute or a better design solves the problem.</li>\n<li>Tactical:  Toil is interrupt-driven and reactive, rather than strategy-driven and proactive.</li>\n<li>No enduring value: If your service remains in the same state after you have finished a task</li>\n<li>O(n) with service growth: If work scales up linearly with service size, traffic volume, or user count, that task is probably toil.</li>\n</ul>\n</li>\n</ul>\n<p>Engineering:</p>\n<ul>\n<li>Software: Code, tests and its documentation. Automation scripts, tools or frameworks, scalability and reliability, changing infrastructure.</li>\n<li>System: Configuring production system, improvements for an one-time effort.</li>\n<li>Toil: Work to a specific service, manual and repetitive.</li>\n<li>Overhead: other meetings</li>\n</ul>\n<p>Toil leads to career stagnation and low morale. In addition to less productivity, precedents, attrition </p>\n<h2>Chapter 6 - Monitoring Distributed Systems</h2>\n<h3>Why Monitor?</h3>\n<blockquote>\n<p>When pages occur too frequently, employees second-guess, skim, or even ignore incoming alerts, sometimes even ignoring a \"real\" page that’s masked by the noise.</p>\n</blockquote>\n<h3>Setting Reasonable Expectations for Monitoring</h3>\n<blockquote>\n<p>a Google SRE team with 10–12 members typically has one or sometimes two members whose primary assignment is to build and maintain monitoring systems for their service.</p>\n</blockquote>\n<h3>Symptoms Versus Causes</h3>\n<blockquote>\n<p>Your monitoring system should address two questions: what’s broken, and why?</p>\n</blockquote>\n<blockquote>\n<p>The \"what’s broken\" indicates the symptom; the \"why\" indicates a (possibly intermediate) cause.</p>\n</blockquote>\n<h3>Black-Box Versus White-Box</h3>\n<blockquote>\n<p>We combine heavy use of white-box monitoring with modest but critical uses of black-box monitoring.</p>\n</blockquote>\n<blockquote>\n<p>Note that in a multilayered system, one person’s symptom is another person’s cause.</p>\n</blockquote>\n<h3>My Summary</h3>\n<ul>\n<li>\n<p>Monitoring</p>\n<ul>\n<li>Whitebox: Internally, JVM or http handler</li>\n<li>Blackbox: externally</li>\n</ul>\n</li>\n</ul>\n<p>Why?</p>\n<ul>\n<li>Long term trend</li>\n<li>Comparing over time or results experiments</li>\n<li>Alerting</li>\n<li>Debugging</li>\n</ul>","frontmatter":{"title":"Site Reliability Engineering","language":"en-US","coverPath":"site-reliability-engineering","status":"Reading","date":"2021-05-07"}}},{"node":{"html":"<h1>Chapter 1 - Escaping monolithic hell</h1>\n<blockquote>\n<p>(Abour Big ball of Mud) “haphazardly structured, sprawling, sloppy, duct-tape and bailing wire, spaghetti code jungle.”</p>\n</blockquote>\n<blockquote>\n<p>Despite having a logically modular architecture, ... The application is an example of the widely used monolithic style of software architecture, which structures a system as a single executable or deployable component.</p>\n</blockquote>\n<blockquote>\n<p>On the one hand, a disciplined team can slow down the pace of its descent toward monolithic hell. Team members can work hard to maintain the modularity of their application. They can write comprehensive automated tests. On the other hand, they can’t avoid the issues of a large team working on a single monolithic application. Nor can they solve the problem of an increasingly obsolete technology stack. The best a team can do is delay the inevitable.</p>\n</blockquote>\n<blockquote>\n<p>Adrian Cockcroft, formerly of Netflix, defines a microservice architecture as a service-oriented architecture composed of loosely coupled elements that have bounded contexts.</p>\n</blockquote>\n<blockquote>\n<p>Note that this definition doesn’t say anything about size. Instead, what matters is that each service has a focused, cohesive set of responsibilities.</p>\n</blockquote>\n<blockquote>\n<p>The microservice architecture uses services as the unit of modularity. A service has an API, which is an impermeable boundary that is difficult to violate. You can’t bypass the API and access an internal class as you can with a Java package.</p>\n</blockquote>\n<blockquote>\n<p>A key characteristic of the microservice architecture is that the services are loosely coupled and communicate only via APIs. One way to achieve loose coupling is by each service having its own datastore.</p>\n</blockquote>\n<blockquote>\n<p>Each one can be independently developed, tested, deployed, and scaled. Also, this architecture does a good job of preserving modularity.</p>\n</blockquote>\n<blockquote>\n<p>One challenge with using the microservice architecture is that there isn’t a concrete, well-defined algorithm for decomposing a system into services.</p>\n</blockquote>\n<blockquote>\n<p>if you decompose a system incorrectly, you’ll build a distributed monolith, a system consisting of coupled services that must be deployed together.</p>\n</blockquote>\n<blockquote>\n<p>The microservice architecture also introduces significant operational complexity. Many more moving parts—multiple instances of different types of service—must be managed in production.</p>\n</blockquote>\n<blockquote>\n<p>Using the microservice architecture makes it much more difficult to iterate rapidly. A startup should almost certainly begin with a monolithic application.</p>\n</blockquote>\n<blockquote>\n<p>But in addition to having the right architecture, successful software development requires you to also have organization, and development and delivery processes.</p>\n</blockquote>\n<blockquote>\n<p>It’s important, therefore, to apply Conway’s law in reverse and design your organization so that its structure mirrors your microservice architecture. By doing so, you ensure that your development teams are as loosely coupled as the services</p>\n</blockquote>\n<blockquote>\n<p>A key characteristic of continuous delivery is that software is always releasable. It relies on a high level of automation, including automated testing. Continuous deployment takes continuous delivery one step further in the practice of automatically deploying releasable code into production.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Monolithic hell is an application's state where it has become too large and too complex. It might be written with obsolete technology. To make changes is almost impossible and take too much time.</p>\n<p>The initial benefits of using a monolithic approach:</p>\n<ul>\n<li>Simple to develop - Less complexity</li>\n<li>Easy to make radical changes - code and its lifecycle tools are all together</li>\n<li>Straightforward to test - Only one service to setup and test ( e2e + ui )</li>\n<li>Straightforward to deploy - Only one service to deploy. </li>\n<li>Easy to scale - Run multiple instances behind and load balancer.</li>\n</ul>\n<p>As the application evolves:</p>\n<ul>\n<li>The code grows</li>\n<li>The size of the application team grows</li>\n<li>One team becomes several teams</li>\n<li>Complexity increases. Eventually becoming too large to someone to fully understand. Leading to more time fixing bugs and longer lead time</li>\n<li>The project goes through a downwards spiral. The more complex, the harder to maintain the healthy of the application, which leads to adding even more complexity</li>\n<li>Development is slow. Longer build time, startup time and so on. Longer the feedback for the developer</li>\n<li>The longer the frequency between deployments</li>\n<li>The more frequent an unreliable state in the version control system</li>\n<li>The higher is the effort on manual testing and the time to run all the battery of tests</li>\n<li>The harder to optimize the hardware when scaling</li>\n<li>The reliability decreases, since all the application run in one process. A problem in the process stops everything</li>\n<li>The more common is to use a non optimal tool for a given task</li>\n</ul>\n<p>A team can delay, but cannot avoid the problems of a large team working on a monolithic application.</p>\n<p>Microservices is a service-oriented architecture composed of loosely coupled elements that have bounded contexts.</p>\n<p>Another definition is to consider a 3D cube with the following axis:</p>\n<p><em>X</em>: Cloning. Multiple identical instances behind a Load Balancer</p>\n<p><em>Z</em>: Partitioning. Multiple identical instances behind a Load Balancer. Which distributes the traffic depending on the <strong>data</strong>.</p>\n<p><em>Y</em>: Functional decomposition. Each service is a mini application with a narrowly focused functionality. A focused and cohesive set of responsibilities.</p>\n<p>Even though you can have a monolithic architecture with modularity. There is only a soft boundary. And a developer is still able to access some internal behavior of another module. With microservices, services are the smallest unit, and the API protects the code. Making it transparent and establishing a hard boundary.</p>\n<p>To achieve the loosely coupling it's important that services communicate through APIs. Meaning that each service should have its own datastore.</p>\n<p>Loosely coupled services enable to develop, test and deploy isolated services.</p>\n<p>SOA and Microservices diverge in:</p>\n<ul>\n<li>SOA uses heavyweight protocols such as SOAP. Microservices uses message brokers or direct service to service through REST or gRPC.</li>\n<li>SOA has global model and shared databases. Microservices uses data models and have a database per service.</li>\n<li>SOA is usually used to integrate large, complex and monolithic applications. Microservices consist of dozens or hundreds of smaller services.</li>\n</ul>\n<p>The Benefits of Microservices:</p>\n<ul>\n<li>\n<p>It enables the continuous delivery and deployment of large, complex applications:</p>\n<ul>\n<li>Testability for CI/CD</li>\n<li>Deployability for CI/CD</li>\n<li>Autonomous and independent teams.</li>\n</ul>\n</li>\n<li>Services are small and easily maintained: Smaller and less complex scope.</li>\n<li>Services are independently deployable and scalable: Independent X and Z axis.</li>\n<li>It allows easy experimenting and adoption of new technologies: Smaller scope and reduced risk to test new tech.</li>\n<li>It has better fault isolation: Several processes running.</li>\n</ul>\n<p>The Drawbacks of Microservices:</p>\n<ul>\n<li>Finding the right set of services is challenging: There is no one algorithm. You might create a distributed monolith.</li>\n<li>Distributed systems are complex, which makes development, testing, and deployment difficult: More complex, requires interprocess communication. Harder to implement transactional queries that span services. Operational complexity due to the several parts running.</li>\n<li>Deploying features that span multiple services requires careful coordination: Coordination between several development teams. Roll-out plan.</li>\n<li>Deciding when to adopt the microservice architecture is difficult: It's better to start with a monolith, since it's faster. But it's harder to break it down later.</li>\n</ul>\n<p>The author then explains about patterns and it's useful because it describes the context it applies.</p>\n<p>The microservices patterns consists of a group of patterns that are solution to problems encountered in microservices. The groups are part of 3 layers:</p>\n<ol>\n<li>Infrastructure patterns - Infrastructure issues</li>\n<li>Application Infrastructure - Infrastructure issues that impact development</li>\n<li>Application patterns - Application issues</li>\n</ol>\n<p>The groups are:</p>\n<ul>\n<li>Decomposition: How to break the microservices</li>\n<li>\n<p>Communication:</p>\n<ul>\n<li>Communication Style: Choose mechanism to communicate</li>\n<li>Discovery: Client and Service recognition</li>\n<li>Reliability: reliable communication even when servers might be unreliable.</li>\n<li>Transactional Messaging: Applying database transaction with messaging and events </li>\n<li>External API: Clients and Service communication</li>\n</ul>\n</li>\n<li>Data consistency: Maintain consistency with distributed databases</li>\n<li>Queries: With data distributed how to retrieve data scattered.</li>\n<li>Deployment: Deploying microservices</li>\n<li>Observability: Runtime behavior and troubleshooting problems</li>\n<li>Testing: Testing smaller pieces but a more complex system.</li>\n<li>Cross-cutting: External configuration</li>\n<li>Security: Securing the whole system.</li>\n</ul>\n<p>When the organization starts to succeed it's expected that the number of developers will  increase. Applying the inverse Conway's, and structuring the organization in a manner to have small, cross-functional and autonomous teams, the system will mirror this structure. Enabling to add more people and maintain the productivity by avoiding communication overhead.</p>\n<p>In addition, it's ideal to use microservice with a agile / DevOps mindsets. Microservices eases to apply concepts like CD by reducing the size of the services you make it easier to apply changes to production faster. </p>\n<p>During a transition to microservice architecture, you have to consider the human side of the developers in order to ensure a good transition.</p>\n<h1>Chapter 2 - Decomposition Strategies</h1>\n<blockquote>\n<p>The key challenge, which is the essence of the microservice architecture, is the functional decomposition of the application into services.</p>\n</blockquote>\n<h2>What is the microservice architecture exactly?</h2>\n<blockquote>\n<p>The architecture of a software application is its high-level structure, which consists of constituent parts and the dependencies between those parts.</p>\n</blockquote>\n<blockquote>\n<p>The reason architecture is important is because it determines the application’s software quality attributes or -ilities.</p>\n</blockquote>\n<h3>What is software architecture and why does it matter?</h3>\n<blockquote>\n<p>The software architecture of a computing system is the set of structures needed to reason about the system, which comprise software elements, relations among them, and properties of both.</p>\n</blockquote>\n<blockquote>\n<p>The 4+1 model, shown in Figure 2.1, defines four different views of a software architecture. Each describes a particular aspect of the architecture and consists of a particular set of software elements and relationships between them.</p>\n</blockquote>\n<blockquote>\n<p>Architecture is important because it enables an application to satisfy the second category of requirements: its quality of service requirements. These are also known as quality attributes and are the so-called -ilities.</p>\n</blockquote>\n<blockquote>\n<p>The quality of service requirements define the runtime qualities such as scalability and reliability. They also define development time qualities including maintainability, testability, and deployability</p>\n</blockquote>\n<h3>Overview of architectural styles</h3>\n<blockquote>\n<p>An architectural style, then, defines a family of such systems in terms of a pattern of structural organization. More specifically, an architectural style determines the vocabulary of components and connectors that can be used in instances of that style, together with a set of constraints on how they can be combined.</p>\n</blockquote>\n<blockquote>\n<p>the monolithic architecture is an architectural style that structures the implementation view as a single (executable/deployable) component. The microservice architecture structures an application as a set of loosely coupled services.</p>\n</blockquote>\n<blockquote>\n<p>A layered architecture organizes software elements into layers. Each layer has a well-defined set of responsibilities. A layered architecture also constraints the dependencies between the layers.</p>\n</blockquote>\n<blockquote>\n<p>hexagonal architecture style organizes the logical view in a way that places the business logic at the center.</p>\n</blockquote>\n<blockquote>\n<p>the application has one or more inbound adapters that handle requests from the outside by invoking the business logic... the application has one or\nmore outbound adapters that are invoked by the business logic and invoke external applications.</p>\n</blockquote>\n<blockquote>\n<p>There are two kinds of ports: inbound and outbound ports. An inbound port is an API exposed by the business logic, which enables it to be invoked\nby external applications. ... An outbound port is how the business logic invokes external systems.</p>\n</blockquote>\n<blockquote>\n<p>Hexagonal architecture is a great way to describe the architecture of each service in a microservice architecture.</p>\n</blockquote>\n<h3>The microservice architecture is an architectural style</h3>\n<blockquote>\n<p>Monolithic architecture is an architectural style that structures the implementation view as a single component</p>\n</blockquote>\n<blockquote>\n<p>A monolithic application can, for example, have a logical view that’s organized along the lines of a hexagonal architecture.</p>\n</blockquote>\n<blockquote>\n<p>It structures the implementation view as a set of multiple components</p>\n</blockquote>\n<blockquote>\n<p>Each service has its own logical view architecture, which is typically a hexagonal architecture</p>\n</blockquote>\n<blockquote>\n<p>A key constraint imposed by the microservice architecture is that the services are loosely coupled. Consequently, there are restrictions on how the services collaborate.</p>\n</blockquote>\n<blockquote>\n<p>A service is a standalone, independently deployable software component that implements some useful functionality.</p>\n</blockquote>\n<blockquote>\n<p>A service has an API that provides its clients access to its functionality. There are two types of operations: commands and queries. The API consists of commands, queries, and events. A command, such as createOrder() , performs actions and updates data. A query, such as findOrderById() , retrieves data. A service also publishes events, such as OrderCreated , which are consumed by its clients.</p>\n</blockquote>\n<blockquote>\n<p>the microservice architecture enforces the application’s modularity.</p>\n</blockquote>\n<blockquote>\n<p>An essential requirement, however, is that a service has an API and is independently deployable.</p>\n</blockquote>\n<blockquote>\n<p>All interaction with a service happens via its API, which encapsulates its implementation details. This enables the implementation of the service to change without impacting its clients</p>\n</blockquote>\n<blockquote>\n<p>You must treat a service’s persistent data like the fields of a class and keep them private</p>\n</blockquote>\n<blockquote>\n<p>[About shared libraries] you need to ensure that you don’t accidentally introduce coupling between your services.</p>\n</blockquote>\n<h2>Defining an application's microservice architecture</h2>\n<blockquote>\n<p>A system operation is an abstraction of a request that the application must handle. It’s either a command, which updates data, or a query, which retrieves data. The behavior of each command is defined in terms of an abstract domain model, which is also derived from the requirements. The system operations become the architectural scenarios that illustrate how the services collaborate.</p>\n</blockquote>\n<h3>Identifying the system operations</h3>\n<blockquote>\n<p>The domain model is derived primarily from the nouns of the user stories, and the system operations are derived mostly from the verbs.</p>\n</blockquote>\n<blockquote>\n<p>The behavior of each system operation is described in terms of its effect on one or more domain objects and the relationships between them.</p>\n</blockquote>\n<blockquote>\n<p>an architecture consisting of services that are primarily organized around business rather than technical concepts.</p>\n</blockquote>\n<h3>Defining services by applying the Decompose by business capability pattern</h3>\n<blockquote>\n<p>An organization’s business capabilities capture what an organization’s business is.</p>\n</blockquote>\n<blockquote>\n<p>An organization’s business capabilities are identified by analyzing the organization’s purpose, structure, and business processes. Each business capability can be thought of as a service, except it’s business-oriented rather than technical.</p>\n</blockquote>\n<blockquote>\n<p>A key benefit of organizing services around capabilities is that because they’re stable, the resulting architecture will also be relatively stable. The individual components of the architecture may evolve as the how aspect of the business changes, but the architecture remains unchanged.</p>\n</blockquote>\n<blockquote>\n<p>They may evolve over time as we learn more about the application domain. In particular, an important step in the architecture definition process is investigating how the services collaborate in each of the key architectural services.</p>\n</blockquote>\n<h3>Defining services by applying the Decompose by sub-domain pattern</h3>\n<blockquote>\n<p>DDD avoids these problems (overly used and complex models) by defining multiple domain models, each with an explicit scope.</p>\n</blockquote>\n<blockquote>\n<p>DDD calls the scope of a domain model a bounded context. A bounded context includes the code artifacts that implement the model. When using the microservice architecture, each bounded context is a service or possibly a set of services. We can create a microservice architecture by applying DDD and defining a service for each subdomain.</p>\n</blockquote>\n<blockquote>\n<p>Also, the microservice architecture’s concept of autonomous teams owning services is completely aligned with the DDD’s concept of each domain\nmodel being owned and developed by a single team.</p>\n</blockquote>\n<h3>Decomposition guidelines</h3>\n<blockquote>\n<p>We can apply SRP when defining a microservice architecture and create small, cohesive services that each have a single responsibility. This will reduce the size of the services and increase their stability.</p>\n</blockquote>\n<blockquote>\n<p>We can apply CCP when creating a microservice architecture and package components that change for the same reason into the same service.</p>\n</blockquote>\n<blockquote>\n<p>A saga is a sequence of local transactions that are coordinated using messaging.</p>\n</blockquote>\n<h3>Defining service APIs</h3>\n<blockquote>\n<p>In order to fully define the service APIs, you need to analyze each system operation and determine what collaboration is required.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>There are several definitions for software architecture. One way of describe it is to think of the architecture as the bigger picture that decomposes the elements and their relationships. Therefore, decomposition plays a important role because will define how the division of knowledge and work will be distributed between teams, the lifecycle of each element, with each other element they interact and so on.</p>\n<p>Another point of view is to consider the 4+1 model. Where as the architecture is composed by 4 views:</p>\n<ul>\n<li>Logical: The software created by developers.</li>\n<li>Implementation: The output of the system and its relationships.</li>\n<li>Process: The components at runtime.</li>\n<li>Deployment: How the processes are mapped to machines.</li>\n<li>Scenarios: The most important requirement as use cases or scenarios.</li>\n</ul>\n<p>The architecture is important because it will define the -ilities of the applications composing the architecture. The quality of service, aka quality attributes or the -ilities determine non functional requirements like scalability, performance, maintainability, testability, deployability and so on...</p>\n<p>An architecture style defines patterns, vocabulary and a set of constraints. A very common architecture style is the layered architecture.</p>\n<p>The popular three-tier architecture is the layered architecture applied to the logical view. Three tiers, the dependency goes top to bottom (constrain between layers).</p>\n<ul>\n<li>Presentation layer: UI</li>\n<li>Business logic layer: Business</li>\n<li>Persistence layer: Database</li>\n</ul>\n<p>The hexagonal architecture is an alternative to the layered architectural style. It also applies to the logical view. And places the business logic in the center of the hexagon. The hexagonal architecture is composed by</p>\n<ul>\n<li>\n<p>Adapters: Middleware between external code and business logic. </p>\n<ul>\n<li>Inbound: Handle request from the outside (HTTP, Queue, ...)</li>\n<li>Outbound: Invoked by the business logic and invokes external application (Database, HTTP, ...)</li>\n</ul>\n</li>\n<li>\n<p>Ports: Set of operations exposed by the business logic</p>\n<ul>\n<li>Inbound: Enables external applications to call the ports</li>\n<li>Outbound: Enables business logic to invoke an external system</li>\n</ul>\n</li>\n</ul>\n<p>Monolithic and microservices are both architectural styles.</p>\n<ul>\n<li>Monolithic: Implementation view as a single component.</li>\n<li>\n<p>Microservices: Implementation view as multiple components.</p>\n<ul>\n<li>Each service has its own logical view which might be an hexagonal architecture.</li>\n<li>Loosely coupled, there are restrictions on how services communicate (usually through APIs)</li>\n<li>A service is a standalone, independently deployable software component that implements some useful functionality</li>\n</ul>\n</li>\n</ul>\n<p>The API is composed of:</p>\n<ol>\n<li>Command: Performs action or updates data</li>\n<li>Query: Retrieves data</li>\n<li>Events: Events triggered by the API</li>\n</ol>\n<p>Three-step process to to define a microservices architecture:</p>\n<ol>\n<li>\n<p>Describe the system operation</p>\n<ol>\n<li>Command: Create, update and delete data</li>\n<li>Operation</li>\n<li>Return</li>\n<li>Pre-condition</li>\n<li>Post-conditions</li>\n<li>Query: System operations that read (query) data</li>\n</ol>\n</li>\n<li>\n<p>Decomposition into services:</p>\n<ol>\n<li>Services accordingly to business capabilities</li>\n<li>DDD Subdomains</li>\n</ol>\n</li>\n<li>Determine the service's API</li>\n</ol>\n<p>A system operation is a more abstract notion of a request. It can be derived from User Stories or some other form of requirements. A system operation will be mapped to a High Level domain.</p>\n<p>Once you have identified the system operations, we have to decompose the services by business capabilities or by domain driven design.</p>\n<p>Business capabilities is something that the business does to generate value. Business capabilities captures the <em>what</em>, in opposition of the <em>how</em>, of a organization. You identify the organization's business capabilities by looking at the organization's purpose, structure and business processes. A business capability might have sub capabilities.</p>\n<p>Once you have the capabilities you can define a service for each capability or group of capabilities. The benefit of this approach is that you end up with a stable architecture since the business capabilities are usually stable. The services that compose the a capability will of course evolve.</p>\n<p>DDD fits extremely well to decompose services due to sub-domains and its bounded contexts. Once you identify the subdomains of your problem, each subdomain has its own bounded context, its scope. Each bounded context can be one or a set of services.</p>\n<p>Obstacles to decomposition:</p>\n<ol>\n<li>Network latency ( or Impractical due to too many round-trips)</li>\n<li>Reduced availability due to synchronous communication</li>\n<li>Maintaining data consistency across services</li>\n<li>Obtaining a consistent view of the data</li>\n<li>God classes preventing decomposition</li>\n</ol>\n<p>Once we have decomposed the system operations and its respective sub-domain. The last step is to define a service API. To define the service API, you assign the system operations to Services. To have fully operating API, one should also map the relationship between the APIs.</p>\n<h1>Chapter 3 -Interprocess communication in a microservice architecture</h1>\n<h2>Overview of interprocess communication in a microservice architecture</h2>\n<h3>Defining API in microservices architecture</h3>\n<blockquote>\n<p>A well-designed interface exposes useful functionality while hiding the implementation. It enables the implementation to change without impacting clients.</p>\n</blockquote>\n<blockquote>\n<p>A service’s API is a contract between the service and its clients.</p>\n</blockquote>\n<blockquote>\n<p>Doing this up-front design increases your chances of building a service that meets the needs of its clients.</p>\n</blockquote>\n<blockquote>\n<p>Ideally, you should strive to only make backward-compatible changes. Backward compatible changes are additive changes to an API:</p>\n<ul>\n<li>Adding optional attributes to request</li>\n<li>Adding attributes to a response</li>\n<li>Adding new operations</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Services should provide default values for missing request attributes. Similarly, clients should ignore any extra response attributes. In order for this to be painless, clients and services must use a request and response format that supports the Robustness principle.</p>\n</blockquote>\n<blockquote>\n<p>Because you can’t force clients to upgrade immediately, a service must simultaneously support old and new versions of an API for some period of time.</p>\n</blockquote>\n<blockquote>\n<p>In order to support multiple versions of an API, the service’s adapters that implement the APIs will contain logic that translates between the old and new versions.</p>\n</blockquote>\n<h3>Message formats</h3>\n<blockquote>\n<p>In either case, it’s essential to use a cross-language message format.</p>\n</blockquote>\n<h2>Communicating using the synchronous Remote procedure invocation pattern</h2>\n<blockquote>\n<p>a client sends a request to a service, and the service processes the request and sends back a response. Some clients may block waiting for a response, and others might have a reactive, non-blocking architecture. But unlike when using messaging, the client assumes that the response will arrive in a timely fashion.</p>\n</blockquote>\n<h3>Using REST</h3>\n<blockquote>\n<p>A key concept in REST is a resource, which typically represents a single business object, such as a Customer or Product, or a collection of business objects</p>\n</blockquote>\n<blockquote>\n<p>REST uses the HTTP verbs for manipulating resources, which are referenced using a URL</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Level 0—Clients of a level 0 service invoke the service by making HTTP POST requests to its sole URL endpoint. Each request specifies the action to perform, the target of the action (for example, the business object), and any parameters.</li>\n<li>Level 1—A level 1 service supports the idea of resources. To perform an action on a resource, a client makes a POST request that specifies the action to perform and any parameters.</li>\n<li>Level 2—A level 2 service uses HTTP verbs to perform actions: GET to retrieve, POST to create, and PUT to update. The request query parameters and body, if any, specify the actions' parameters. This enables services to use web infrastructure such as caching for GET requests.</li>\n<li>Level 3—The design of a level 3 service is based on the terribly named HATEOAS (Hypertext As The Engine Of Application State) principle. The\nbasic idea is that the representation of a resource returned by a GET request contains links for performing actions on that resource. For example, a client can cancel an order using a link in the representation returned by the GET request that retrieved the order. The benefits of HATEOAS include no longer having to hard-wire URLs into client code</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Consequently, a common problem when designing a REST API is how to enable the client to retrieve multiple related objects in a single request</p>\n</blockquote>\n<blockquote>\n<p>This has led to the increasing popularity of alternative API technologies such as GraphQL and Netflix Falcor, which are designed to support efficient data fetching.</p>\n</blockquote>\n<blockquote>\n<p>A REST API should use PUT for updates, but there may be multiple ways to update an order, including cancelling it, revising the order, and so on.</p>\n</blockquote>\n<h3>Using gRPC</h3>\n<blockquote>\n<p>A gRPC API consists of one or more services and request/response message definitions</p>\n</blockquote>\n<blockquote>\n<p>As well as supporting simple request/response RPC, gRPC support streaming RPC.</p>\n</blockquote>\n<blockquote>\n<p>gRPC uses Protocol Buffers as the message format.</p>\n</blockquote>\n<blockquote>\n<p>Each field of a Protocol Buffers message is numbered and has a type code. A message recipient can extract the fields that it needs and skip over the fields that it doesn’t recognize.</p>\n</blockquote>\n<h3>Handling partial failure using the Circuit breaker pattern</h3>\n<blockquote>\n<p>In a distributed system, whenever a service makes a synchronous request to another service, there is an ever-present risk of partial failure.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Network timeouts—Never block indefinitely and always use timeouts when waiting for a response. Using timeouts ensures that resources are never tied up indefinitely.</li>\n<li>Limiting the number of outstanding requests from a client to a service—Impose an upper bound on the number of outstanding requests that a client can make to a particular service. If the limit has been reached, it’s probably pointless to make additional requests, and those attempts should fail immediately.</li>\n<li>Circuit breaker pattern —Track the number of successful and failed requests, and if the error rate exceeds some threshold, trip the circuit breaker so that further attempts fail immediately. A large number of requests failing suggests that the service is unavailable and that sending more requests is pointless. After a timeout period, the client should try again, and, if successful, close the circuit breaker.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Service discovery is conceptually quite simple: its key component is a service registry, which is a database of the network locations of an application’s service instances.</p>\n</blockquote>\n<blockquote>\n<p>One drawback of platform-provided service discovery is that it only supports the discovery of services that have been deployed using the platform.</p>\n</blockquote>\n<h2>Communicating using the Asynchronous messaging pattern</h2>\n<blockquote>\n<p>A messaging-based application typically uses a message broker, which acts as an intermediary between the services, although another option is to use a brokerless architecture, where the services communicate directly with each other. A service client makes a request to a service by sending it a message. If the service instance is expected to reply, it will do so by sending a separate message back to the client. Because the communication is asynchronous, the client doesn’t block waiting for a reply</p>\n</blockquote>\n<h3>Overview of messaging</h3>\n<blockquote>\n<p>A sender (an application or service) writes a message to a channel, and a receiver (an application or service) reads messages from a channel.</p>\n</blockquote>\n<blockquote>\n<p>message consists of a header and a message body</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Document—A generic message that contains only data. The receiver decides how to interpret it. The reply to a command is an example of a document message.</li>\n<li>Command—A message that’s the equivalent of an RPC request. It specifies the operation to invoke and its parameters.</li>\n<li>Event—A message indicating that something notable has occurred in the sender. An event is often a domain event, which represents a state change of a domain object such as an Order , or a Customer .</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>There are two kinds of channels:</p>\n<ul>\n<li>A point-to-point channel delivers a message to exactly one of the consumers that is reading from the channel. Services use point-to-point channels for the one-to-one interaction styles described earlier. For example, a command message is often sent over a point-to-point channel.</li>\n<li>A publish-subscribe channel delivers each message to all of the attached consumers. Services use publish-subscribe channels for the one-to-many interaction styles described earlier. For example, an event message is usually sent over a publish-subscribe channel.</li>\n</ul>\n</blockquote>\n<h3>Creating an API specification for a messaging-based service API</h3>\n<blockquote>\n<p>specify the names of the message channels, the message types that are exchanged over each channel, and their formats. You must also describe the format of the messages using a standard such as JSON, XML, or Protobuf</p>\n</blockquote>\n<h3>Using a message broker</h3>\n<blockquote>\n<p>You can also use a brokerless-based messaging architecture, in which the services communicate with one another directly.</p>\n</blockquote>\n<blockquote>\n<p>In fact, some of these drawbacks, such as reduced availability and the need for service discovery, are the same as when using synchronous, response/response.</p>\n</blockquote>\n<blockquote>\n<p>A message broker is an intermediary through which all messages flow. A sender writes the message to the message broker, and the message broker delivers it to the receiver.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Supported programming languages—You probably should pick one that supports a variety of programming languages.</li>\n<li>Supported messaging standards—Does the message broker support any standards, such as AMQP and STOMP, or is it proprietary?</li>\n<li>Messaging ordering—Does the message broker preserve ordering of messages?</li>\n<li>Delivery guarantees—What kind of delivery guarantees does the broker make?</li>\n<li>Persistence—Are messages persisted to disk and able to survive broker crashes?</li>\n<li>Durability—If a consumer reconnects to the message broker, will it receive the messages that were sent while it was disconnected?</li>\n<li>Scalability—How scalable is the message broker?</li>\n<li>Latency—What is the end-to-end latency?</li>\n<li>Competing consumers—Does the message broker support competing consumers?</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Explicit interprocess communication—RPC-based mechanism attempts to make invoking a remote service look the same as calling a local service. But due to the laws of physics and the possibility of partial failure, they’re in fact quite different.</p>\n</blockquote>\n<h3>Competing receivers and message ordering</h3>\n<blockquote>\n<p>Using multiple threads and service instances to concurrently process messages increases the throughput of the application. But the challenge with processing messages concurrently is ensuring that each message is processed once and in order.</p>\n</blockquote>\n<h3>Handling duplicate messages</h3>\n<blockquote>\n<p>A message broker should ideally deliver each message only once, but guaranteeing exactly-once messaging is usually too costly. Instead, most message brokers promise to deliver a message at least once.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Two dimensions to characterize client server communication</p>\n<ol>\n<li>\n<p>Interactions</p>\n<ol>\n<li>One-to-one</li>\n<li>One-to-many</li>\n</ol>\n</li>\n<li>\n<p>Sincronicity</p>\n<ol>\n<li>Synchronous</li>\n<li>Asynchronous</li>\n</ol>\n</li>\n<li>\n<p>One-to-one</p>\n<ul>\n<li>Request and Response: One to one and synchronous communication. The client requests and awaits for the response. (Tightly coupled services).</li>\n<li>Asynchronous Request and Response: The client requests, the response is sent asynchronously. The client doesn't block while waiting.</li>\n<li>One-way notifications: The request is sent, but no response is expected</li>\n</ul>\n</li>\n<li>\n<p>One-to-many</p>\n<ul>\n<li>publish/subscribe: Client publishes a message. Zero or more consumers consume it.</li>\n<li>Publish/async response: The client publishes a message and awaits for a certain time for the response from interested services.</li>\n</ul>\n</li>\n</ol>\n<p>When defining APIs a good approach is to use a API first approach. Designing the API and aligning with the clients of the API, once it's aligned you start the implementation.</p>\n<p>As the APIs evolve with time it's a good idea to use a SemVer to version your API, you can use it in the URI, through a HTTP header, or in the message body. In addition to SemVer, apply the Robustness principle to enable backward compatible changes. Servers should provide default values, and clients ignore any additional fields in the request.</p>\n<p>When applying a major change, which are incompatible for clients, you should maintain two version concurrently, since you cannot ensure the client speed to change to the new API. You can then create an adapter that maps the logic from one API to another.</p>\n<p>Message format:</p>\n<ol>\n<li>\n<p>Text</p>\n<ol>\n<li>JSON</li>\n<li>XML</li>\n</ol>\n</li>\n<li>\n<p>Binary</p>\n<ol>\n<li>Protocol buffers</li>\n<li>Avro</li>\n<li>Thrift</li>\n</ol>\n</li>\n</ol>\n<p>Text message format are verbose and require an overhead to parse the text.</p>\n<p>Binary message format provide a Interface definition language (IDL) for defining the structure of the message.</p>\n<p>Remote Procedure Invocation: Sends a request and expects a response.</p>\n<p>The book covers two models of RPI: REST and gRPC</p>\n<ol>\n<li><em>REST</em></li>\n</ol>\n<p>Models the APIs based on the resources of the domain. And have well defined usage of HTTP methods.</p>\n<p>There's the Richardson maturity model for REST's APIs. See in the quotes.</p>\n<p>The main problems with REST APIs are:</p>\n<ol>\n<li>Fetching data from multiple resources at the same time (E.g. Customer, orders, contractors, ...)</li>\n<li>Mapping HTTP verbs to business operations (E.g. Create/update lead)</li>\n<li><em>gRPC</em></li>\n</ol>\n<p>It's a binary based protocol, that unlike REST it's not restricted by the HTTP methods. It uses a Protocol Buffer to create to generate client-side stubs and server side skeleton.</p>\n<p>The gRPC API is composed of a set of services and request/response messages definitions.  It can only be used as synchronous communication.</p>\n<p>Since RPI invocations are synchronous they have the problem of partial failure, where either the server does not respond within an expected time and blocking the client meanwhile.</p>\n<p>To handle this problem</p>\n<ol>\n<li>Use network timeouts</li>\n<li>Limiting the number of outstanding requests from a client to a service</li>\n<li>Circuit breaker pattern. Track requests and if exceeds a threshold, trip the circuit breaker so that following requests fails.</li>\n</ol>\n<p>To recover from an unavailable service, you have some options:</p>\n<ul>\n<li>Respond with an error</li>\n<li>Respond with cached information</li>\n<li>Respond with default information</li>\n</ul>\n<p>Service discovery:</p>\n<ul>\n<li>Application level - the services and their clients interact directly with the service registry.</li>\n<li>The deployment infrastructure handles service discovery.</li>\n</ul>\n<p>The author describes the basics of messaging system described in the Enterprise Integration Patterns.</p>\n<p>Messaging systems enable several Interaction Styles:</p>\n<ul>\n<li>request/response</li>\n<li>async request/response</li>\n<li>one way notification</li>\n<li>publish/subscribe</li>\n<li>publish/ async responses</li>\n</ul>\n<p>Differently from HTTP APIs messaging APIs do not have a standardized tool for documentation. One should focus on document the channels (both command and reply), the message expected and events channels.</p>\n<p>Messaging can have two different architecture</p>\n<ul>\n<li>\n<p>broker based</p>\n<ul>\n<li>Loose coupling</li>\n<li>Message buffering: Store the messages until a consumer consumes it</li>\n<li>Flexible communication</li>\n<li>Explicit interprocess comunication: very explicit RPC</li>\n<li>Potential performance bottleneck</li>\n<li>Potential single point of failure</li>\n<li>Additional operational complexity</li>\n</ul>\n</li>\n<li>\n<p>brokerless</p>\n<ul>\n<li>Better latency (less nodes to traffic the message)</li>\n<li>avoid the broker performance bottleneck</li>\n<li>less complexity</li>\n<li>Requires 'service discovery'</li>\n<li>reduced availability</li>\n</ul>\n</li>\n</ul>\n<p>Must consider the points highlighted above in the notes when considering a message broker tool.</p>\n<p>A common solution to avoid competing receivers and messages out of order is to use <em>sharded</em> (partitioned) channels. A channel is a set of shards ( channels). The client sends the shard key. The messaging routes to the shards of a channel based on the shard key.</p>\n<p><strong>I need to finish notes from the chapter 3</strong></p>\n<h1>Chapter 4 - Managing transactions with sagas</h1>\n<blockquote>\n<p>ACID (Atomicity, Consistency, Isolation, Durability) transactions greatly simplify the job of the developer by providing the illusion that each transaction has exclusive access to the data. In a microservice architecture, transactions that are within a single service can still use ACID transactions.</p>\n</blockquote>\n<blockquote>\n<p>an operation that spans services must use what’s known as a saga, a message-driven sequence of local transactions, to maintain data\nconsistency.</p>\n</blockquote>\n<blockquote>\n<p>They lack the isolation feature of traditional ACID transactions. As a result, an application must use what are known as countermeasures, design techniques that prevent or reduce the impact of concurrency anomalies caused by the lack of isolation.</p>\n</blockquote>\n<h2>Transaction management in a microservice architecture</h2>\n<h3>The trouble with distributed transactions</h3>\n<blockquote>\n<p>The traditional approach to maintaining data consistency across multiple services, databases, or message brokers is to use distributed transactions. The de facto standard for distributed transaction management is the X/Open Distributed Transaction Processing (DTP) Model (X/Open XA).</p>\n</blockquote>\n<blockquote>\n<p>XA uses two-phase commit (2PC) to ensure that all participants in a transaction either commit or rollback.</p>\n</blockquote>\n<blockquote>\n<p>One problem is that many modern technologies, including NoSQL databases such as MongoDB and Cassandra, don’t support them</p>\n</blockquote>\n<blockquote>\n<p>Another problem with distributed transactions is that they are a form of synchronous IPC, which reduces availability. In order for a distributed transaction to commit, all the participating services must be available.</p>\n</blockquote>\n<h3>Using the Saga pattern to maintain data consistency</h3>\n<blockquote>\n<p>You define a saga for each system command that needs to update data in multiple services. A saga is a sequence of local transactions. Each local transaction updates data within a single service using the familiar ACID transaction frameworks and libraries mentioned earlier.</p>\n</blockquote>\n<h2>Coordinating sagas</h2>\n<blockquote>\n<ul>\n<li>Choreography—Distribute the decision making and sequencing among the saga participants. They primarily communicate by exchanging events.</li>\n<li>Orchestration—Centralize a saga’s coordination logic in a saga orchestrator class. A saga orchestrator sends command messages to saga participants telling them which operations to perform.</li>\n</ul>\n</blockquote>\n<h3>Choreography-based sagas</h3>\n<blockquote>\n<p>When using choreography, there’s no central coordinator telling the saga participants what to do. Instead, the saga participants subscribe to each other’s events and respond accordingly</p>\n</blockquote>\n<blockquote>\n<p>The first issue is ensuring that a saga participant updates its database and publishes an event as part of a database transaction.</p>\n</blockquote>\n<blockquote>\n<p>The second issue you need to consider is ensuring that a saga participant must be able to map each event that it receives to its own data.</p>\n</blockquote>\n<blockquote>\n<p>choreography distributes the implementation of the saga among the services. Consequently, it’s sometimes difficult for a developer to understand how a given saga works.</p>\n</blockquote>\n<blockquote>\n<p>The saga participants subscribe to each other’s events, which often creates cyclic dependencies</p>\n</blockquote>\n<h3>Orchestration-based sagas</h3>\n<blockquote>\n<p>When using orchestration, you define an orchestrator class whose sole responsibility is to tell the saga participants what to do</p>\n</blockquote>\n<blockquote>\n<p>A good way to model a saga orchestrator is as a state machine. A state machine consists of a set of states and a set of transitions between states that are triggered by events. Each transition can have an action, which for a saga is the invocation of a saga participant. The transitions between states are triggered by the completion of a local transaction performed by a saga participant. The current state and the specific outcome of the local transaction determine the state transition and what action, if any, to perform. There are also effective testing strategies for state machines. As a result, using a state machine model makes designing, implementing, and testing sagas easier.</p>\n</blockquote>\n<blockquote>\n<p>One benefit of orchestration is that it doesn’t introduce cyclic dependencies.</p>\n</blockquote>\n<blockquote>\n<p>This results in a design where the smart orchestrator tells the dumb services what operations to do. Fortunately, you can avoid this problem by designing orchestrators that are solely responsible for sequencing and don’t contain any other business logic.</p>\n</blockquote>\n<h2>Handling the lack of isolation</h2>\n<blockquote>\n<p>The isolation property of ACID transactions ensures that the outcome of executing multiple transactions concurrently is the same as if they were executed in some serial order.</p>\n</blockquote>\n<blockquote>\n<p>First, other sagas can change the data accessed by the saga while it’s executing. And other sagas can read its data before the saga has completed its updates, and consequently can be exposed to inconsistent data. You can, in fact, consider a saga to be ACD:</p>\n<ul>\n<li>Atomicity—The saga implementation ensures that all transactions are executed or all changes are undone.</li>\n<li>Consistency—Referential integrity within a service is handled by local databases. Referential integrity across services is handled by the services.</li>\n<li>Durability—Handled by local databases.</li>\n</ul>\n</blockquote>\n<h3>Overview of Anomalies</h3>\n<blockquote>\n<ul>\n<li>Lost updates—One saga overwrites without reading changes made by another saga.</li>\n<li>Dirty reads—A transaction or a saga reads the updates made by a saga that has not yet completed those updates.</li>\n<li>Fuzzy/nonrepeatable reads—Two different steps of a saga read the same data and get different results because another saga has made updates.</li>\n</ul>\n</blockquote>\n<h3>Countermeasures for handling the lack of isolation</h3>\n<blockquote>\n<p>It’s the responsibility of the developer to write sagas in a way that either prevents the anomalies or minimizes their impact on the business.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Semantic lock—An application-level lock.</li>\n<li>Commutative updates—Design update operations to be executable in any order.</li>\n<li>Pessimistic view—Reorder the steps of a saga to minimize business risk.</li>\n<li>Reread value—Prevent dirty writes by rereading data to verify that it’s unchanged before overwriting it.</li>\n<li>Version file—Record the updates to a record so that they can be reordered.</li>\n<li>By value—Use each request’s business risk to dynamically select the concurrency mechanism.</li>\n</ul>\n</blockquote>\n<blockquote>\n<ul>\n<li>Compensatable transactions—Transactions that can potentially be rolled back using a compensating transaction.</li>\n<li>Pivot transaction—The go/no-go point in a saga. If the pivot transaction commits, the saga will run until completion. A pivot transaction can be a transaction that’s neither compensatable nor retriable. Alternatively, it can be the last compensatable transaction or the first retriable</li>\n<li>Retriable transactions—Transactions that follow the pivot transaction and are guaranteed to succeed.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>when migrating to microservices, the monolith must sometimes participate in sagas and that it’s significantly simpler if the monolith only ever needs to execute retriable transactions.</p>\n</blockquote>\n<blockquote>\n<p>When using the semantic lock countermeasure, a saga’s compensatable transaction sets a flag in any record that it creates or updates.</p>\n</blockquote>\n<blockquote>\n<p>The flag can either be a lock that prevents other transactions from accessing the record or a warning that indicates that other transactions should treat that record with suspicion. It’s cleared by either a retriable transaction—saga is completing successfully—or by a compensating transaction: the saga is rolling back.</p>\n</blockquote>\n<blockquote>\n<p>(about managing the lock) One option is for the cancelOrder() system command to fail and tell the client to try again later</p>\n</blockquote>\n<blockquote>\n<p>(about managing the lock) Another option is for cancelOrder() to block until the lock is released.</p>\n</blockquote>\n<blockquote>\n<p>One straightforward countermeasure is to design the update operations to be commutative. Operations are commutative if they can be executed in any order</p>\n</blockquote>\n<blockquote>\n<p>It reorders the steps of a saga to minimize business risk due to a dirty read</p>\n</blockquote>\n<blockquote>\n<p>A saga that uses this countermeasure rereads a record before updating it, verifies that it’s unchanged, and then updates the record. If the record has changed, the saga aborts and possibly restarts </p>\n</blockquote>\n<blockquote>\n<p>it records the operations that are performed on a record so that it can reorder them. It’s a way to turn noncommutative operations into commutative operations</p>\n</blockquote>\n<blockquote>\n<p>An application that uses this countermeasure uses the properties of each request to decide between using sagas and distributed transactions</p>\n</blockquote>\n<h2>My Summary</h2>\n<p><strong>I need to finish notes from the chapter 4</strong></p>\n<h1>Chapter 5 - Designing business logic in a microservice architecture</h1>\n<blockquote>\n<p>First, a typical domain model is a tangled web of interconnected classes. Although this isn’t a problem in a monolithic application, in a microservice architecture, where classes are scattered around different services, you need to eliminate object references that would otherwise span service boundaries. The second challenge is designing business logic that works within the transaction management constraints of a microservice architecture.</p>\n</blockquote>\n<blockquote>\n<p>The Aggregate pattern structures a service’s business logic as a collection of aggregates. An aggregate is a cluster of objects that can be treated as a unit. There are two reasons why aggregates are useful when developing business logic in a microservice architecture:</p>\n<ul>\n<li>Aggregates avoid any possibility of object references spanning service boundaries, because an inter-aggregate reference is a primary key value rather than an object reference.</li>\n<li>Because a transaction can only create or update a single aggregate, aggregates fit the constraints of the microservices transaction model.</li>\n</ul>\n</blockquote>\n<h2>Business Logic organization patterns</h2>\n<h3>Designing business logic using the Transaction script pattern</h3>\n<blockquote>\n<p>an important characteristic of this approach is that the classes that implement behavior are separate from those that store state.</p>\n</blockquote>\n<h3>Designing business logic using the Domain model pattern</h3>\n<blockquote>\n<p>Organize the business logic as an object model consisting of classes that have state and behavior.</p>\n</blockquote>\n<blockquote>\n<p>In an object-oriented design, the business logic consists of an object model, a network of relatively small classes. These classes typically correspond directly to concepts from the problem domain.</p>\n</blockquote>\n<blockquote>\n<p>In such a design some classes have only either state or behavior, but many contain both, which is the hallmark of a well-designed class.</p>\n</blockquote>\n<blockquote>\n<p>Moreover, its state is private and can only be accessed indirectly via its methods.</p>\n</blockquote>\n<h2>Designing a domain model using the DDD aggregate pattern</h2>\n<h3>Aggregates have explicit boundaries</h3>\n<blockquote>\n<p>An aggregate is a cluster of domain objects within a boundary that can be treated as a unit.</p>\n</blockquote>\n<blockquote>\n<p>It consists of a root entity and possibly one or more other entities and value objects.</p>\n</blockquote>\n<blockquote>\n<p>Aggregates decompose a domain model into chunks, which are individually easier to understand.</p>\n</blockquote>\n<blockquote>\n<p>A service, for example, uses a repository to load an aggregate from the database and obtain a reference to the aggregate root. It updates an aggregate by invoking a method on the aggregate root.</p>\n</blockquote>\n<blockquote>\n<p>The use of identity rather than object references means that the aggregates are loosely coupled. It ensures that the aggregate boundaries between aggregates are well defined and avoids accidentally updating a different aggregate.</p>\n</blockquote>\n<blockquote>\n<p>Another rule that aggregates must obey is that a transaction can only create or update a single aggregate.</p>\n</blockquote>\n<h3>Aggregate granularity</h3>\n<blockquote>\n<p>When developing a domain model, a key decision you must make is how large to make each aggregate. On one hand, aggregates should ideally be small. Because updates to each aggregate are serialized, more fine-grained aggregates will increase the number of simultaneous requests that the application can handle, improving scalability. It will also improve the user experience because it reduces the chance of two users attempting conflicting updates of the same aggregate. On the other hand, because an aggregate is the scope of transaction, you may need to define a larger aggregate in order to make a particular update atomic.</p>\n</blockquote>\n<h2>Publishing domain events</h2>\n<blockquote>\n<p>In the context of DDD, a domain event is something that has happened to an aggregate. It’s represented by a class in the domain model. An event usually represents a state change.</p>\n</blockquote>\n<h3>What is a domain event?</h3>\n<blockquote>\n<p>A domain event is a class with a name formed using a past-participle verb. It has properties that meaningfully convey the event. Each property is either a primitive value or a value object.</p>\n</blockquote>\n<blockquote>\n<p>A domain event typically also has metadata, such as the event ID, and a timestamp. It might also have the identity of the user who made the change, because that’s useful for auditing. The metadata can be part of the event object, perhaps defined in a superclass. Alternatively, the event metadata can be in an envelope object that wraps the event object</p>\n</blockquote>\n<h3>Event enrichment</h3>\n<blockquote>\n<p>An alternative approach known as event enrichment is for events to contain information that consumers need. It simplifies event consumers because they no longer need to request that data from the service that published the event</p>\n</blockquote>\n<blockquote>\n<p>Although event enrichment simplifies consumers, the drawback is that it risks making the event classes less stable.</p>\n</blockquote>\n<h3>Identifying domain events</h3>\n<blockquote>\n<p>Event storming is an event-centric workshop format for understanding a complex domain. It involves gathering domain experts in a room, lots of sticky notes, and a very large surface—a whiteboard or paper roll—to stick the notes on.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Two main patterns for organizing business logic:</p>\n<ul>\n<li>\n<p>Transaction script pattern</p>\n<ul>\n<li>One script (method) for each system operation which contains the business logic.</li>\n<li>Classes that store state are not the same that implement behavior</li>\n</ul>\n</li>\n<li>Object oriented domain model pattern</li>\n</ul>\n<h1>Chapter 6 - Developing business logic with event sourcing</h1>\n<blockquote>\n<p>event sourcing, an event-centric way of writing business logic and persisting domain objects.</p>\n</blockquote>\n<h2>Developing business logic using event sourcing</h2>\n<blockquote>\n<p>It persists an aggregate as a sequence of events. Each event represents a state change of the aggregate. An application recreates the current state of an aggregate by replaying the events.</p>\n</blockquote>\n<h3>The trouble with traditional persistence</h3>\n<blockquote>\n<p>The traditional approach to persistence maps classes to database tables, fields of those classes to table columns, and instances of those classes to rows in those tables.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Object-Relational impedance mismatch.</li>\n<li>Lack of aggregate history.</li>\n<li>Implementing audit logging is tedious and error prone.</li>\n<li>Event publishing is bolted on to the business logic.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>There’s a fundamental conceptual mismatch between the tabular relational schema and the graph structure of a rich domain model with its complex relationships.</p>\n</blockquote>\n<blockquote>\n<p>Another limitation of traditional persistence is that it only stores the current state of an aggregate.</p>\n</blockquote>\n<blockquote>\n<p>rather than store each Order as a row in an ORDER table, event sourcing persists each Order aggregate as one or more rows in an EVENTS table.</p>\n</blockquote>\n<h3>Overview of event sourcing</h3>\n<blockquote>\n<p>Every state change of an aggregate, including its creation, is represented by a domain event.</p>\n</blockquote>\n<blockquote>\n<p>What’s more, an event must contain the data that the aggregate needs to perform the state transition.</p>\n</blockquote>\n<blockquote>\n<p>The business logic handles a request to update an aggregate by calling a command method on the aggregate root.</p>\n</blockquote>\n<blockquote>\n<p>The first method takes a command object parameter, which represents the request, and determines what state changes need to be performed. It validates its arguments, and without changing the state of the aggregate, returns a list of events representing the state changes. This method typically throws an exception if the command cannot be performed.</p>\n<p>The other methods each take a particular event type as a parameter and update the aggregate. There’s one of these methods for each event. It’s important to note that these methods can’t fail, because an event represents a state change that has happened. Each method updates the aggregate based on the event.</p>\n</blockquote>\n<h1>Chapter 7 - Implementing queries in a microservice architecture</h1>\n<h1>Chapter 8 - External API patterns</h1>\n<h1>Chapter 9 - Testing microservices: Part 1</h1>\n<h1>Chapter 10 - Testing microservices: Part 2</h1>\n<h2>Writing integration tests</h2>\n<blockquote>\n<p>One approach is to launch all the services and test them through their APIs. This, however, is what’s known as end-to-end testing, which is slow, brittle, and costly.</p>\n</blockquote>\n<blockquote>\n<p>A contract is a concrete example of an interaction between a pair of services.</p>\n</blockquote>\n<blockquote>\n<p>A contract consists of either one message, in the case of publish/subscribe style interactions, or two messages, in the case of request/response and asynchronous request/response style interactions.</p>\n</blockquote>\n<blockquote>\n<p>The contracts are used to test both the consumer and the provider, which ensures that they agree on the API.</p>\n</blockquote>\n<h3>Persistence integration tests</h3>\n<blockquote>\n<p>Each phase of a persistence integration test behaves as follows:</p>\n<ul>\n<li>Setup - Set up the database by creating the database schema and initializing it to a known state. It might also begin a database transaction.</li>\n<li>Execute - Perform a database operation.</li>\n<li>Verify - Make assertions about the state of the database and objects retrieved from the database.</li>\n<li>Teardown - An optional phase that might undo the changes made to the database by, for example, rolling back the transaction that was started by the setup phase.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Apart from relying on JPA to create the database schema, the persistence integration tests don’t make any assumption about the state of the database.</p>\n</blockquote>\n<h3>Integration testing REST-based request/response style interactions</h3>\n<blockquote>\n<p>The client must send an HTTP request to the correct endpoint, and the service must send back the response that the client expects.</p>\n</blockquote>\n<h3>Integration testing publish/subscribe-style interactions</h3>\n<blockquote>\n<p>Services often publish domain events that are consumed by one or more other services. Integration testing must verify that the publisher and its consumers agree on the message channel and the structure of the domain events.</p>\n</blockquote>\n<h3>Integration contract tests for asynchronous request/response interactions</h3>\n<blockquote>\n<p>They must agree on the name of command message channel and the structure of the command and reply messages.</p>\n</blockquote>\n<blockquote>\n<p>Integration tests and unit tests verify the behavior of individual parts of a service.</p>\n</blockquote>\n<blockquote>\n<p>The integration tests verify that services can communicate with their clients and dependencies.</p>\n</blockquote>\n<blockquote>\n<p> The unit tests verify that a service’s logic is correct</p>\n</blockquote>\n<h2>Developing component tests</h2>\n<blockquote>\n<p>Component testing verifies the behavior of a service in isolation. It replaces a service’s dependencies with stubs that simulate their behavior. It might even use in-memory versions of infrastructure services such as databases</p>\n</blockquote>\n<blockquote>\n<p>Each scenario defines an acceptance test. The givens correspond to the test’s setup phase, the when maps to the execute phase, and the then and the and to the verification phase.</p>\n</blockquote>\n<h3>Writing acceptance tests using Gherkin</h3>\n<blockquote>\n<p>When using Gherkin, you define your acceptance tests using English-like scenarios, such as the one shown earlier.</p>\n</blockquote>\n<blockquote>\n<p>The givens are the preconditions, the when is the action or event that occurs, and the then/and are the expected outcome.</p>\n</blockquote>\n<h3>Desigining component tests</h3>\n<blockquote>\n<p>You need to test Order Service in isolation, so the component test must configure stubs for several services, including Kitchen Service.</p>\n</blockquote>\n<blockquote>\n<p>An in-process component test runs the service with in-memory stubs and mocks for its dependencies.</p>\n</blockquote>\n<blockquote>\n<p>In-process tests are simpler to write and faster, but have the downside of not testing the deployable service.</p>\n</blockquote>\n<blockquote>\n<p>A more realistic approach is to package the service in a production-ready format and run it as a separate process.</p>\n</blockquote>\n<blockquote>\n<p>key benefit of out-of-process component testing is that it improves test coverage, because what’s being tested is much closer to what’s being deployed</p>\n</blockquote>\n<h2>Writing end-to-end tests</h2>\n<blockquote>\n<p>End-to-end tests have a large number of moving parts. You must deploy multiple services and their supporting infrastructure services.</p>\n</blockquote>\n<h3>Designing end-to-end tests</h3>\n<blockquote>\n<p>A good strategy is to write user journey tests. A user journey test corresponds to a user’s journey through the system.</p>\n</blockquote>\n<h3>Running end-to-end tests</h3>\n<blockquote>\n<p>End-to-end tests must run the entire application, including any required infrastructure services</p>\n</blockquote>\n<h2>My Summary</h2>\n<h1>Chapter 11 - Developing production-ready services</h1>\n<h1>Chapter 12 - Deploying microservices</h1>\n<h1>Chapter 13 - Refactoring to microservices</h1>\n<blockquote>\n<p>A strangler application is a new application consisting of microservices that you develop by implementing new functionality as services and extracting services from the monolith.</p>\n</blockquote>\n<h2>Overview of refactoring to microservices</h2>\n<h3>Why refactor a monolith?</h3>\n<blockquote>\n<p>it’s likely that the business will only support the adoption of microservices if it solves a significant business problem.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Slow delivery—The application is difficult to understand, maintain, and test, so developer productivity is low. As a result, the organization is unable to compete effectively and risks being overtaken by competitors.</li>\n<li>Buggy software releases—The lack of testability means that software releases are often buggy. This makes customers unhappy, which results in losing customers and reduced revenue.</li>\n<li>Poor scalability—Scaling a monolithic application is difficult because it combines modules with very different resource requirements into one executable component. The lack of scalability means that it’s either impossible or prohibitively expensive to scale the application beyond a certain point. As a result, the application can’t support the current or predicted needs of the business.</li>\n</ul>\n</blockquote>\n<h3>Strangling the monolith</h3>\n<blockquote>\n<p>Application modernization is the process of converting a legacy application to one having a modern architecture and technology stack.</p>\n</blockquote>\n<blockquote>\n<p>The most important lesson learned over the years is to not do a big bang rewrite.</p>\n</blockquote>\n<blockquote>\n<p>You could, for example, get to a point where you have tasks that are more important than breaking up the monolith, such as implementing revenue-generating features. If the monolith isn’t an obstacle to ongoing development, you may as well leave it alone.</p>\n</blockquote>\n<blockquote>\n<p>Some organizations have difficulty eliminating technical debt because past attempts were too ambitious and didn’t provide much benefit. As a result, the business becomes reluctant to invest in further cleanup efforts.</p>\n</blockquote>\n<blockquote>\n<p>The only thing you can’t live without is a deployment pipeline that performs automating testing.</p>\n</blockquote>\n<h2>Strategies for refactoring a monolith to microservices</h2>\n<blockquote>\n<ol>\n<li>Implement new features as services.</li>\n<li>Separate the presentation tier and backend.</li>\n<li>Break up the monolith by extracting functionality into services.</li>\n</ol>\n</blockquote>\n<h3>Implement new features as services</h3>\n<blockquote>\n<p>The Law of Holes states that “if you find yourself in a hole, stop digging”</p>\n</blockquote>\n<blockquote>\n<p>If you attempted to implement this kind of feature as a service you would typically find that performance would suffer because of excessive interprocess communication. You might also have problems maintaining data consistency.</p>\n</blockquote>\n<h3>Separate presentation tier from the backend</h3>\n<blockquote>\n<ul>\n<li>Presentation logic—This consists of modules that handle HTTP requests and generate HTML pages that implement a web UI. In an application that has a sophisticated user interface, the presentation tier is often a substantial body of code.</li>\n<li>Business logic—This consists of modules that implement the business rules, which can be complex in an enterprise application.</li>\n<li>Data access logic—This consists of modules that access infrastructure services such as databases and message brokers.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>In particular, it allows the presentation layer developers to rapidly iterate on the user interface and easily perform A/B testing, for example, without having to deploy the backend.</p>\n</blockquote>\n<h3>Extract business capabilities into services</h3>\n<blockquote>\n<p>If you want to significantly improve your application’s architecture and increase your development velocity, you need to break apart the monolith by incrementally migrating business capabilities from the monolith to services.</p>\n</blockquote>\n<blockquote>\n<p>Extracting a service is often time consuming, especially because the monolith’s code base is likely to be messy. Consequently, you need to carefully think about which services to extract.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Splitting the domain model</li>\n<li>Refactoring the database</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>In order to extract a service, you need to extract its domain model out of the monolith’s domain model. You’ll need to perform major surgery to split the domain models.</p>\n</blockquote>\n<blockquote>\n<p>Aggregates reference each other using primary keys rather than object references.</p>\n</blockquote>\n<blockquote>\n<p>An even greater challenge with splitting a domain model is extracting functionality that’s embedded in a class that has other responsibilities.</p>\n</blockquote>\n<blockquote>\n<p>Many classes in a domain model are persistent. Their fields are mapped to a database schema. Consequently, when you extract a service from the monolith, you’re also moving data. You need to move tables from the monolith’s database to the service’s database.</p>\n</blockquote>\n<blockquote>\n<p>One such technique is the idea of replicating data in order to allow you to incrementally update clients of the database to use the new schema.</p>\n</blockquote>\n<blockquote>\n<p>A great way to delay and possibly avoid making these kinds of expensive changes is to use an approach that’s similar to the one described in Refactoring Databases. A major obstacle to refactoring a database is changing all the clients of that database to use the new schema. The solution proposed in the book is to preserve the original schema for a transition period and use triggers to synchronize the original and new schemas. You then migrate clients from the old schema to the new schema over time.</p>\n</blockquote>\n<blockquote>\n<p>A good way to start the migration to microservices is with a time-boxed architecture definition effort. You should spend a short amount of time, such as a couple of weeks, brainstorming your ideal architecture and defining a set of services. This gives you a destination to aim for.</p>\n</blockquote>\n<blockquote>\n<p>Instead of implementing features or fixing bugs in the monolith, you extract the necessary service or service(s) and change those. One benefit of this approach is that it forces you to break up the monolith.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Accelerates development—If your application’s roadmap suggests that a particular part of your application will undergo a lot of development over the next year, then converting it to a service accelerates development.</li>\n<li>Solves a performance, scaling, or reliability problem—If a particular part of your application has a performance or scalability problem or is unreliable, then it’s valuable to convert it to a service.</li>\n<li>Enables the extraction of some other services—Sometimes extracting one service simplifies the extraction of another service, due to dependencies between modules.</li>\n</ul>\n</blockquote>\n<h2>Designing how the service and the monolith collaborate</h2>\n<h3>Designing the integration glue</h3>\n<blockquote>\n<p>One important concern is maintaining data consistency between the service and monolith. In particular, when you extract a service from the monolith, you invariably split what were originally ACID transactions. You must be careful to ensure that data consistency is still maintained.</p>\n</blockquote>\n<blockquote>\n<p>Because the two domain models are different, you must implement what DDD calls an anti-corruption layer (ACL) in order for the service to communicate with the monolith.</p>\n</blockquote>\n<blockquote>\n<p>The goal of an ACL is to prevent a legacy monolith’s domain model from polluting a service’s domain model</p>\n</blockquote>\n<h3>Maintaining data consistency across a service and a monolith</h3>\n<blockquote>\n<p>When you develop a service, you might find it challenging to maintain data consistency across the service and the monolith. A service operation might need to update data in the monolith, or a monolith operation might need to update data in the service.</p>\n</blockquote>\n<h3>Handling authentication and authorization</h3>\n<blockquote>\n<p>Another design issue you need to tackle when refactoring a monolithic application to a microservice architecture is adapting the monolith’s security mechanism to support the services</p>\n</blockquote>\n<h2>Implementing a new feature as a service: handling misdelivered orders</h2>\n<h1>TODO</h1>\n<ul>\n<li>CAP Theorem</li>\n<li>BASE </li>\n</ul>","frontmatter":{"title":"Microservices Patterns","language":"en-US","coverPath":"microservices-patterns","status":"Reading","date":"2021-05-04"}}},{"node":{"html":"<h1>Introdução</h1>\n<blockquote>\n<p>Se você estiver tentando adicionar automação a uma empresa humana complicada, seu software não tem como fugir dessa complexidade - tudo que ele pode fazer é controla-la.</p>\n</blockquote>\n<blockquote>\n<p>Mas a lição aprendida com a experiência de Eric é que os modelos de domínio verdadeiramente bons evoluem com o tempo, e até mesmo os modeladores mais experientes acreditam que adquirem suas melhores ideias após o lançamento inicial de um sistema.</p>\n</blockquote>\n<h1>Prefácio</h1>\n<blockquote>\n<p>Uma característica comum aos sucessos obtidos foi um modelo de domínio rico que evoluiu através de iterações de design e passaram a fazer parte do tecido que compunha o projeto.</p>\n</blockquote>\n<h2>O desafio da complexidade</h2>\n<blockquote>\n<p>Quando a complexidade foge ao controle os desenvolvedores já não podem entender o software bem o suficiente para alterá-lo ou expandi-lo com facilidade e segurança. Por outro lado, um bom design pode criar oportunidades para explorar essas características complexas.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Na maioria dos projetos de software, o principal foco de ser o domínio e a lógica do domínio.</li>\n<li>Designs de domínio complexos devem se basear em um modelo.</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>O DDD é uma maneira de pensar e um conjunto de prioridades, voltado para a aceleração de projetos de software que tem que trabalhar com domínios complicados.</p>\n</blockquote>\n<h2>Design <em>versus</em> processo de desenvolvimento</h2>\n<blockquote>\n<p>É uma colaboração entre quem conhece o domínio e quem sabe como construir softwares.</p>\n</blockquote>\n<blockquote>\n<p>Embora o medo de exigências não antecipadas geralmente leve ao exagero na área de engenharia, a tentativa de se evitar o exagero na engenharia pode se transformar em outro medo, o medo de raciocinar profundamente sobre o design.</p>\n</blockquote>\n<h1>I - Colocando o modelo de domínio em ação</h1>\n<blockquote>\n<p>Mapas são modelos, e cada modelo representa algum aspecto da realidade com uma ideia que seja de interesse. Um modelo é uma simplificação. Ele é uma interpretação da realidade que destaca os aspectos relevantes para resolver o problema que se tem em mãos ignorando os detalhes estranhos.</p>\n</blockquote>\n<blockquote>\n<p>(O modelo) Ele não é simplesmente o conhecimento existente na cabeça de um especialista em domínios; <em>ele é uma abstração rigorosamente organizada e seletiva daquele conhecimento.</em></p>\n</blockquote>\n<blockquote>\n<p>O coração do software está na sua capacidade de resolver problemas relacionados ao domínio para o seu usuário. Todas as outras características, por mais vitais que possam ser, se apoiam nessa finalidade básica.</p>\n</blockquote>\n<blockquote>\n<p>A complexidade existente no coração de um software deve ser enfrentada cara a cara. Qualquer tentativa de se fazer o contrário é arriscar a irrelevância.</p>\n</blockquote>\n<blockquote>\n<p>Um desenvolvedor de software tem essa mesma perspectiva quando se depara com um domínio complicado e que nunca foi formalizado. Criar um modelo lúcido que elimine essa complexidade é algo excitante.</p>\n</blockquote>\n<h2>Meu Sumário</h2>\n<p>Mapas são modelos que representam aspectos da realidade. Uma simplificação da realidade. Programas de software estão relacionados com domínios de negócio. Modelos ajudam em estruturar o conhecimento necessário para criar o software de forma simplificada.</p>\n<p>O foco dos modelos é transmitir o conhecimento, e não detalhar todo o conhecimento existente.</p>\n<p>Um modelo DDD tem 3 utilidades básicas:</p>\n<ol>\n<li><em>O modelo e o coração do design dão forma um ao outro</em>: Modelo e implementação tem ligação intima.</li>\n<li><em>O modelo é a espinha dorsal de uma linguagem utilizada por todos os membros da equipe</em>: Linguagem de comunicação entre desenvolvedores e especialistas.</li>\n<li><em>O modelo é um conhecimento destilado</em>: É a forma que a equipe entende e estrutura o sistema. É dinâmico e evolui.</li>\n</ol>\n<p>O coração do software está na capacidade de resolver problemas do domínio. Se o domínio é muito complexo, deve-se enfrentar a complexidade para tentar reduzi-la.</p>\n<h2>UM - Assimilando o conhecimento</h2>\n<blockquote>\n<p>Escrevi um protótipo bastante simples, movido por um framework de teste automatizado. Evitei toda a infraestrutura. Não havia nenhuma persistência e nenhuma interface com o usuário (IU).</p>\n</blockquote>\n<h3>Os ingredientes de uma modelagem eficaz</h3>\n<blockquote>\n<p>É a criatividade das ideias e da experimentação maciça, alavancadas através de uma linguagem baseada em modelos e disciplinada pelo ciclo de feedback, através da implementação, que possibilita encontrar um modelo rico em conhecimento e destila-lo.</p>\n</blockquote>\n<h3>Assimilando o conhecimento</h3>\n<blockquote>\n<p>Essa destilação é uma expressão rigorosa do conhecimento especifico considerado mais relevante.</p>\n</blockquote>\n<blockquote>\n<p>Mas, se não estiverem interessados no domínio, os programadores aprendem somente o que o aplicativo deve fazem e não os princípios existentes por trás dele. Softwares úteis podem ser construídos dessa forma, mas o projeto nunca chegará ao ponto em que novos recursos poderosos revelam como corolários aos antigos recursos.</p>\n</blockquote>\n<blockquote>\n<p>A interação entre os membros da equipe muda à medida que todos os membros digerem aquele modelo em conjunto.</p>\n</blockquote>\n<h3>Aprendizado contínuo</h3>\n<blockquote>\n<p>Quando decidimos escrever um software, nunca sabemos o suficiente.</p>\n</blockquote>\n<blockquote>\n<p>Equipes altamente produtivas aumentam seu conhecimento conscientemente, praticando o aprendizado contínuo.</p>\n</blockquote>\n<h3>Extraindo um conceito oculto</h3>\n<blockquote>\n<p>Um design mais explicito apresenta as seguintes vantagens:</p>\n<ol>\n<li>Para que o design chegue a esse estágio [Explícito], os programadores e todas as outras pessoas envolvidas terão de entender a natureza do overbooking [regra do domínio] como uma regra de negócio distinta e importante, e não simplesmente um cálculo obscuro.</li>\n<li>Os programadores podem mostrar aos especialistas daquele negócio artefatos técnicos, até mesmo códigos, que devem ser inteligíveis para os especialistas de domínio (com um pouco de ajuda), fechando assim um ciclo de feedback.</li>\n</ol>\n</blockquote>\n<h3>Modelos profundos</h3>\n<blockquote>\n<p>A assimilação do conhecimento é uma exploração, e é impossível saber onde se pode chegar.</p>\n</blockquote>\n<h3>Meu Sumário</h3>\n<p>O capitulo começa com uma conversa entre especialistas do domínio e os desenvolvedores. Onde eles desenham juntos um modelo do domínio. Uma vez que o desenvolvedor da conversa tem esse exemplo ele foca em construir esse modelo focando nas camadas de negócios, e usando testes para trazer segurança que o código foi escrito da maneira correta. Isso trouxe uma velocidade para a demonstração do domínio para os especialistas, o que trouxe novos feedbacks e evolução do modelo.</p>\n<p>O que torna uma modelagem eficaz?</p>\n<ol>\n<li>Ligando o modelo e a implementação: Protótipo do modelo para entender viabilidade.</li>\n<li>Cultivando uma linguagem baseada no modelo: Tanto os desenvolvedores quanto os especialistas conhecem e podem utilizar a linguagem do modelo. A linguagem é útil e representa as partes mais relevantes do domínio.</li>\n<li>Desenvolvendo um modelo rico em conhecimento: O modelo não é somente um desenho, ele de fato representa o conhecimento da área de negócio.</li>\n<li>Destilando o modelo: Os conceitos representados no modelo são mutáveis de acordo com a importância que eles tem. Quando param de ser importantes são removidos do modelo. Assim como, novos conceitos são adicionados conforme o entendimento avança.</li>\n<li>Colhendo ideias e experimentando: A linguagem tem um papel fundamental em transformar qualquer conversa em um laboratório de experimentação. Cada frase poem em prova o modelo, e traz visibilidade para possíveis melhorias.</li>\n</ol>\n<p>Ou seja, uma modelagem eficaz, traz transparência e entendimento conjunto para o domínio de negócio. A transparência permite que o domínio seja constantemente exercitado e criticado o que facilita no refinamento do modelo assim como no assimilação do conhecimento.</p>\n<p>Os modeladores de domínio devem buscar ativamente quais são as informações mais importantes do domínio. Não é esperado que uma primeira seja uma versão que representa perfeitamente o domínio, justamente o oposto, vários modelos serão tentados, rejeitados e sofrerão transformações até encontrar um modelo que representa o conhecimento considerado mais importante do domínio.</p>\n<p>O modelador deve ativamente engajar em discussões com especialistas do domínio, não sendo portanto, uma responsabilidade exclusiva dele a assimilação do conhecimento. Metodologias em etapas (método cascata) que separam os desenvolvedores dos especialistas do domínio tendem portanto a alcançar resultados inferiores pela falta de feedback e restrição da informação. Metodologias iterativas, reduzem o problema, e permite a criação de uma aplicação que atende as necessidades, principalmente se a refatoração acontecer conforme o projeto evolui. Entretanto, se não houver uma busca dos desenvolvedores em aprender o domínio, a aplicação nunca chegará a um ponto onde novos recursos sejam construídos com facilidade sobre o que já existe.</p>\n<p>Conforme o entendimento dos desenvolvedores evolui, o modelo acaba sendo refinado e os conhecimentos mais relevantes descritos por ele mudam. Ou seja, o refinamento do modelo faz parte de um processo de aprendizado contínuo e não pontual.</p>\n<p><strong>Palavras chaves</strong>: modelagem eficaz, assimilar conhecimento, aprendizado contínuo, refinamento do modelo, design explicito, modelo profundo.</p>\n<h2>DOIS - Comunicação e uso da linguagem</h2>\n<h3>LINGUAGEM ONIPRESENTE</h3>\n<blockquote>\n<p>Para criar um design flexível e rico em conhecimento é necessário ter uma linguagem versátil compartilhada pela equipe e uma experiência ativa com a linguagem que raramente acontecem em projetos de software.</p>\n</blockquote>\n<blockquote>\n<p>Um projeto enfrenta sérios problemas quendo sua linguagem é ferida.</p>\n<p>Os especialistas daquele domínio utilizam seu jargão enquanto os membros da equipe técnica possuem sua própria linguagem sintonizada para discutir o domínio em termos do design.</p>\n<p>A terminologia das discussões do dia a dia fica desligada da tecnologia embutida no código (que, em última instância, é o produto mais importante de um projeto de software). E até mesmo um única pessoa utiliza uma linguagem diferente na fala e na escrita de forma que as expressões mais incisivas do domínio geralmente aparecem em uma forma transitória que nunca é captda no código ou até mesmo na escrita.</p>\n<p>A tradução enfraquece a comunicação e torna anêmica a assimilação do conhecimento</p>\n<p>Nenhum desses dialetos pode se tornar uma linguagem comum porque nenhum deles atende todas as necesseidades.</p>\n</blockquote>\n<blockquote>\n<p>O vocabuláro dessa LINGUAGEM ONIPRESENTE inclui os nomes das classes e operações de destaque. A LINGUAGEM inclui termos para discutir regras que se tornaram explícitas no modelo. E é suplementada com termos provenientes de princípios de organização de alto nível impostos sobre o modelo.</p>\n</blockquote>\n<h3>Modelando em voz alta</h3>\n<blockquote>\n<p>Á medida que usamos a LINGUAGEM ONIPRESENTE do modelo do domínio as discussões - sobretudo discussões em que os desenvolvedores e os especialistas do domínio esboçam cenários e requisitos - ficamos mais fluentes na linguagem e ensinamos uns aos outros suas nuances.</p>\n</blockquote>\n<h3>Uma equipe, uma linguagem</h3>\n<blockquote>\n<p>Se especialistas de um domínio sofisticados não entenderem o modelo, há algo errado com o modelo.</p>\n</blockquote>\n<blockquote>\n<p>Em um processo Agile, os requisitos evoluem à medida que o projeto anda, pois raramente o conhecimento existo logo no inicio para especificar adeequadamente um aplicativo.</p>\n</blockquote>\n<h3>Documentos e diagramas</h3>\n<blockquote>\n<p>Diagramas são um meio de comunicação e de explicação e facilitam a captação de ideias.</p>\n</blockquote>\n<blockquote>\n<p>O modelo não é o diagrama.</p>\n</blockquote>\n<h3>Documentos de design escritos</h3>\n<blockquote>\n<p>Caa processo Agile tem sua própria filosofia sobre os documetnos. O Extreme Programming defende não usar absolutamente nenhum documento de dessign extra e deixar o código falar por si mesmo.</p>\n</blockquote>\n<blockquote>\n<p>Outros documentos precisam esclarecer o significado, fazer entender as estruturas em larga escala e concetntrar a atenção nos elementos principais.</p>\n</blockquote>\n<blockquote>\n<p>Documentos escritos devem complementar o código e a conversa.</p>\n</blockquote>\n<blockquote>\n<p>Ao manter os documetnos encutos e contrá-los na complementação do código e das conversas, eles podem permanecer conectados ao projeto.</p>\n</blockquote>\n<h3>Base executável</h3>\n<blockquote>\n<p>É preciso ser meticuloso para escrever um código que não simplismente <em>faça</em> a coisa certa mas tambem <em>diga</em> a coisa certa.</p>\n</blockquote>\n<h3>Modelos explanatórios</h3>\n<blockquote>\n<p>O fator propulsor deste livro é que um modelo deve trazer consigo a implementação, o design e a comunicação da equipe.</p>\n</blockquote>\n<blockquote>\n<p>Modelos explanatórios oferecem a liberdade de criar estilos muito mais comunicativos feitos de acordo com um determinado tópico.</p>\n</blockquote>\n<blockquote>\n<p>Não há necessidade de que os modelos explanatórios sejam modelos de objetos, e geralmente é melhor que não sejam.</p>\n</blockquote>\n<h3>Meu Sumário</h3>","frontmatter":{"title":"Domain Driven Design","language":"pt-BR","coverPath":"domain-driven-design","status":"Reading","date":"2021-04-04"}}},{"node":{"html":"<h1>Praise for accelerate</h1>\n<blockquote>\n<p>Whether they recognize it or not, most organization today are in the business of software development in one way, shape, or form. And most are being dragged down by slow lead times, buggy output, and complicated features that add expense and frustrate users. It doesn't need to be this way.</p>\n</blockquote>\n<h1>Foreword by Martin Fowler</h1>\n<blockquote>\n<p>Their evidence refutes the bimodal IT notion that you have to choose between speed and stability - instead, speed depends on stability, so good IT practices gives you both.</p>\n</blockquote>\n<h1>Preface</h1>\n<blockquote>\n<p>improvements in software delivery are possible for every team and in every company, as long as leadership provides consistent support - including time, actions, and resources - demonstrating a true commitment to improvement, and as long as team members commit themselves to work.</p>\n</blockquote>\n<blockquote>\n<p>We also found that throughput and stability move together, and that an organization's ability to make software positively impacts profitability, productivity, and market share. We saw that culture and technical practices matter, ...</p>\n</blockquote>\n<h1>Part One - What We Found</h1>\n<h2>Chapter 1 - Accelerate</h2>\n<blockquote>\n<p>... they are using small teams that work in short cycles and measure feedback from users to build products and services that delight their customers and rapidly deliver value to their organizations. These high performers are working incessantly to get better at what they do, letting no obstacles stand in their path, even in the face of high levels of risk and uncertainty about how they may achieve their goals.</p>\n</blockquote>\n<blockquote>\n<p>DevOps emerged from a small number of organizations facing a wicked problem: how to build secure, resilient, rapidly, evolving distributed systems at scale.</p>\n</blockquote>\n<h3>Focus on capabilities, not maturity</h3>\n<blockquote>\n<p>The key to successful change is measuring and understating the right things with a focus on capabilities - not on maturity.</p>\n</blockquote>\n<blockquote>\n<p>... capabilities are multidimensional and dynamic, allowing different parts if the organization to take a customized approach to improvement, and focus on capabilities that will give them the most benefits.</p>\n</blockquote>\n<blockquote>\n<p>capabilities models focus on key outcomes and how the capabilities, or levers, drive improvements in those outcomes - that is, they are outcome based.</p>\n</blockquote>\n<blockquote>\n<p>Our own research and data have confirmed that the industry is changing: what is good enough and even \"high performing\" today is no longer good enough in the next year.</p>\n</blockquote>\n<h3>The value of adopting DevOps</h3>\n<blockquote>\n<ul>\n<li>46 times more frequent code deployments</li>\n<li>440 times faster lead time from commit to deploy</li>\n<li>170 times faster mean time to recover from downtime</li>\n<li>5 times lower change failure rate (1/5 as likely for a change to fail)</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>High performers understand that they don't have to trade speed for stability or vice versa, because by building quality in they got both.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>The initial assumption is that companies need to accelerate their businesses to remain competitive, otherwise, they will fail. In the attempt to transform and accelerate their businesses using software, these high performer companies reached a problem. How to build systems that are scalable, reliable, stable, secure, ..., while maintaining their agility. That's the beginning of the DevOps movement.</p>\n<p>Even though there are companies exceeding with DevOps, this is not a true for the whole market. And even for the companies that are part of the movement, there is a disconnection between the leadership and the DevOps practitioners. Meaning the potential for value delivery and business acceleration might be greater than expected.</p>\n<p>How to measure the maturity of DevOps?</p>\n<p>Four reasons not to use maturity model:</p>\n<ol>\n<li>Once on mature state, declare the journey done</li>\n<li>Maturity level is not the same across teams and organizations</li>\n<li>Maturity model do not measure the outcome. It usually only measures the technical profit.</li>\n<li>Maturity model do not consider the ever changing nature of IT.</li>\n</ol>\n<p>Use capability model:</p>\n<ol>\n<li>Endless journey. Continually improving and progressing technology and business.</li>\n<li>Customized approach enabling to focus on key capabilities to achieve the most benefit.</li>\n<li>Outcome based.</li>\n<li>Focus on the right capabilities. Never settling for yesterday's accomplishment.</li>\n</ol>\n<p><em>Which</em> capabilities to focus varies. But, it's not because of age and technology, because or developers or operations doing deployments.</p>\n<p>It's astonishingly to see the gap between high performers that are applying DevOps with those that aren't. It was never about stability or speed. By adding quality in the process you achieve both.</p>\n<h2>Chapter 2 - Measuring Performance</h2>\n<blockquote>\n<p>Measuring performance in the domain of software is hard - in part because, unlike manufacturing, the inventory is invisible.</p>\n</blockquote>\n<h3>The flaws in previous attempts to measure performance</h3>\n<blockquote>\n<p>First, they focus on outputs rather than outcome. Second, they focus on individual or local measures rather than team or global ones.</p>\n</blockquote>\n<blockquote>\n<p>Ideally, we should reward developers for solving business problems with the minimum amount of code - and it's even better if we can solve a problem without writing code at all or by deleting code ( perhaps by a business process change).</p>\n</blockquote>\n<blockquote>\n<p>Velocity is designed to be used as a <em>capacity planning tool</em>; for example, it can be used to extrapolate how long it will take the team to complete all the work that has been planned and estimated. However, some managers have also used it as a way to measure team productivity or even to compare teams.</p>\n</blockquote>\n<blockquote>\n<p>Since lead time - a measure of how fast work can be completed - is a productivity metric that doesn't suffer from the drawbacks of the other metrics we've seen, it's essential that we manage utilization to balance it against lead time in an economically optional way.</p>\n</blockquote>\n<h3>Measuring software delivery performance</h3>\n<blockquote>\n<p>In our search for measures of delivery performance that meet these criteria, we settled on four: delivery lead time, deployment frequency, time to restore service and change fail rate.</p>\n</blockquote>\n<blockquote>\n<p>Lead time is the time it takes to go from a customer making a request being satisfied.</p>\n</blockquote>\n<blockquote>\n<p>Strictly, deployment frequency is the reciprocal of batch size - the more frequently we deploy, the smaller the size of the batch.</p>\n</blockquote>\n<blockquote>\n<p>Reducing batch sizes reduces cycle times and variability in flow, accelerates feedback, reduces risk and overhead, improves efficiency, increases motivation and urgency, and reduces costs and schedule growth.</p>\n</blockquote>\n<blockquote>\n<p>... in modern software products and services, which are rapidly changing complex systems, failure is inevitable, so the key question becomes: How quickly can service be restored?</p>\n</blockquote>\n<blockquote>\n<p>Astonishingly, these results demonstrate that there is no trade-off between improving performance and achieving higher levels of stability and quality. Rather, high performers do better at all of these measures. This is precisely what the Agile and Lean movements predict, but much dogma in our industry still rests on the false assumption that moving faster means trading off against other performance goals, rather than enabling and reinforcing them.</p>\n</blockquote>\n<h3>The impact of delivery performance on organizational performance</h3>\n<blockquote>\n<p>Analysis over several years shoes that high-performing organizations were consistently twice as likely to exceed these goals as low performers. This demonstrates that your organization's software delivery capability can in fact provide a competitive advantage to your business.</p>\n</blockquote>\n<blockquote>\n<p>Whatever the mission, how a technology organization performs can predict overall organizational performance.</p>\n</blockquote>\n<blockquote>\n<p>The fact that software delivery performance matter provides a strong argument against outsourcing the development of software that is strategic to your business, and instead bringing this capability into the core of your organization.</p>\n</blockquote>\n<h3>My summary</h3>\n<p>There are tons of frameworks and methodologies that state that they create better software and services. The second chapter tries to establish a scientific definition of what 'good' means in the context software and services, and the processes to create it. After that, they question whether having a  good delivery performance has impact on organizational performance.</p>\n<p>Previous measurement of performance focus productivity. And this have two drawbacks:</p>\n<ol>\n<li>Focus on <em>outputs</em> rather than <em>outcome</em></li>\n<li>Focus on <em>Individual measures</em> rather than <em>global measures</em></li>\n</ol>\n<p>Examples:</p>\n<ul>\n<li>Lines of code: Rewarding developer for most lines of code is bad, because we would rather have a solution with less lines of code (or even none, improvement in the business process)</li>\n<li>Velocity: It's a <em>Capacity Planning Tool</em>. The measures are relative from team to team and eventually teams will try to booster it, which might destroy the collaboration and the ability to use this tool in planning.</li>\n<li>Utilization: High utilization is good until a certain point. The closer to 100% lead time approaches infinity.</li>\n</ul>\n<p>Lead time doesn't suffer from the drawbacks above. It's important to balance utilization with lead time.</p>\n<p>A successful measure of performance must have two characteristics:</p>\n<ol>\n<li>Outcome based: Code that helps to achieve business goals</li>\n<li>Global: Teams are not fighting each other</li>\n</ol>\n<p>Their research points to 4 measures of delivery performance:</p>\n<ul>\n<li>Delivery lead time</li>\n<li>Deployment frequency</li>\n<li>Time to restore service</li>\n<li>Change fail rate</li>\n</ul>\n<h4>Lead time</h4>\n<p>From Lean Theory.</p>\n<p>Two parts:</p>\n<ul>\n<li>\n<p>discovery ( Fuzzy front end ):</p>\n<ul>\n<li>Hypothesis driven delivery, UX and design thinking</li>\n<li>Work that has never done before</li>\n<li>Estimates are highly uncertain</li>\n<li>Outcomes are highly variable</li>\n</ul>\n</li>\n<li>\n<p>delivery: </p>\n<ul>\n<li>Flow from development to production. Standardized work.</li>\n<li>Integration, test, deployment must be performed continually as quick as possible</li>\n<li>Cycle times are well known and predictable</li>\n<li>Outcomes should have low variability.</li>\n</ul>\n</li>\n</ul>\n<p>The shorter the better. Faster feedback, and to adapt plans and change more quickly. Important to fix defects.</p>\n<p>It's measured as the time from code committed to code successfully running in production. The range in their research vary from \"<em>less than one hour</em>\" to \"<em>more than six months</em>\".</p>\n<h4>Batch size</h4>\n<p>From Lean Theory.</p>\n<p>Because the inventory in software development is invisible they used deployment frequency as a proxy. Meaning that the more frequent you deploy software to production smaller are the batches of the changes.</p>\n<p>This metric ranges from \"<em>On demand( multiple deploys per day)</em>\" and \"<em>fewer than once every six months</em>\".</p>\n<h4>Tempo</h4>\n<p>Is composed by the metrics of delivery lead time and deployment frequency.</p>\n<h4>Time to restore service</h4>\n<p>Failure is inevitable in modern systems, therefore, the question changes to how quickly can the system be restored to a healthy state?</p>\n<p>This metric is measured by the same metrics used in lead time. From \"<em>less than one hour</em>\" to \"<em>more than six months</em>\".</p>\n<h4>Change fail percentage</h4>\n<p>What is the percentage of changes to production that introduces fail?</p>\n<p>The same as the <em>Percent complete and accurate</em> in lean.</p>\n<p>This metric is measured by the <em>percentage of changes that cause fail</em>.</p>\n<h4>No trade-off</h4>\n<p>The results in the their research achieve by clustering the data shows that there is no trade off between improving performance and achieving higher levels of stability and quality.</p>\n<p>High-performance excel in all four of the measurement of delivery performance.</p>\n<h4>Twice as likely</h4>\n<p>Organization that are high performers on delivery performance are twice as likely to exceed profitability, market share and productivity, which is highly correlated with ROI.</p>\n<p>Even for non commercial organization, how technology performs can predict overall organizational performance.</p>\n<h4>Outsourcing</h4>\n<p>Because, delivery performance impacts on organizational performance, you have a strong argument to avoid outsourcing the development of software that is strategic to your business, most likely this software should be considered a core of your company.</p>\n<h2>Chapter 3 - Measuring and Changing Culture</h2>\n<h3>Modeling and measuring culture</h3>\n<blockquote>\n<p>Organizational culture can exist at three levels in organizations: basic assumptions, values, and artifacts (Schein 1985).</p>\n</blockquote>\n<blockquote>\n<p>... further insight was that the organizational culture predicts the way information flows through an organization.</p>\n</blockquote>\n<h3>Measuring Culture</h3>\n<blockquote>\n<p>First, in organizations with a generative culture, people collaborate more effectively and there is a higher level of trust both across the organization and up and down the hierarchy. Second, \"generative culture emphasizes the mission, an emphasis that allows people involved to put aside their personal issues and also the departmental issues that are so evident in bureaucratic organizations. The mission is primary. And third, generativity encourages a 'level playing filed,' in which hierarchy plays less of a role\"</p>\n</blockquote>\n<blockquote>\n<p>... the goal of bureaucracy is to \"ensure fairness by applying rules to administrative behavior. The rules would be the same for all cases - no one would receive preferential or discriminatory treatment. Not only that, but the rules would represent the best products of the accumulated knowledge of the organization: Formulated by bureaucrats who were experts in their fields, the rules would impose efficient structures and processes while guaranteeing fairness and eliminating arbitrariness\".</p>\n</blockquote>\n<blockquote>\n<p>Westrum's description of a rule-oriented culture is perhaps best thought of as one where following the rules is considered more important than achieving the mission.</p>\n</blockquote>\n<h3>What does Westrum organizational culture predict?</h3>\n<blockquote>\n<p>Westrum's theory posits that organizations with better information flow function more effectively.</p>\n</blockquote>\n<h3>Consequences of Westrum's theory for technology organizations</h3>\n<blockquote>\n<p>Analysis showed that only lead time, release frequency and time to restore together form a valid and reliable construct.</p>\n</blockquote>\n<blockquote>\n<p>Our goal should be to discover how we could improve information flow, or to find better tools to help prevent catastrophic failures following apparently mundane operations.</p>\n</blockquote>\n<h3>How do we change culture?</h3>\n<blockquote>\n<p>What my ... experience taught me that was so powerful was that the way to change culture is not to first change how people think, bu instead to start by changing how people behave - what they do\".</p>\n</blockquote>\n<blockquote>\n<p>... implementing the practices of these movements (Lean and agile) can have an effect on culture</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This chapter the authors are looking for a scientific definition of culture.</p>\n<p>Culture is formed by three levels in organizations:</p>\n<ul>\n<li>\n<p>Basic assumptions: </p>\n<ul>\n<li>Formed over time as members of a group make sens of relationships, events and activities.</li>\n<li>Least visible. The things we know.</li>\n</ul>\n</li>\n<li>\n<p>Values</p>\n<ul>\n<li>A lens through which group members view and interpret the relationships, events and activities around them.</li>\n<li>Values establish social norms which impact the interactions and practices</li>\n<li>More visible than <em>\"Basic assumptions\"</em>.</li>\n<li>The culture we think of when we talk about the culture of a group of people.</li>\n</ul>\n</li>\n<li>\n<p>Artifacts</p>\n<ul>\n<li>Written mission statements or creeds, technology, formal procedures or even heroes and rituals.</li>\n<li>The most visible.</li>\n</ul>\n</li>\n</ul>\n<p>Their research choose the Westrum model of culture in the second level (Values).</p>\n<p>Westrum describes the following typology of organizational culture:</p>\n<ul>\n<li>\n<p>Pathological (power-oriented): </p>\n<ul>\n<li>Large amount of fear and threat.</li>\n<li>People hoard information or withhold it for political reasons, or distort it.</li>\n</ul>\n</li>\n<li>\n<p>Bureaucratic (rule-oriented):</p>\n<ul>\n<li>Protect departments.</li>\n<li>Departments wants to maintain their autonomy and their rules.</li>\n</ul>\n</li>\n<li>\n<p>Generative (performance-oriented):</p>\n<ul>\n<li>Focus on the mission.</li>\n<li>Everything is subordinated to good performance, to doing what we are supposed to do.</li>\n</ul>\n</li>\n</ul>\n<p>The organizational culture predicts the information flow and performance outcomes.</p>\n<p>A good information flow must have 3 characteristics:</p>\n<ol>\n<li>It provides answers to the questions that the receiver needs answered.</li>\n<li>It is timely.</li>\n<li>It is presented in such a way that it can be effectively used by the receiver.</li>\n</ol>\n<p>Generative culture enables information processing through 3 mechanisms:</p>\n<ol>\n<li>Effective collaboration and trust across different layers of the hierarchy</li>\n<li>Emphasis on the mission to put aside personal or department issues</li>\n<li>Level playing field. Hierarchy plays less of a role.</li>\n</ol>\n<p>In order to a organization culture to impact the information flow there are some requisites:</p>\n<ol>\n<li>Trust and cooperation between people across the organization</li>\n<li>Higher quality decision making due to more information available and the ability to easily reverse wrong decisions, since the culture is more transparent and open.</li>\n<li>The cultural norms are likely to do a better job with people. Problems are more rapidly discovered and improved.</li>\n</ol>\n<p>When trying to build a construct they found that the only valid and reliable construct didn't consider <em>Change fail rate</em>. So software delivery performance only considers <em>Delivery lead time</em>, <em>Deployment frequency</em> and <em>Time to restore</em>. But, consider that <em>Change fail rate</em> is highly correlated with the construct.</p>\n<p>Reliability and ability to quickly innovate are two culture characteristics that are connected. These culture characteristics positively impacts software delivery and organizational performance. For example, a company that uses failures to improve their information flow instead of looking for a responsible for the problem are more likely due to their culture to have better software delivery and organizational performance.</p>\n<p>At last the authors propose that by applying Lean and agile practices you can change the culture of your organization.</p>\n<h2>Chapter 4 - Technical Practices</h2>\n<h3>What is continuous delivery?</h3>\n<blockquote>\n<p>Continuous delivery is a set of capabilities that enable us to get changes of all kinds - features, configuration changes, bug fixes, experiments - into production or into the hands of users <em>safely</em>, <em>quickly</em> and <em>sustainably</em>.</p>\n</blockquote>\n<blockquote>\n<p>A key objective for management is making the state of these system-level outcomes transparent, working with the rest of the organization to set measurable, achievable, time-bound goals for these outcomes, and then helping their teams work toward them.</p>\n</blockquote>\n<blockquote>\n<p>Implementing continuous delivery means crating multiple feedback loops to ensure that high-quality software gets delivered to users more frequently and more reliably.</p>\n</blockquote>\n<h3>The impact of continuous delivery</h3>\n<blockquote>\n<p>However, they also have other significant benefits: they help to decrease deployment pain and team burnout.</p>\n</blockquote>\n<blockquote>\n<p>By giving developers the tools to detect problems when they occur, the time and resources to invest in their development, and the authority to fix problems straight away, we create an environment where developers accept responsibility for global outcomes such as quality and stability.</p>\n</blockquote>\n<blockquote>\n<p>This means that investments in technology are also investments in people, and these investments will make our technology process more sustainable.</p>\n</blockquote>\n<h3>The impact of continuous delivery on quality</h3>\n<blockquote>\n<p>Furthermore, continuous delivery predicts lower levels of unplanned work and rework in a statistically significant way.</p>\n</blockquote>\n<blockquote>\n<p>Unplanned work and rework are useful proxies for quality because they represent a failure to build quality into our products.</p>\n</blockquote>\n<blockquote>\n<p>( Analogy to the car with low fuel alert, and running out of gas in the highway) ... the organization can fix the problem in a planned manner, without much urgency or disruption to other scheduled work. In the second case, they must fix the problem in a highly urgent manner, often requiring all hands on deck - for example, have six engineers drop everything ...</p>\n</blockquote>\n<blockquote>\n<p>demand for work caused by the failure to do the right thing the first time by improving quality of service we provide.</p>\n</blockquote>\n<h3>Continuous delivery practices: What works and what doesn't</h3>\n<blockquote>\n<p>Too many test suites are flaky and unreliable, producing false positives and negatives - it's worth investing ongoing effort into a suite that is reliable.</p>\n</blockquote>\n<blockquote>\n<p>First, the code becomes more testable when developers write tests. This is one of the main reasons why test-driven development (TDD) is an important practice - it forces developers to create more testable designs.</p>\n</blockquote>\n<blockquote>\n<p>In our data, successful teams had adequate test data to run their fully automated test suites and could acquire test data for running automated tests on demand.</p>\n</blockquote>\n<blockquote>\n<p>We should note that GitHub Flow is sustainable for open source projects whose contributors are not working on a project full time.</p>\n</blockquote>\n<h3>Adopting Continuous Delivery</h3>\n<blockquote>\n<p>A critical obstacle to implementing continuous delivery is enterprise and application architecture.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This chapter demonstrates the impact of technical practices on software delivery performance, organizational culture, team burnout and deployment pain.</p>\n<p>Continuous delivery is a set of practices that enable us to ship code into production quick,  safely and sustainably. Continuous delivery creates several feedback loops to ensure shipped code is high-quality and reliable.</p>\n<p>CD Practices are Version control, Deployment automation, Continuous integration, Trunk-based development, Test automation, Test data management, Shift left on security, Loosely coupled architecture, Empowered teams, Monitoring, Proactive notification.</p>\n<p>The 5 key principles of continuous delivery:</p>\n<ul>\n<li>\n<p>Build quality in:</p>\n<ul>\n<li>We invest in building a cultured supported by people and tools that can detect and fix issues quickly and cheaply.</li>\n</ul>\n</li>\n<li>\n<p>Work in small batches:</p>\n<ul>\n<li>Smaller chunks provide faster feedback.</li>\n<li>It's easier to adapt the plans.</li>\n<li>Avoid delivering zero or non value items.</li>\n</ul>\n</li>\n<li>\n<p>Computers perform repetitive tasks; people solve problems:</p>\n<ul>\n<li>Automate repetitive work to reduce cost and errors</li>\n<li>Free people to higher value tasks</li>\n</ul>\n</li>\n<li>\n<p>Relentlessly pursue continuous improvement:</p>\n<ul>\n<li>Continuous improvement</li>\n</ul>\n</li>\n<li>\n<p>Everyone is responsible:</p>\n<ul>\n<li>Close collaboration and everyone involved in the software delivery process</li>\n</ul>\n</li>\n</ul>\n<p>In order to create Continuous delivery, we must build this foundation:</p>\n<ul>\n<li>\n<p>Comprehensive configuration management:</p>\n<ul>\n<li>Infrastructure as a code</li>\n<li>Config, the 3rd principle of the Twelve-factor app</li>\n</ul>\n</li>\n<li>\n<p>Continuous integration (CI):</p>\n<ul>\n<li>short lived feature branches or trunk based development</li>\n<li>Pipelines that ensure code quality and project standard</li>\n</ul>\n</li>\n<li>\n<p>Continuous testing:</p>\n<ul>\n<li>Test as integral part of development</li>\n<li>High amount of test automation </li>\n<li>Testers doing exploratory testing</li>\n</ul>\n</li>\n</ul>\n<p>Their research shows that Continuous Delivery capabilities have the following outcomes:</p>\n<ul>\n<li>A strong positive impact on software delivery performance, which leads to organizational performance.</li>\n<li>Reduces the deployment pain and team burnout.</li>\n<li>Stronger identification with the organization.</li>\n<li>A generative, performance-oriented, culture.</li>\n<li>Lower change fail rate ( Less rework )</li>\n</ul>\n<p>With those outcomes we can also conclude that investing in technology is investing in people. And, applying CD can also be linked to the agile manifest principle: \"<em>Agile processes promote sustainable development. The sponsors, developers, and users should be able to maintain a Technical praticeconstant pace indefinitely</em>\".</p>\n<p>Then the authors also answer the question <em>whether CD increases quality</em>. And the answer is also yes. Not only by reducing the change fail rate. But, also,  High performers applying CD, have more time to spent on New Work, 11% more than lower performers which have to spent more time working on Unplanned Work or Rework.</p>\n<p>Unplanned Work or Rework has a great analogy about a car with low fuel warning, and running out of gas on the highway.</p>\n<p>About the CD practices:</p>\n<ul>\n<li>Version Control: Handle not only code but also configuration as a primary concern for versioning.</li>\n<li>\n<p>Test automation: </p>\n<ul>\n<li>Reliable automated tests. You have to fix flaky and unreliable tests.</li>\n<li>Developers as responsible for test automation.</li>\n<li>TDD, and achieve a more testable design</li>\n<li>Invest more effort maintaining the tests</li>\n</ul>\n</li>\n<li>\n<p>Test Data Management: </p>\n<ul>\n<li>Successful teams must had adequate test data</li>\n</ul>\n</li>\n<li>\n<p>Trunk-based development: </p>\n<ul>\n<li>Short feature branches (less than a day) or trunk based</li>\n<li>No code freezes or stabilization period</li>\n</ul>\n</li>\n<li>\n<p>Information security:</p>\n<ul>\n<li>Integrating security practices into the delivery process contributed to software delivery performance.</li>\n</ul>\n</li>\n</ul>\n<p>Although those practices do have a lot of positive points they do require rethink architecture, how the teams works, relationships, tools, processes and so on...</p>\n<h2>Chapter 5 - Architecture</h2>\n<blockquote>\n<p>We found that High performance is possible with all kinds of systems, provided that systems -and the teams that build and maintain them - are loosely coupled</p>\n</blockquote>\n<h3>Types of systems and delivery performance</h3>\n<blockquote>\n<p>We discovered that low performers were more likely to say that the software they were building - or the set of services they had to interact with - was custom software developed by another company (e.g., an outsourcing partner). Low performers were also more likely to be working on mainframe systems.</p>\n</blockquote>\n<blockquote>\n<p>It's possible to achieve these characteristics even with packaged software and \"legacy\" mainframe systems - and, conversely, employing the latest whizzy microservices architecture deployed on containers is no guarantee of higher performance if you ignore these characteristics.</p>\n</blockquote>\n<blockquote>\n<p>testability and deployability, are important in creating high performance.</p>\n</blockquote>\n<h3>Focus on deployability and testability</h3>\n<blockquote>\n<p>To enable this, we must also ensure delivery teams are cross-functional, with all the skills necessary to design, develop, test, deploy, and operate the system on the team.</p>\n</blockquote>\n<blockquote>\n<p>Architectural approaches that enable this strategy include use of bounded context and APIs as a way to decouple large domains into smaller, more loosely coupled units, and the use of test double and virtualization as a way to test services or components in isolation.</p>\n</blockquote>\n<blockquote>\n<p>Of course DevOps is all about better collaboration between teams, and we don't mean to suggest teams shouldn't work together.</p>\n</blockquote>\n<h3>A loosely coupled architecture enables scaling</h3>\n<blockquote>\n<p>Second, we can substantially grow the size of our engineering organization and increase productivity linearly - or better than linearly - as we do so.</p>\n</blockquote>\n<blockquote>\n<p>(Scaling the frequency of deployments per day) This allows our business to move <em>faster</em> as we add more people, not slow down, as is more typically the case.</p>\n</blockquote>\n<h3>Allow teams to choose their own tools</h3>\n<blockquote>\n<p>However, there is a downside to this lack of flexibility: it prevents teams from choosing technologies that will be most suitable for their particular needs, and from experimenting with new approaches and paradigms to solve their problems.</p>\n</blockquote>\n<blockquote>\n<p>When the tools provided actually make life easier for the engineers who use them, they will adopt them of their own free will.</p>\n</blockquote>\n<h3>Architects should focus on engineers and outcomes, not tools or technologies</h3>\n<blockquote>\n<p>What is important is enabling teams to make changes to their products or services without depending on other teams or systems.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>The architecture of your system can be a barrier to apply continuous delivery practices and increase both tempo and stability.</p>\n<p>Loose coupling is the key architectural property that enables teams to test, deploy, and scale with ease. Also, it's the responsible for ensuring the organization productivity scales as the organization scales.</p>\n<p>The type of system is not a key factor to team's performance. You can achieve either good performance with legacy systems or bad performance with latest microservices architecture. It all depends whether you achieve a loose coupled architecture. In order, to decouple the architecture you could use bounded context and APIs.</p>\n<p>The authors found two main characteristics that are more likely to be found on high performers:</p>\n<ul>\n<li>Testing without required integrated environment (testability)</li>\n<li>Independent Deploy (deployability)</li>\n</ul>\n<p>To achieve that you'll need cross functional teams (designer, ops, sec, devs, ...).</p>\n<p>Organizations should evolve their organization structure to achieve the desired architecture (inverse Conway law). The intent of DevOps is not to avoid collaboration, but that the communication is not a bottleneck for delivering value.</p>\n<p>In addition to delivery performance, a loosely coupled architecture enables linear or better than linear productivity at scale. High performers deploy at a significantly increasing frequency at scale, which allows company to move faster as they add more people.</p>\n<p>Even though, restricting the technologies to a approved list has benefits like reducing the technological complexity, that the teams have all the skills to manage technology's lifecycle and so on. It also means that teams will be restricted without experimentation and they might lack the most suitable tools for their needs.</p>\n<p>The organization must work closely to team, providing tools, libraries, and what they need with a great developer experience. This way organization's developer will choose by their will the tools provided by the organization.</p>\n<p>Architects should be focusing on collaborate closely with teams to enable them to change their products and services without depending on other teams.</p>\n<h2>Chapter 6 - Integrating infosec into delivery lifecycle</h2>\n<blockquote>\n<p>(Referring to bring people together) However this kind of behavior is not limited to just development and operations, it occurs wherever different functions within the software delivery value stream do not work effectively together.</p>\n</blockquote>\n<h3>Shifting Left On Security</h3>\n<blockquote>\n<p>Information security should be integrated into the entire software delivery lifecycle from development through operations.</p>\n</blockquote>\n<blockquote>\n<p>We found that high performers were spending 50% less time remediating security issues than low performers.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Whenever different functions of the value stream does not work effectively together, you should work to bring together these functions.</p>\n<p>The role of Infosec is downplayed in the organizations.</p>\n<p>Shifting security left, or bringing to the beginning of the development increases software delivery performance.</p>\n<p>There are 3 aspects that represent shift left:</p>\n<ol>\n<li>Security reviews are conducted for all major features.</li>\n<li>Security is integrated with the development lifecycle.</li>\n<li>Security team provide tools and processes required to the development and operation team.</li>\n</ol>\n<p>The Rugged manifest describes the practices or the mindset behind organization building survivable, defensible, secure and resilient software.</p>\n<h2>Chapter 7 - Management Practices For Software</h2>\n<h3>Lean Management Practices</h3>\n<blockquote>\n<p>Limit Work In Progress (WIP)</p>\n<p>Visual Management</p>\n<p>Feedback from Production</p>\n<p>Lightweight Change Approvals</p>\n</blockquote>\n<blockquote>\n<p>WIP limits are no good if they don't lead to improvements that increase flow.</p>\n</blockquote>\n<blockquote>\n<p>The central concepts here are the types of information being displayed, how broadly it is being shared, and how easy it is to access. Visibility, and the high-quality communication it enables are key.</p>\n</blockquote>\n<h3>Implement a Lightweight Change Management Process</h3>\n<blockquote>\n<p>We found that approval for high-risk changes was not correlated with software delivery performance. Teams that reported no approval process or used peer review achieved higher software delivery performance.</p>\n</blockquote>\n<blockquote>\n<p>Our recommendation based on these results is to use a lightweight change approval process based on peer review, such as pair programming or intra team code review, combined with a deployment pipeline to detect and reject bad changes.</p>\n</blockquote>\n<blockquote>\n<p>This idea is a form of risk management theater: we check boxes so that when something goes wrong, we can say that at least we followed the process.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Consider as Lean Management practices the following:</p>\n<ol>\n<li>Limiting WIP: Use it to drive process improvements and increase throughput.</li>\n<li>Visual management: Key quality, productive metrics and current status of work visible to everyone. Also, aligning metrics with operational goals.</li>\n<li>Feedback from production: Application performance and infrastructure monitoring.</li>\n<li>Lightweight process</li>\n</ol>\n<p>Limiting WIP by itself does not strongly predict delivery performance. But combined with points 2 and 3 they found a strong effect in delivery performance.</p>\n<p>The teams should focus on using the WIP to highlight the bottlenecks and work to improve the overall stream value.</p>\n<p>The visual management together with feedback from production brings visibility to people working on software and enables a high-quality communication.</p>\n<p>The researchers found that is better a lightweight process for changes such as peer review, pair programming or intra team code review.</p>\n<h2>Chapter 8 - Product Development</h2>\n<h3>Lean Product Development Practices</h3>\n<blockquote>\n<p>Improving your software delivery effectiveness will improve your ability to work in small batches and incorporate customer feedback along the way.</p>\n</blockquote>\n<blockquote>\n<p>The key to working in small batches is to have work decomposed into features that allow for rapid development, instead of complex features developed on branches and released infrequently.</p>\n</blockquote>\n<blockquote>\n<p>Working in small batches enables short lead times and faster feedbacks loops.</p>\n</blockquote>\n<h3>Team Experimentation</h3>\n<blockquote>\n<p>Many development teams working in organizations that claim to be Agile are nonetheless obliged to follow requirements created by different teams. This restriction can create some real problems and can result in products that don't actually delight and engage customers and won't deliver the expected business results.</p>\n</blockquote>\n<blockquote>\n<p>Our analysis showed that the ability of teams to try out new ideas and create and update specifications during the development process, without requiring the approval of people outside th team, is an important factor in predicting organizational performance as measured in terms of profitability, productivity and market share.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Working in small batches, making a flow of work visible, Gather and implement customer feedback and team experimentation are statistically significant to predict higher software delivery performance and organizational performance, as well as improving organizational culture and decreasing burnout.</p>\n<ul>\n<li><em>Working in small batches</em>: Short lead time and reduced feedback loop.</li>\n<li><em>Visible flow of work</em>: Stream of value from business to customer.</li>\n<li><em>Customer feedback</em>: Actively and regularly seeking customer feedback.</li>\n<li><em>Team experimentation</em>: The team uses small batches and the visible flow of work to implement the customer feedback. Increasing the probability of delivering delight to customers.</li>\n</ul>\n<h2>Chapter 9 - Making Work Sustainable</h2>\n<h3>Deployment Pain</h3>\n<blockquote>\n<p>The fear and anxiety that engineers and technical staff feel when they push code into production can tell us a lot about a team's software delivery performance.</p>\n</blockquote>\n<blockquote>\n<p>And the barriers that hide the work of deployment from developers are rarely good, because they isolate developers from the downstream consequences of their work.</p>\n</blockquote>\n<blockquote>\n<p>... the technical practices that improve our ability to deliver software with both speed and stability also reduce the stress and anxiety associated with pushing code to production.</p>\n</blockquote>\n<blockquote>\n<p>First, software is often not written with deployability in mind. A common symptom here is when complex, orchestrated deployments are required because the software expects its environment and dependencies to be set up in a very particular way and does not tolerate any kind of deviation from these expectations, giving little useful information to administrators on what is wrong and why it is failing to operate correctly.</p>\n</blockquote>\n<blockquote>\n<p>Second, the probability of a failed deployment rises substantially when manual changes must be made to production environments as part of the deployment process.</p>\n</blockquote>\n<blockquote>\n<p>Finally, complex deployments often require multiple handoffs between teams, particularly in siloed organizations where database administrators, network administrators, system administrators, infosec, testing/QA and developers all work in separate teams.</p>\n</blockquote>\n<h3>Burnout</h3>\n<blockquote>\n<p>Burnout can make the things we once loved about our work and life seem insignificant and dull.</p>\n</blockquote>\n<blockquote>\n<p>Job stress also affects employers, costing the US economy $300 billion per year in sick time, long-term disability, and excessive job turnover.</p>\n</blockquote>\n<blockquote>\n<p>Technology managers, like so many other well-meaning managers, often try to fix the person while ignoring the work environment, even though changing the environment is far more vital for long-term success.</p>\n</blockquote>\n<blockquote>\n<p>Managers are ultimately responsible for fostering a supportive and respectful work environment, and they can do so by creating a blame-free environment, striving to learn from failures and communicating a shared sense of purpose.</p>\n</blockquote>\n<blockquote>\n<p>Managers and leaders should ask their teams how painful their deployments are and fix the things that hurt the most.</p>\n</blockquote>\n<blockquote>\n<p>Responsibility of a team leader include limiting work in process and eliminating roadblocks for the team so they can get their work done.</p>\n</blockquote>\n<blockquote>\n<p>Investing in training and providing people with the necessary support and resources (including time)to acquire new skills are critical to the successful adoption of DevOps.</p>\n</blockquote>\n<blockquote>\n<p>This means creating a work environment that supports experimentation, failure, and learning, and allows employees to make decisions that affect their jobs.</p>\n</blockquote>\n<blockquote>\n<p>When organizational values and individual values aren't aligned, you are more likely to see burnout in employees, particularly in demanding and high-risk work like technology.</p>\n</blockquote>\n<blockquote>\n<p>(Lack of alignment between individuals and organizational values) This is an area of potential impact that organizations neglect at their own peril</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Deployment pain is the fear and anxiety that engineers feel to push code to production.</p>\n<p>The Microsoft team, found an increase of 38% in work/life balance satisfaction score, after they started to apply CD practices.</p>\n<p>The development team should be aware of the software deployment process. If they don't, it could be an warning that software delivery performance could be low.</p>\n<p>CD reduces the deployment pain, which creates a better IT performance, organizational performance and organizational culture.</p>\n<p>Deployments outside business hours are a good signal of deployment pain.</p>\n<p>Pillars of deployment pain:</p>\n<ul>\n<li>Software not build with deployability in mind: Requires the environments to be in a very specific state otherwise won't work. The system don't provide enough information to troubleshoot.</li>\n<li>Manual changes in production: Human are prone to errors. Manual steps allow a deviation between environments.</li>\n<li>Handoffs between teams: Loss of information, delay in delivery, complex deployment process.</li>\n</ul>\n<p>To reduce deployment pain:</p>\n<ul>\n<li>Reproducibility: Build systems that are able to be easily reproduced in multiple environments.</li>\n<li>Fault tolerant: The system can detect and tolerate failures.</li>\n<li>Automate: Ideally it should have near to none manual steps.</li>\n</ul>\n<p>To avoid burnout, manager should concentrate on:</p>\n<ul>\n<li>Learning with failure</li>\n<li>Communicating a strong sense of purpose</li>\n<li>Investing in employee development</li>\n<li>Removing obstacles</li>\n<li>Giving employee time, space and resources to experiment and learn</li>\n</ul>\n<p>Six organizational risk factors that predict burnout:</p>\n<ol>\n<li>Work Overload: excessive job demands</li>\n<li>Lack of control: unable to change influence decisions</li>\n<li>Insufficient reward: financial, institutional or social rewards.</li>\n<li>Breakdown of community: unsupportive workplace.</li>\n<li>Absence of fairness: lack of fairness in decision making.</li>\n<li>Value conflicts: mismatch organizational values.</li>\n</ol>\n<p>Improving technical practices contribute to reduce the feelings of burnout.</p>\n<p>According to their own research, to reduce or fight burnout:</p>\n<ol>\n<li>\n<p>Organizational Culture: </p>\n<ol>\n<li>Avoid pathological or power-oriented culture.</li>\n<li>Create a blame-free environment with a transparent purpose (human error is never the root cause).</li>\n</ol>\n</li>\n<li>\n<p>Deployment pain:</p>\n<ol>\n<li>Avoid out of business hour deploy.</li>\n<li>Tackle the deployment pain. Managers should hear the team's pain and help to fix it.</li>\n</ol>\n</li>\n<li>\n<p>Effectiveness of leaders:</p>\n<ol>\n<li>Reduce WIP</li>\n<li>Remove obstacles of teams</li>\n</ol>\n</li>\n<li>\n<p>Organizational investments in DevOps:</p>\n<ol>\n<li>Investing in training</li>\n<li>Providing people the necessary support (including time)</li>\n</ol>\n</li>\n<li>\n<p>Organizational performance:</p>\n<ol>\n<li>Create work environment with support to experimentation, failure and learning</li>\n<li>Enable employees to change their reality</li>\n</ol>\n</li>\n</ol>\n<p>It's also very important to have the organization's values aligned with the employee values. This lessens the effects of the burnout.</p>\n<h2>Chapter 10 -  Employee satisfaction, identity and engagement</h2>\n<blockquote>\n<p>With market pressures to deliver technologies and solutions even faster the importance of hiring, retaining, and engaging our workforce is greater than ever.</p>\n</blockquote>\n<h3>Employee Loyalty</h3>\n<blockquote>\n<p>Our research found that employees in high-performing organization were 2.2 times more likely to recommend their organization as a great place to work.</p>\n</blockquote>\n<blockquote>\n<p>... when employees see the connection between the work they do and its positive impact on customers, they identify more strongly with the company's purpose, which leads to better software delivery and organizational performance.</p>\n</blockquote>\n<h3>Changing organizational culture and identity</h3>\n<blockquote>\n<p>People are an organization's greatest asset - yet so often they're treated like expendable resources.</p>\n</blockquote>\n<blockquote>\n<p>by creating higher levels of software delivery performance, we increase the rate at which teams can validate their ideas, creating higher levels of job satisfaction and organizational performance.</p>\n</blockquote>\n<blockquote>\n<p>This is in contrast to the way many companies still work: requirements are handed down to development teams who must then deliver large stacks of work in batches. In this model , employees feel little control over the products they build and the customer outcomes they create, and little connections to the organizations they work for.</p>\n</blockquote>\n<blockquote>\n<p>Our analysis is clear: in today's fast-moving and competitive world, the best thing you can do for your products, your company, and your people is institute a culture of experimentation and learning, and invest in the technical and management capabilities that enable it.</p>\n</blockquote>\n<h3>How does job satisfaction impact organizational performance?</h3>\n<blockquote>\n<p>The cycle of continuous improvement and learning is what sets successful companies apart, enabling them to innovate, get ahead of the competition - and win.</p>\n</blockquote>\n<blockquote>\n<p>Automation matters because it gives over to computers the things computers are good at- rote tasks that require no thinking and that in fact are done better when you don't think too much about them.</p>\n</blockquote>\n<h3>Diversity in tech - what our research found</h3>\n<blockquote>\n<p>It is also important to note that diversity is not enough. Teams and organizations must also be inclusive. An inclusive organization is one where \"all organizational members feel welcome and valued for who they are and what they 'bring to the table'.\"</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>33% reported working on teams with no women.</li>\n<li>56% reported working on teams that were less than 10% female</li>\n<li>81% reported working on teams that were less than 25% female</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Women and underrepresented minorities report harassment, microaggressions, and unequal pay. These are all things we can actively watch for and improve as leaders and peers.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>The people are the most important asset of an organization. Therefore, you should ensure hiring, retaining and engaging workforce is as smooth as possible. You can use the employee Net Promoter Score (eNPS) to measure employee loyalty.</p>\n<p>Enabling workers to do their best, fostering continuous delivery and lean practices, creates an identity between the employee and the organization, which leads to organizational performance. Employees are more likely to go get the extra mile to achieve success.</p>\n<p>There are two virtuous cycle to look for:</p>\n<ul>\n<li>The higher the software delivery performance the faster the teams can validate ideas and deliver software, this creates a higher level of job satisfaction which leads to better organizational performance and software delivery performance</li>\n<li>Investments in technology, like CD and Lean help to reduce burnout, which lead to higher level of job satisfaction which leads to better organizational performance and enables more investments in technology.</li>\n</ul>\n<p>The default work model for many companies is the exact opposite of the virtuous cycle. Where there are lots of top down decisions and handover of work. The teams have little to none autonomy to experiment and learn.</p>\n<p>You should aim for a culture with high experimentation and learning. In addition, applying continuous improvement enables the teams to innovate and get ahead of competition.</p>\n<p>Diversity also plays a role in better performance. Other research shows that teams with more diversity achieve better performance and better business outcomes. Organizations must be not only diverse but also inclusive. Everyone in the organization should feel welcome and valued.</p>\n<p>91% of the respondents considered themselves Male. 6% Female and 3% Non-binary or others. 77% of the respondents don't consider to be part of a underrepresented group. These numbers are an example of the lack of diversity in the technology field.</p>\n<p>Not only there are few women and underrepresented groups, but they also suffer from harassment, microaggressions and unequal pay.</p>\n<h2>Chapter 11 - Leaders and Managers</h2>\n<h3>Transformational Leadership</h3>\n<blockquote>\n<p>A good leadership affects a team's ability to deliver code, architect good systems, and apply Lean principles to how the team manages its work and develops products.</p>\n</blockquote>\n<blockquote>\n<p>...transformational leadership is essential for:</p>\n<ul>\n<li>Establishing and supporting generative and high-trust cultural norms.</li>\n<li>Creating technologies that enable developer productivity, reducing code deployment lead times and supporting more reliable infrastructures</li>\n<li>Supporting team experimentation and innovation, and creating and implementing better products faster.</li>\n<li>Working across organizational silos to achieve strategic alignment</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Leaders are those who set the tone of the organization and reinforce the desired cultural norms.</p>\n</blockquote>\n<blockquote>\n<p>...the five characteristics of a transformational leader are:</p>\n<ul>\n<li>Vision. Has a clear understanding of where the organization is going and where it should be in five years.</li>\n<li>Inspirational communication. Communicates in a way that inspires and motivates, even in an uncertain or changing environment.</li>\n<li>Intellectual stimulation. Challenges followers to think about problems in new ways.</li>\n<li>Supportive leadership. Demonstrates care and consideration of followers' personal need and feelings.</li>\n<li>Personal recognition. Praises and acknowledges achievement of goals and improvements in work quality; personally compliments others when they do outstanding work.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>High-performing teams reported having leaders with the strongest behaviors across all dimensions: vision, inspirational communication, intellectual stimulation, supportive leadership and Personal recognition.</p>\n</blockquote>\n<blockquote>\n<p>Though we often hear stories of DevOps and technology transformation success coming from the grassroots, it is far easier to achieve success when you have leadership support.</p>\n</blockquote>\n<blockquote>\n<p>This makes sense, because <em>leaders cannot achieve goals on their own</em> .</p>\n</blockquote>\n<h3>The Role of Managers</h3>\n<blockquote>\n<p>Managers, in particular, play a critical role in connecting the strategic objectives of the business to the work their teams do.</p>\n</blockquote>\n<h3>Tips to improve culture and support your teams</h3>\n<blockquote>\n<p>As the real value of a leader or manager is manifest how they amplify the work of their teams, perhaps the most valuable work they can do is growing and supporting a strong organizational culture among those they serve: their teams.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>A good leadership eases the technological transformation of a company. It's possible to achieve it, without a good leadership, but the research found that is far easier when you have a leadership that sets a clear the vision with a inspirational communication. The leadership and it's vision challenges the intellectual of the organization's staff, but also supports and recognize their needs and contributions.</p>\n<p>The leaders cannot achieve by their own the results, they have to amplify their team's results in order tho achieve the transformation.</p>\n<p><strong>Investing in your team :</strong></p>\n<ul>\n<li>Existing resource made available and accessible to everyone.</li>\n<li>Dedicated training budget</li>\n<li>Foster participation in tech conferences</li>\n<li>Internal hack-days</li>\n<li>\"Yak days\" - slack time.</li>\n<li>Regular DevOps mini conference</li>\n<li>Dedicated time to experiment new tools and technologies - PDI.</li>\n</ul>\n<h1>Part Two - The Research</h1>\n<h2>Chapter 12 - The Science Behind This Book</h2>\n<blockquote>\n<p>Secondary research reports can be valuable, particularly if the existing data is difficult to find, the summary is particularly insightful or the reports are delivered at regular intervals.</p>\n</blockquote>\n<blockquote>\n<p>In contrast, primary research involves collecting new data by the research team.</p>\n</blockquote>\n<blockquote>\n<p>Qualitative research is any kind of research whose data isn't in numerical form.</p>\n</blockquote>\n<blockquote>\n<p>Quantitative research is any kind of research with data that includes numbers.</p>\n</blockquote>\n<blockquote>\n<p>descriptive findings are only as good as the underlying research design and data collection methods.</p>\n</blockquote>\n<blockquote>\n<p>Many people have heard the phrase \"correlation doesn't imply causation\" but what does that mean? The analyses done in the exploratory stage include correlation but not causation. Correlation looks at how closely two variables move together - or don't - but it doesn't tell us if one variable's movement predicts or causes the movement in another variable.</p>\n</blockquote>\n<blockquote>\n<p>(Causal analysis) This type of analysis generally requires randomized studies. A common type of causal analysis done in business is A/B testing in prototyping or websites, when randomized data can be collected and analyzed.</p>\n</blockquote>\n<h2>Chapter 13 - Introduction to Psychometrics</h2>\n<blockquote>\n<p>(About surveys) Common weaknesses are:</p>\n<ul>\n<li>Leading questions. Survey questions should let the respondent answer without biasing them in a direction. For example, \"How would you describe Napoleon's height?\" is better than \"Was Napoleon short?\"</li>\n<li>Loaded questions. Questions should not force respondents into an answer that isn't true for them. For example, \"Where did you take your certification exam?\" doesn't allow for the possibility that they didn't take a certification exam.</li>\n<li>Multiple questions in one. Questions should only ask one thing. For example, \"Are you notified of failures by your customers and the NOC?\" doesn't tell you which part of the quest your respondent was answering for. Customers? the NOC? Both? Or if no, neither?</li>\n<li>Unclear language. Survey questions should use language that your respondents are familiar with, and should clarify and provide examples when necessary.</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>A good example of something that can't be measured directly is organizational culture. We can't take a team's or an organization's organizational culture \"temperature\" - we need to measure culture by measuring its component parts (called manifest variables), and we measure these component parts through survey questions.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Latent constructs help us think carefully about what we want to measure and how we define our constructs.</li>\n<li>They give us several views into the behavior and performance of the system we are observing, helping us eliminate rogue data.</li>\n<li>They make it more difficult or a single bad data source(whether through misunderstanding or a bad actor) t skew our results.</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>Take a step back and think about <em>what</em> it is you are truing to measure and how you will measure.</p>\n</blockquote>\n<h2>Chapter 14 - Why Use a Survey</h2>\n<blockquote>\n<p>But if you want to know how they <em>feel</em> about the work environment and how supportive it is to their work and their goals - if you want to know <em>why</em> they're behaving in the way you observe - you have to ask them.</p>\n</blockquote>\n<h2>Chapter 15 - The Data for the Project</h2>\n<blockquote>\n<p>Similarly, having an organizational culture that values transparency, trust and innovation is likely to have positive impacts in technology organizations regardless f software development paradigm</p>\n</blockquote>\n<h1>Part Three - Transformation</h1>\n<h2>Chapter 16 - High-Performance Leadership and Management</h2>","frontmatter":{"title":"Accelerate","language":"en-US","coverPath":"accelerate","status":"Read","date":"2021-03-25"}}},{"node":{"html":"<h1>Foreword to the First Edition</h1>\n<blockquote>\n<p>Frameworkers know that a framework won't be right the first time around - it must evolve as they gain experience. They also know that the code will be read and modified more frequently than it will be written. The key to keeping code readable and more modifiable is refactoring - for frameworks, in particular, but also for software in general.</p>\n</blockquote>\n<blockquote>\n<p>So what's the problem ? Simply this: Refactoring is risky. It requires changes to working code that can introduce subtle bugs. Refactoring if not done properly, can set you back days, even weeks. And refactoring becomes riskier when practiced informally or ad hoc. You start digging in the code. Soon you discover new opportunities for change, and you dig deeper. The more you dig, the more stuff you turn up ... and the more changes you make. Eventually you dig yourself into a hole you can't get out of.</p>\n</blockquote>\n<h1>Preface</h1>\n<blockquote>\n<p>Yes, the design was a bit more \"pure\" an a bit more \"clean\". But the project had to ship code that -worked, not code that would please an academic.</p>\n</blockquote>\n<blockquote>\n<p>Refactoring is the process of changing a software system in a way that does not alter the external behavior of the code yet improves its internal structure. It is a discipled way to clean up code that minimizes the changes of introducing bugs.</p>\n</blockquote>\n<blockquote>\n<p>In essence, when you refactor, you are improving the design of the code after it has been written.</p>\n</blockquote>\n<h1>Chapter 1 - Refactoring: A first example</h1>\n<blockquote>\n<p>A poorly designed system is hard to change - because it is difficult to figure out what to change and how these changes will interact with the existing code to get the behavior I want. And if it is hard to figure out what to change, there is a good chance that I will make mistakes and introduce bugs.</p>\n</blockquote>\n<blockquote>\n<p>When you have to add a feature to a program but the code is not structured in a convenient way, first refactor the program to make it easy to add the feature, then add the feature.</p>\n</blockquote>\n<blockquote>\n<p>Let me stress that it's these changes that drive the need to perform refactoring. If the code works and doesn't ever need to change, it's perfectly fine to leave it alone. It would be nice to improve it, but unless someone does need to understand it, it isn't causing any real harm. Yet as soon as someone does need to understand how that code works and struggles to follow it, then you have to do something about it.</p>\n</blockquote>\n<blockquote>\n<p>Whenever I do refactoring, the first step is always the same. I need to ensure I have a solid set of tests for that section of code. The tests are essential because even though I will follow refactorings structured to avoid most of the opportunities for introducing bugs, I'm still human and still make mistakes.</p>\n</blockquote>\n<blockquote>\n<p>This is the essence of the refactoring process: small changes and testing after each change.</p>\n</blockquote>\n<blockquote>\n<p>Is this renaming worth the effort? Absolutely. Good code should clearly communicate what it is doing and variable names are a key to clear code.</p>\n</blockquote>\n<blockquote>\n<p>Any fool can write code that a computer can understand. Good programmers write code that humans can understand.</p>\n</blockquote>\n<blockquote>\n<p>It's hard to get names right the first time, so I use the best name I can think of for the moment, and don't hesitate to rename it later.</p>\n</blockquote>\n<blockquote>\n<p>The performance of software usually depends on just a few parts of the code, and changes anywhere else don't make an appreciable difference.</p>\n</blockquote>\n<blockquote>\n<p>At the moment, I'm just making a copy of the performance object, but I'll shortly add to this new record. I take a copy because I don't want to modify the data passed into the function. I prefer to treat data a immutable as much as I can - mutable state quickly becomes something rotten.</p>\n</blockquote>\n<blockquote>\n<p>Brevity is the soul of wit, but clarity is the soul of evolvable software. Adding this modularity allows to me to support the HTML version of the code without any duplication of the calculation.</p>\n</blockquote>\n<blockquote>\n<p>When programming, follow the camping rule: Always leave the code base healthier than when you found it.</p>\n</blockquote>\n<blockquote>\n<p>Read the code, gain some insight, and use refactoring to move that insight from tour head back into the code. The clearer code then makes it easier to understand it, leading to deeper insights and a beneficial positive feedback loop.</p>\n</blockquote>\n<blockquote>\n<p>I believe however that we can go beyond taste and say that the true test of good code is how easy it is to change it. Code should be obvious: When someone needs to make a change, they should be able to find the code to be changed easily and to make the change quickly without introducing any errors.</p>\n</blockquote>\n<blockquote>\n<p>The key to effective refactoring is recognizing that you go faster when you take tiny steps, the code is never broken, and you can compose those small steps into substantial changes. Remember that - and the rest is silence.</p>\n</blockquote>\n<h1>Chapter 2 - Principles in Refactoring</h1>\n<h2>Defining Refactoring</h2>\n<blockquote>\n<p>Refactoring (noun): a change made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior</p>\n</blockquote>\n<blockquote>\n<p>Refactoring(verb): to restructure software by applying a series of refactorings without changing its observable behavior.</p>\n</blockquote>\n<blockquote>\n<p>I use \"restructuring\" as a general term to mean any kind of reorganization or cleaning up of a code base, and see refactoring as a particular kind of restructuring.</p>\n</blockquote>\n<blockquote>\n<p>If someone says their code was broken for a couple of days while they are refactoring, you can be pretty sure they were not refactoring</p>\n</blockquote>\n<blockquote>\n<p>Refactoring is very similar to performance optimization, as both involve carrying out code manipulations that don't change the overall functionality of he program. Refactoring is always done to make the \"easier to understand and cheaper to modify\".</p>\n</blockquote>\n<h2>The Two Hats</h2>\n<blockquote>\n<p>Kent beck came up with a metaphor of the two hats. When i use refactoring to develop software, I divide my time between two distinct activities: adding functionality and refactoring. When I add functionality, I shouldn't be changing existing code; I'm just adding new capabilities.</p>\n</blockquote>\n<h2>Why Should We Refactor?</h2>\n<blockquote>\n<p>Without refactoring, the internal design - the architecture - of software tends to decay. As people change code to achieve short-term goals, often without a full comprehension of the architecture, the code loses its structure. It becomes harder for me to see the design by reading the code. Loss of the structure of code has a cumulative effect. </p>\n</blockquote>\n<blockquote>\n<p>The harder it is to see the design in the code, the harder it is for me to preserve it, and the more rapidly it decays. Regular refactoring helps keep the code in shape.</p>\n</blockquote>\n<blockquote>\n<p>By eliminating duplication, I ensure that the code says everything once and only once, which is the essence of good design.</p>\n</blockquote>\n<blockquote>\n<p>That user, who we often forget, is actually the most important. Who cares if the computer takes a few more cycles to compile something? Yet it does matter if it takes a programmer a week to make a change that would have taken only an hour with proper understanding of my code.</p>\n</blockquote>\n<blockquote>\n<p>By clarifying the structure of the program, I clarify certain assumptions I've made - to a point where even I can't avoid spotting the bugs.</p>\n</blockquote>\n<blockquote>\n<p>In the end, all the earlier points come down to this: Refactoring helps me develop code more quickly.</p>\n</blockquote>\n<blockquote>\n<p>I refer to this effect as the Design Stamina Hypothesis: By putting our effort into a good internal design, we increase the stamina of the software effort, allowing us to go faster for longer.</p>\n</blockquote>\n<h2>When Should We Refactor?</h2>\n<blockquote>\n<p>The Rule of Three</p>\n<p>Here's a a guideline Don Roberts gave me: The first time you do something you just do it. The second time you do something similar, you wince at duplication, but you do the duplicate thing anyway. The third time you do something similar, you refactor. </p>\n<p>Or for those who like baseball: Three strikes, then you refactor.</p>\n</blockquote>\n<blockquote>\n<p>The best time to refactor is just before I need to add a new feature to the code base.</p>\n</blockquote>\n<blockquote>\n<p>The same happens when fixing a bug, Once I've found the cause of the problem, I see that it would be much easier to fix should I unify the three bits of copied code causing the error into one.</p>\n</blockquote>\n<blockquote>\n<p>Before I can change some code, I need to understand what it does. This code may have been written by me or by someone else. Whenever I have to think to understand what the code is doing, I ask myself if I can refactor the code  to make that understanding more immediately apparent.</p>\n</blockquote>\n<blockquote>\n<p> ... by refactoring I move the understanding from m head into the code itself. I then test that understanding by running the software to see if it still works. If I move my understanding into the code, it will be preserved longer and be visible to my colleagues.</p>\n</blockquote>\n<blockquote>\n<p>There's a bit of tradeoff here. I don't want to spend a lot of time distracted from the task I'm currently doing, but I also don't want to leave the trash lying around and getting in the way of future changes. If it's easy to change, I 'll do it right away, If it's a bit more effort to fix, I might make a note of it and fix it when I'm done with my immediate task.</p>\n</blockquote>\n<blockquote>\n<p>This is an important point that's frequently  missed. Refactoring isn't an activity that's separated from programming - any more than you set aside time to write if statements. I don't put time on my plans to do refactoring most refactoring happens while I'm doing other things.</p>\n</blockquote>\n<blockquote>\n<p>You have to refactor when you run into ugly code - but excellent code needs plenty of refactoring too.</p>\n</blockquote>\n<blockquote>\n<p>To add new features, we should be mostly adding new code. But good developer know that, often, the fastest way to add a new feature is to change the code to make it easy to add. Software should thus be never thought of as \"done\". As new capabilities are needed, the software changes to reflect that. Those changes can often be greater in the existing code than in the new code.</p>\n</blockquote>\n<blockquote>\n<p>One bit of advice I've heard is to separate refactoring work and new feature additions into different version-control commits. The big advantage of this is that they can be reviewed and approved independently. I'm not convinced of this, however. Too often, the refactorings are closely interwoven with adding new features, and it's not worth the time to separate them out.</p>\n</blockquote>\n<blockquote>\n<p>(about long-term refactoring) Even in such cases, I'm reluctant to have a team do dedicated refactoring. Often a useful strategy is to agree to gradually work on the problem over the course of the next few weeks, Whenever anyone goes near any code that's in the refactoring zone, they move it a little way in the direction they want to improve.</p>\n</blockquote>\n<blockquote>\n<p>Code reviews help spread knowledge through a development team . Reviews help more experienced developers pass knowledge to those less experienced. They help more people understand more aspects of a large software system, They are also very important in writing clear code.</p>\n</blockquote>\n<blockquote>\n<p>The common pull request model, where a reviewer looks at code without the original author, doesn't work too well. It's better to have the original author of the code present because the author can provide context on the code and fully appreciate the reviewers' intentions for their changes. I've had my best experiences with this by sitting one-on-one with the original author, going through the code and refactoring as we go. The logical conclusion of this style is pair programming: continuous code review embedded within the process of programming.</p>\n</blockquote>\n<blockquote>\n<p>To a manager who is genuinely savvy about technology and understands the design stamina hypothesis , refactoring isn't hard to justify. Such managers should be encouraging refactoring on a regular basis and be looking for signs that indicate a team isn't doing enough.</p>\n</blockquote>\n<blockquote>\n<p>It may sound like I always recommend refactoring - but there are cases when it's not worthwhile.</p>\n</blockquote>\n<blockquote>\n<p>If I run across code that is a mess, but I don't need to modify it, then I don't need to refactor it. Some ugly code that I can treat as an API may remain ugly. It's only when I need to understand how it works that refactoring gives me any benefit.</p>\n</blockquote>\n<blockquote>\n<p>Another case is when it's easier to rewrite it than to refactor it. This is a tricky decision. Often, I can't tell how easy it is to refactor some code unless I spend some time trying and thus get a sense of how difficult it is. The decision to refactor or rewrite requires good judgment and experience, and I can't really boil it down into a piece of simple advice.</p>\n</blockquote>\n<h2>Problems With Refactoring</h2>\n<blockquote>\n<p>Although many people see time spent refactoring as slowing down the development of new features, the whole purpose of refactoring is to speed thing up. But, while this is true, it's also true that the perception of refactoring as slowing this down is still common- and perhaps the biggest barrier to people doing enough refactoring.</p>\n</blockquote>\n<blockquote>\n<p>If you are a tech lead in a team, it's important to show team members that you value improving the health of a code base. That judgment I mentioned earlier on whether to refactor or not is something that takes lots of years of experience to build. The less experience in refactoring need lots of mentoring to accelerate them through the process.</p>\n</blockquote>\n<blockquote>\n<p>We refactor because it makes us faster - faster to add features, faster to fix bugs. It's important to keep the in front of your mind and in front of communication with others. The economic benefits of refactoring should always be the driving factor and the more that is understood by developers, managers, and customers, the more of the \"good design\" curve we'll see.</p>\n</blockquote>\n<blockquote>\n<p>Such function are part of a published interface - an interface that is used by clients independent of those who declare the interface.</p>\n</blockquote>\n<blockquote>\n<p>Code ownership boundaries get in the way of refactoring because I cannot make the kinds of changes I want without breaking my clients.</p>\n</blockquote>\n<blockquote>\n<p>My preference is to allow team ownership of code - so that anyone in the same team can modify the team's code, even if originally written by someone else.</p>\n</blockquote>\n<blockquote>\n<p>The problem of complicated merges gets exponentially worse as the length of features branches increases. Integrating branches that are four weeks old is more than twice as hard as those that are a couple of weeks old. Many people, therefore, argue for keeping feature branches short - perhaps just a couple of days. Others, such as me, want them even shorter that that. This is an approach called Continuous Integration (CI), also known as Trunk-Based Development. With CI, each team member integrates with mainline at least once per day. This prevents any branches diverting too far from each other and this greatly reduces the complexity of merges. CI doesn't come for free: It means you use practices to ensure the feature toggles (aka feature flags) to switch off any in-process features that can't be broken down.</p>\n</blockquote>\n<blockquote>\n<p>Refactorings often involve making lots of little changes all over the code base—which are particularly prone to semantic merge conflicts (such as renaming a widely used function). Many of us have seen feature-branching teams that find refactorings so exacerbate merge problems that they stop refactoring. CI and refactoring work well together, which is why Kent Beck combined them in Extreme Programming.</p>\n</blockquote>\n<blockquote>\n<p>The key here is being able to catch an error quickly. To do this, realistically, I need to be able to run a comprehensive test suite on the code—and run it quickly, so that I'm not deterred from running it frequently. This means that in most cases, if I want to refactor, I need to have self-testing code.</p>\n</blockquote>\n<blockquote>\n<p>The obvious answer to this problem is that you add tests. But while this sounds a simple, if laborious, procedure, it's often much more tricky in practice. Usually, a system is only easy to put under test if it was designed with testing in mind—in which case it would have the tests and I wouldn't be worrying about it.</p>\n</blockquote>\n<blockquote>\n<p>Even when I do have tests, I don't advocate trying to refactor a complicated legacy mess into beautiful code all at once. What I prefer to do is tackle it in relevant pieces. Each time I pass through a section of the code, I try to make it a little bit better—again, like leaving a camp site cleaner than when I found it. If this is a large system, I'll do more refactoring in areas I visit frequently—which is the right thing to do because, if I need to visit code frequently, I'll get a bigger payoff by making it easier to understand.</p>\n</blockquote>\n<blockquote>\n<p>One difference from regular refactorings is that database changes often are best separated over multiple releases to production. This makes it easy to reverse any change that causes a problem in production. So, when renaming a field, my first commit would add the new database field but not use it. I may then set up the updates so they update both old and new fields at once. I can then gradually move the readers over to the new field. Only once they have all moved to the new field, and I've given a little time for any bugs to show themselves, would I remove the now-unused old field.</p>\n</blockquote>\n<h2>Refactoring, Architecture, and Yagni</h2>\n<blockquote>\n<p>The real impact of refactoring on architecture is in how it can be used to form a well-designed code base that can respond gracefully to changing needs. The biggest issue with finishing architecture before coding is that such an approach assumes the requirements for the software can be understood early on. But experience shows that this is often, even usually, an unachievable goal. Repeatedly, I saw people only understand what they really needed from software once they'd had a chance to use it, and saw the impact it made to their work.</p>\n</blockquote>\n<blockquote>\n<p>With refactoring, I can use a different strategy. Instead of speculating on what flexibility I will need in the future and what mechanisms will best enable that, I build software that solves only the currently understood needs, but I make this software excellently designed for those needs. As my understanding of the users' needs changes, I use refactoring to adapt the architecture to those new demands. I can happily include mechanisms that don't increase complexity (such as small, well-named functions) but any flexibility that complicates the software has to prove itself before I include it.</p>\n</blockquote>\n<blockquote>\n<p>Yagni doesn't imply that architectural thinking disappears, although it is sometimes naively applied that way. I think of yagni as a different style of incorporating architecture and design into the development process—a style that isn't credible without the foundation of refactoring.</p>\n</blockquote>\n<h2>Refactoring and the Wider Software Development Process</h2>\n<blockquote>\n<p>To really operate in an agile way, a team has to be capable and enthusiastic refactorers—and for that, many aspects of their process have to align with making refactoring a regular part of their work.</p>\n</blockquote>\n<blockquote>\n<p>The first foundation for refactoring is self-testing code.</p>\n</blockquote>\n<blockquote>\n<p>To refactor on a team, it's important that each member can refactor when they need to without interfering with others' work. This is why I encourage Continuous Integration.</p>\n</blockquote>\n<blockquote>\n<p>With this trio of practices in place, we enable the Yagni design approach that I talked about in the previous section. Refactoring and yagni positively reinforce each other: Not just is refactoring (and its prerequisites) a foundation for yagni—yagni makes it easier to do refactoring. This is because it's easier to change a simple system than one that has lots of speculative flexibility included. Balance these practices, and you can get into a virtuous circle with a code base that responds rapidly to changing needs and is reliable.</p>\n</blockquote>\n<blockquote>\n<p>Stated like this, it all sounds rather simple—but in practice it isn't. Software development, whatever the approach, is a tricky business, with complex interactions between people and machines. The approach I describe here is a proven way to handle this complexity, but like any approach, it requires practice and skill.</p>\n</blockquote>\n<h2>Refactoring and Performance</h2>\n<blockquote>\n<p>The secret to fast software, in all but hard real-time contexts, is to write tunable software first and then tune it for sufficient speed.</p>\n</blockquote>\n<blockquote>\n<p>Here, every programmer, all the time, does whatever she can to keep performance high. This is a common approach that is intuitively attractive—but it does not work very well. Changes that improve performance usually make the program harder to work with. This slows development. This would be a cost worth paying if the resulting software were quicker—but usually it is not.</p>\n</blockquote>\n<blockquote>\n<p>The lesson is: Even if you know exactly what is going on in your system, measure performance, don’t speculate. You’ll learn something, and nine times out of ten, it won’t be that you were right!</p>\n</blockquote>\n<blockquote>\n<p>The interesting thing about performance is that in most programs, most of their time is spent in a small fraction of the code. If I optimize all the code equally, I'll end up with 90 percent of my work wasted because it's optimizing code that isn’t run much. The time spent making the program fast—the time lost because of lack of clarity—is all wasted time.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Refactoring can be either a noun or a verb, respectively:</p>\n<ul>\n<li>A change made to the internal structure to make it cleaner and that do not change its observable behavior</li>\n<li>to restructure software by applying a series of refactorings without changing its observable behavior</li>\n</ul>\n<p>Refactoring is different from restructuring. Restructuring do not imposes any specific technique while refactoring does. Refactoring is one of a set of possible restructuring. Usually if someone has a broken code for a long time, it's not refactoring.</p>\n<p>Refactoring and performance optimization are similar, since both change internal structures without changing the observable behavior.</p>\n<p>There are two types of hats one for adding functionalities another for refactoring.</p>\n<p>We should refactor because:</p>\n<ul>\n<li>Software tends to decay.</li>\n<li>Software's decay is cumulative. The worst the code, more rapidly it decays.</li>\n<li>The worst the code, harder to add functionalities to it.</li>\n<li>The worst the code, more difficult to spot bugs</li>\n</ul>\n<p>Refactoring helps to develop more quickly. The Design Stamina Hypotesis states that the bigger the effort we put into good internal design, we increase the stamina of the software effort, allowing us to go faster for longer.</p>\n<p>The Rule of the three helps to identify when to refactor:</p>\n<ol>\n<li>do it</li>\n<li>you wince at duplication, but, do it anyway</li>\n<li>Refactor</li>\n</ol>\n<p>Three strikes, then you refactor.</p>\n<p>Refactor as a preparatory activity. Just before I need to add a new feature to the code base, or before fixing a bug.</p>\n<p>You should also refactor to gain understand of the code and to make it easier to share this understanding with your colleagues.</p>\n<p>We refactor as we do other things. Refactoring should be an activity just like adding ifs to the code. We avoid long-term refactoring, we try to improve it gradually as people have to go near the refactoring zone. </p>\n<p>Applying the boy scout rule, if the refactoring is small, do it together with the task. If it's big enough, postpone it to immediate after the task is done.</p>\n<p>When refactoring, you can either create two separate changes in two pull requests or just one change with refactoring + new feature. It's important to understand what is more worthy. One might be little slower, but you can review it separately, the other might be faster (and sometimes the only option).</p>\n<p>Clean code, as well, as ugly code, requires refactoring. The code is never done, as we add more features, we might have more changes to the existing code than new code.</p>\n<p>Code review foster knowledge sharing, and eases the writing of clear code. Refactoring works best with a pair-programming-like exercise. Author and reviewer together, on one-on-one, discussing code, refactoring code, and continuously reviewing code.</p>\n<p>The managers should be encouraging refactoring on a regular basis and be looking for signs of lack of it.</p>\n<p>There are cases where refactoring is not worthwhile:</p>\n<ul>\n<li>If you don't have to understand it, don't do it. Code that may remain as API, may remain ugly.</li>\n<li>Whenever is easier to rewrite than to refactor. Although, that's a though question to answer.</li>\n</ul>\n<p>It is a common perception that refactoring is slows down, in fact, the whole purpose of refactoring is to speed things up. As a Tech lead  it's important to show team members that you value improving the health of the code base. Not only value, but support less experience to do it. It's common that not only management, but also developers sacrifice quality. The economic benefit of refactoring is the driving factor do it. </p>\n<p>Code ownership boundaries make it harder to refactor some pieces of code. Code that is a published interface may be used by several independent clients. Most of the time, we should try to have a culture where everyone can do modify the team's code, even if originally written by someone else.</p>\n<p>Besides code ownership and difficulties with management, refactor has a difficult relationship with long lived feature branches. The longer it takes to integrate higher the effort to integrate it. This difficulty increases when we do lots of little semantic changes all over the code base. Therefore, a better approach is to use a trunk based development, with everyone integrating several times a day in the master branch.</p>\n<p>Self-testing code is very important to be able to have a fast feedback when refactoring. For legacy codes, where you don't have the tests and it's difficult to add tests, because the system wasn't designed since the beginning with it, there is no simple routing. But, even with tests, it's important to go slowly when working with legacy code, and start with the areas where you visit the most.</p>\n<p>The perspective for refactoring databases evolve a lot in the last 20 years (since the first edition). When refactoring databases, it's better to have multiple releases to production. This makes easier to revert any changes that caused harm.</p>\n<p>Refactoring does a great impact in the architecture, it's an way to maintain a well-designed code base that flexible enough to respond to changes. Refactor enables you to avoid big complex architecture, because you can start with a simple architecture that resolves the actual problem and you don't have to try to guess the future requirements. When the future comes, you can refactor to apply changes to evolve your system as required by your requirements. The greatest difference that know you know the requirements and you can create a better design. Therefore, refactor enables YAGNI.</p>\n<p>Yagni (you ain't gonna need it) is not about not having architectural thinking. But, perhaps, to postpone big architectural decisions to when you have enough information to take it.</p>\n<p>In the wider development process, a team needs to look for self-testing code. Once the team have that the team is enabled to refactor and evolve the system internal design overtime, as a regular part of their work. To avoid interfering with other's work, it's important to have a CI that integrates constantly the work done by everyone. In addition to that, as said previously, the team may be using pair programming to have a continuously reviewd/refactored code.</p>\n<p>With theses practices, you enable the Yagni design approach, which ease decision making and creates a less complex software. Less complex software that is easier to refactor. Although, it sounds rather simple, it isn't.  But this approach, is a proven approach to handle software development complexity.</p>\n<p>At last when we consider refactoring and performance. The secret to fast software, is to write tunable software first and then tune it for sufficient speed.</p>\n<p>If every developer sacrifices the code design over performance, it is likely that will slow down development. Which would make code more complex and more costly to add more features. Most programs, spend most of their time in a small fraction of the code. Therefore, is better to look for those bottlenecks instead of trying to make 100% of the code with a better performance.</p>\n<p>In addition to that, always measure your system before trying to apply performance optimizations. You should never speculate.</p>\n<h1>Chapter 3 - Bad Smells in Code</h1>","frontmatter":{"title":"Refactoring 2nd edition - Improving the design of existing code","language":"en-US","coverPath":"refactoring-2nd-edition-improving-the-design-of-existing-code","status":"Read","date":"2021-03-16"}}},{"node":{"html":"<h2>Chapter 2</h2>\n<h3>Comparisons</h3>\n<h4>Equal...ish</h4>\n<p>We must be aware of the nuanced differences between an <strong>equality</strong> comparison and an <strong>equivalence</strong> comparison.</p>\n<p><strong>All</strong> value comparisons in JS consider the type of the values being compared, not <em>just</em> the <code>===</code> operator. Specifically, <code>===</code> disallows any sort of type conversion (aka, \"coercion\") in its comparison, where other JS comparisons <em>do</em> allow coercion.</p>\n<h4>Coercive Comparisons</h4>\n<p>Coercion means a value of one type being converted to its respective representation in another type (to whatever extent possible).</p>\n<p>The <code>==</code> operator performs an equality comparison similarly to how the <code>===</code> performs it. In fact, both operators consider the type of the values being compared. And if the comparison is between the same value type, both <code>==</code> and <code>===</code> <strong>do exactly the same thing, no difference whatsoever.</strong></p>\n<p>In other words, they both want to compare values of like types, but <code>==</code> allows type conversions <em>first</em>, and once the types have been converted to be the same on both sides, then <code>==</code> does the same thing as <code>===</code>.</p>\n<p>There's a pretty good chance that you'll use relational comparison operators like <code>&#x3C;</code>, <code>></code> (and even <code>&#x3C;=</code> and <code>>=</code>).</p>\n<h2>Chapter 3</h2>\n<h3>Consuming iterators</h3>\n<p>This operator actually has two symmetrical forms: <em>spread</em> and <em>rest</em> (or <em>gather</em>, as I prefer). The <em>spread</em> form is an iterator-consumer.</p>\n<p>There are two possibilities in JS: an array or an argument list for a function call.</p>\n<p>Maps have a different default iteration than seen here, in that the iteration is not just over the map's values but instead its <em>entries</em>. An <em>entry</em> is a tuple (2-element array) including both a key and a value.</p>\n<p>In the <code>for..of</code> loop over the default map iteration, we use the <code>[btn,btnName]</code> syntax (called \"array destructuring\") to break down each consumed tuple into the respective key/value pairs </p>\n<p>For the most part, all built-in iterables in JS have three iterator forms available: keys-only (<code>keys()</code>), values-only (<code>values()</code>), and entries (<code>entries()</code>).</p>\n<h3>Closure</h3>\n<p>Closure is when a function remembers and continues to access variables from outside its scope, even when the function is executed in a different scope.</p>\n<p>These closures are not a snapshot of the <code>msg</code> variable's value; they are a direct link and preservation of the variable itself. That means closure can actually observe (or make!) updates to these variables over time.</p>\n<h3><code>this</code> Keyword</h3>\n<p>One common misconception is that a function's <code>this</code> refers to the function itself. Because of how <code>this</code> works in other languages, another misconception is that <code>this</code> points the instance that a method belongs to. Both are incorrect.</p>\n<p>when a function is defined, it is <em>attached</em> to its enclosing scope via closure. Scope is the set of rules that controls how references to variables are resolved.</p>\n<p>But functions also have another characteristic besides their scope that influences what they can access. This characteristic is best described as an <em>execution context</em>, and it's exposed to the function via its <code>this</code> keyword.</p>\n<p>Scope is static and contains a fixed set of variables available at the moment and location you define a function, but a function's execution <em>context</em> is dynamic, entirely dependent on <strong>how it is called</strong> (regardless of where it is defined or even called from).</p>\n<p><code>this</code> is not a fixed characteristic of a function based on the function's definition, but rather a dynamic characteristic that's determined each time the function is called.</p>\n<p>The benefit of <code>this</code>-aware functions—and their dynamic context—is the ability to more flexibly re-use a single function with data from different objects.</p>\n<p>A function that closes over a scope can never reference a different scope or set of variables. But a function that has dynamic <code>this</code> context awareness can be quite helpful for certain tasks.</p>\n<h3>Prototypes</h3>\n<p>A prototype is a characteristic of an object,</p>\n<p>Think about a prototype as a linkage between two objects;</p>\n<p>This prototype linkage occurs when an object is created; it's linked to another object that already exists.</p>\n<p>A series of objects linked together via prototypes is called the \"prototype chain.\"</p>\n<p>To define an object prototype linkage, you can create the object using the <code>Object.create(..)</code></p>\n<p>Delegation through the prototype chain only applies for accesses to lookup the value in a property. If you assign to a property of an object, that will apply directly to the object regardless of where that object is prototype linked to.</p>\n<h2>Apendix A</h2>\n<h3>Values vs. References</h3>\n<p>In many languages, the developer can choose between assigning/passing a value as the value itself, or as a reference to the value. In JS, however, this decision is entirely determined by the kind of value. </p>\n<p>primitive values are always assigned/passed as <strong>value copies</strong>.</p>\n<p>By contrast, references are the idea that two or more variables are pointing at the same value, such that modifying this shared value would be reflected by an access via any of those references. In JS, only object values (arrays, objects, functions, etc.) are treated as references.</p>\n<p> Primitives are held by value, objects are held by reference. </p>\n<h3>So Many Functions Forms</h3>\n<p>The function expression here is referred to as an <em>anonymous function expression</em>, since it has no name identifier between the <code>function</code> keyword and the <code>(..)</code> parameter list.</p>\n<p>The <code>name</code> property of a function will reveal either its directly given name (in the case of a declaration) or its inferred name in the case of an anonymous function expression. That value is generally used by developer tools when inspecting a function value or when reporting an error stack trace.</p>\n<p>However, name inference only happens in limited cases such as when the function expression is assigned (with <code>=</code>).</p>\n<p>An anonymous function doesn't have an identifier to use to refer to itself from inside itself—for recursion, event unbinding, etc.</p>\n<p>Since I don't think anonymous functions are a good idea to use frequently in your programs, I'm not a fan of using the <code>=></code> arrow function form. This kind of function actually has a specific purpose (i.e., handling the <code>this</code> keyword lexically), but that doesn't mean we should use it for every function we write. Use the most appropriate tool for each job.</p>\n<h3>Coercive Conditional Comparsion</h3>\n<p><code>if</code> and <code>? :</code>-ternary statements, as well as the test clauses in <code>while</code> and <code>for</code> loops, all perform an implicit value comparison.</p>","frontmatter":{"title":"You Dont know JS - Get Started","language":"en-US","coverPath":"you-dont-know-js-get-started","status":"Read","date":"2021-01-01"}}},{"node":{"html":"<h1>Chapter 1 - What's the Scope?</h1>\n<h2>Compiled vs Interpreted</h2>\n<p>Compiled: Typically, the whole source code is transformed at once, and those resulting instructions are saved as output (usually in a file) that can later be executed.</p>\n<p>Interpretation performs a similar task to compilation, in that it transforms your program into machine-understandable instructions. But the processing model is different. Unlike a program being compiled all at once, with interpretation the source code is transformed line by line; each line or statement is executed before immediately proceeding to processing the next line of the source code.</p>\n<h2>Compiling code</h2>\n<p>In classic compiler theory, a program is processed by a compiler in three basic stages:</p>\n<ol>\n<li>\n<p><strong>Tokenizing/Lexing:</strong> breaking up a string of characters into meaningful (to the language) chunks, called tokens. For instance, consider the program: <code>var a = 2;</code>. This program would likely be broken up into the following tokens: <code>var</code>, <code>a</code>, <code>=</code>, <code>2</code>, and <code>;</code>. Whitespace may or may not be persisted as a token, depending on whether it's meaningful or not.</p>\n<p>(The difference between tokenizing and lexing is subtle and academic, but it centers on whether or not these tokens are identified in a <em>stateless</em> or <em>stateful</em> way. Put simply, if the tokenizer were to invoke stateful parsing rules to figure out whether <code>a</code> should be considered a distinct token or just part of another token, <em>that</em> would be <strong>lexing</strong>.)</p>\n</li>\n<li>\n<p><strong>Parsing:</strong> taking a stream (array) of tokens and turning it into a tree of nested elements, which collectively represent the grammatical structure of the program. This is called an Abstract Syntax Tree (AST).</p>\n<p>For example, the tree for <code>var a = 2;</code> might start with a top-level node called <code>VariableDeclaration</code>, with a child node called <code>Identifier</code> (whose value is <code>a</code>), and another child called <code>AssignmentExpression</code> which itself has a child called <code>NumericLiteral</code> (whose value is <code>2</code>).</p>\n</li>\n<li>\n<p><strong>Code Generation:</strong> taking an AST and turning it into executable code. This part varies greatly depending on the language, the platform it's targeting, and other factors.</p>\n<p>The JS engine takes the just described AST for <code>var a = 2;</code> and turns it into a set of machine instructions to actually <em>create</em> a variable called <code>a</code> (including reserving memory, etc.), and then store a value into <code>a</code>.</p>\n</li>\n</ol>\n<h2>Required two phases</h2>\n<p>While the JS specification does not require \"compilation\" explicitly, it requires behavior that is essentially only practical with a compile-then-execute approach.</p>\n<p>There are three program characteristics you can observe to prove this to yourself: syntax errors, early errors, and hoisting.</p>\n<h2>Compiler speaks</h2>\n<p>Other than declarations, all occurrences of variables/identifiers in a program serve in one of two \"roles\": either they're the <em>target</em> of an assignment or they're the <em>source</em> of a value.</p>\n<p>However, assignment targets and sources don't always literally appear on the left or right of an <code>=</code>, so it's probably clearer to think in terms of <em>target</em> / <em>source</em> rather than <em>left</em> / <em>right</em>.)</p>\n<h3>Target</h3>\n<p>A <code>function</code> declaration is a special case of a <em>target</em> reference. You can think of it sort of like <code>var getStudentName = function(studentID)</code>, but that's not exactly accurate. An identifier <code>getStudentName</code> is declared (at compile time), but the <code>= function(studentID)</code> part is also handled at compilation; the association between <code>getStudentName</code> and the function is automatically set up at the beginning of the scope rather than waiting for an <code>=</code> assignment statement to be executed.</p>\n<h2>Lexical scope</h2>\n<p>If you place a variable declaration inside a function, the compiler handles this declaration as it's parsing the function, and associates that declaration with the function's scope. If a variable is block-scope declared (<code>let</code> / <code>const</code>), then it's associated with the nearest enclosing <code>{ .. }</code> block, rather than its enclosing function (as with <code>var</code>).</p>\n<p>a variable must be resolved as coming from one of the scopes that are <em>lexically available</em> to it; otherwise the variable is said to be \"undeclared\" (which usually results in an error!). If the variable is not declared in the current scope, the next outer/enclosing scope will be consulted. This process of stepping out one level of scope nesting continues until either a matching variable declaration can be found, or the global scope is reached and there's nowhere else to go.</p>\n<p>compilation creates a map of all the lexical scopes that lays out what the program will need while it executes. You can think of this plan as inserted code for use at runtime, which defines all the scopes (aka, \"lexical environments\") and registers all the identifiers (variables) for each scope.</p>\n<h1>Chapter 2 - Illustrating Lexical Scope</h1>\n<p>The term \"lexical\" refers to the first stage of compilation (lexing/parsing).</p>\n<h2>Marbles, and Buckets, and Bubbles... Oh My!</h2>\n<p>References (non-declarations) to variables/identifiers are allowed if there's a matching declaration either in the current scope, or any scope above/outside the current scope, but not with declarations from lower/nested scopes.</p>\n<p>We can conceptualize the process of determining these non-declaration marble colors during runtime as a lookup. </p>\n<h2>Nested scope</h2>\n<p> Scopes can be lexically nested to any arbitrary depth as the program defines.</p>\n<p> Each scope automatically has all its identifiers registered at the start of the scope being executed (variable hoisting)</p>\n<h2>Lookup failures</h2>\n<p>If the variable is a <em>source</em>, an unresolved identifier lookup is considered an undeclared (unknown, missing) variable, which always results in a <code>ReferenceError</code> being thrown. Also, if the variable is a <em>target</em>, and the code at that moment is running in strict-mode, the variable is considered undeclared and similarly throws a <code>ReferenceError</code>.</p>\n<p>The error message for an undeclared variable condition, in most JS environments, will look like, \"Reference Error: XYZ is not defined.\" The phrase \"not defined\" seems almost identical to the word \"undefined,\" as far as the English language goes. But these two are very different in JS, and this error message unfortunately creates a persistent confusion.</p>\n<p>\"Not defined\" really means \"not declared\"—or, rather, \"undeclared,\" as in a variable that has no matching formal declaration in any <em>lexically available</em> scope. By contrast, \"undefined\" really means a variable was found (declared), but the variable otherwise has no other value in it at the moment, so it defaults to the <code>undefined</code> value.</p>\n<h1>Chapter 3 - The scope chain</h1>\n<p>The connections between scopes that are nested within other scopes is called the scope chain, which determines the path along which variables can be accessed. The chain is directed, meaning the lookup moves upward/outward only.</p>\n<h2>\"Lookup\" Is (Mostly) Conceptual</h2>\n<p>Consider a reference to a variable that isn't declared in any lexically available scopes in the current file—see <em>Get Started</em>, Chapter 1, which asserts that each file is its own separate program from the perspective of JS compilation. If no declaration is found, that's not <em>necessarily</em> an error. Another file (program) in the runtime may indeed declare that variable in the shared global scope.</p>\n<p>So the ultimate determination of whether the variable was ever appropriately declared in some accessible bucket may need to be deferred to the runtime.</p>\n<h2>Shadowing</h2>\n<p>Where having different lexical scope buckets starts to matter more is when you have two or more variables, each in different scopes, with the same lexical names. A single scope cannot have two or more variables with the same name; such multiple references would be assumed as just one variable.</p>\n<p>When you choose to shadow a variable from an outer scope, one direct impact is that from that scope inward/downward (through any nested scopes) it's now impossible for any marble to be colored as the shadowed variable.</p>\n<p>It's lexically impossible to reference the global <code>studentName</code> anywhere inside of the <code>printStudent(..)</code> function (or from any nested scopes).</p>\n<h3>Global Unshadowing Trick</h3>\n<p>It <em>is</em> possible to access a global variable from a scope where that variable has been shadowed, but not through a typical lexical identifier reference.</p>\n<h3>Copying Is Not Accessing</h3>\n<p>No. Mutating the contents of the object value via a reference copy is <strong>not</strong> the same thing as lexically accessing the variable itself. We still can't reassign the BLUE(2) <code>special</code> parameter.</p>\n<h3>Illegal Shadowing</h3>\n<p>Not all combinations of declaration shadowing are allowed. <code>let</code> can shadow <code>var</code>, but <code>var</code> cannot shadow <code>let</code></p>\n<p>Summary: <code>let</code> (in an inner scope) can always shadow an outer scope's <code>var</code>. <code>var</code> (in an inner scope) can only shadow an outer scope's <code>let</code> if there is a function boundary in between.</p>\n<h2>Function Name Scope</h2>\n<p>Such a <code>function</code> declaration will create an identifier in the enclosing scope.</p>\n<pre><code class=\"language-javascript\">function askQuestion() {\n    // ..\n}\n</code></pre>\n<p>The same is true for the variable <code>askQuestion</code> being created. But since it's a <code>function</code> expression—a function definition used as value instead of a standalone declaration—the function itself will not \"hoist\".</p>\n<p>A function without a name identifier is referred to as an \"anonymous function expression.\"</p>\n<p>Anonymous function expressions clearly have no name identifier that affects either scope.</p>\n<pre><code class=\"language-javascript\">var askQuestion = function(){\n    // ..\n};\n</code></pre>\n<p>One major difference between <code>function</code> declarations and <code>function</code> expressions is what happens to the name identifier of the function.</p>\n<p>We know <code>askQuestion</code> ends up in the outer scope.</p>\n<p><code>ofTheTeacher</code> is declared as an identifier <strong>inside the function itself</strong></p>\n<p>Not only is <code>ofTheTeacher</code> declared inside the function rather than outside, but it's also defined as read-only</p>\n<pre><code class=\"language-javascript\">var askQuestion = function ofTheTeacher() {\n    console.log(ofTheTeacher);\n};\n\naskQuestion();\n// function ofTheTeacher()...\n\nconsole.log(ofTheTeacher);\n// ReferenceError: ofTheTeacher is not defined\n</code></pre>\n<h2>Arrow Functions</h2>\n<p>Arrow functions are lexically anonymous, meaning they have no directly related identifier that references the function. </p>\n<p>Other than being anonymous (and having no declarative form), <code>=></code> arrow functions have the same lexical scope rules as <code>function</code> functions do.</p>\n<h2>Backing out</h2>\n<p>When a function (declaration or expression) is defined, a new scope is created. The positioning of scopes nested inside one another creates a natural scope hierarchy throughout the program, called the scope chain. The scope chain controls variable access, directionally oriented upward and outward.</p>\n<p>Each new scope offers a clean slate, a space to hold its own set of variables. When a variable name is repeated at different levels of the scope chain, shadowing occurs, which prevents access to the outer variable from that point inward.</p>\n<h1>Chapter 4 - Around the global scope</h1>\n<h2>Why Global Scope?</h2>\n<p>So how exactly do all those separate files get stitched together in a single runtime context by the JS engine?</p>\n<p>First, if you're directly using ES modules (not transpiling them into some other module-bundle format), these files are loaded individually by the JS environment. Each module then <code>import</code>s references to whichever other modules it needs to access. The separate module files cooperate with each other exclusively through these shared imports, without needing any shared outer scope.</p>\n<p>Second, if you're using a bundler in your build process, all the files are typically concatenated together before delivery to the browser and JS engine, which then only processes one big file. Even with all the pieces of the application co-located in a single file, some mechanism is necessary for each piece to register a <em>name</em> to be referred to by other pieces, as well as some facility for that access to occur.</p>\n<p>In some build setups, the entire contents of the file are wrapped in a single enclosing scope, such as a wrapper function, universal module</p>\n<p>And finally, the third way: whether a bundler tool is used for an application, or whether the (non-ES module) files are simply loaded in the browser individually (via <code></code> tags or other dynamic JS resource loading), if there is no single surrounding scope encompassing all these pieces, the <strong>global scope</strong> is the only way for them to cooperate with each other</p>\n<p>Most developers agree that the global scope shouldn't just be a dumping ground for every variable in your application. </p>\n<p>But it's also undeniable that the global scope is an important <em>glue</em> for practically every JS application.</p>\n<h2>Where Exactly is this Global Scope?</h2>\n<p>Different JS environments handle the scopes of your programs, especially the global scope, differently.</p>\n<h3>Browser \"Window\"</h3>\n<p>the global object (commonly, <code>window</code> in the browser)</p>\n<h3>Globals Shadowing Globals</h3>\n<p>A simple way to avoid this gotcha with global declarations: always use <code>var</code> for globals. Reserve <code>let</code> and <code>const</code> for block scopes</p>\n<h3>DOM Globals</h3>\n<p>One surprising behavior in the global scope you may encounter with browser-based JS applications: a DOM element with an <code>id</code> attribute automatically creates a global variable that references it.</p>\n<h3>What's in a (Window) name?</h3>\n<p>We used <code>var</code> for our declaration, which <strong>does not</strong> shadow the pre-defined <code>name</code> global property. That means, effectively, the <code>var</code> declaration is ignored, since there's already a global scope object property of that name. As we discussed earlier, had we used <code>let name</code>, we would have shadowed <code>window.name</code> with a separate global <code>name</code> variable.</p>\n<h3>Web workers</h3>\n<p>Web Workers are a web platform extension on top of browser-JS behavior, which allows a JS file to run in a completely separate thread (operating system wise) from the thread that's running the main JS program.</p>\n<p>Since there is no DOM access, the <code>window</code> alias for the global scope doesn't exist.</p>\n<p>In a Web Worker, the global object reference is typically made using <code>self</code></p>\n<h3>Developer Tools Console/REPL</h3>\n<p>The take-away is that Developer Tools, while optimized to be convenient and useful for a variety of developer activities, are <strong>not</strong> suitable environments to determine or verify explicit and nuanced behaviors of an actual JS program context.</p>\n<h3>ES Modules (ESM)</h3>\n<p>One of the most obvious impacts of using ESM is how it changes the behavior of the observably top-level scope in a file.</p>\n<p>Despite being declared at the top level of the (module) file, in the outermost obvious scope, <code>studentName</code> and <code>hello</code> are not global variables. Instead, they are module-wide, or if you prefer, \"module-global.\"</p>\n<p> It's just that global variables don't get <em>created</em> by declaring variables in the top-level scope of a module.</p>\n<p>The module's top-level scope is descended from the global scope, almost as if the entire contents of the module were wrapped in a function. Thus, all variables that exist in the global scope (whether they're on the global object or not!) are available as lexical identifiers from inside the module's scope.</p>\n<h3>Node</h3>\n<p>The practical effect is that the top level of your Node programs <strong>is never actually the global scope</strong>, the way it is when loading a non-module file in the browser.</p>\n<p>As noted earlier, Node defines a number of \"globals\" like <code>require()</code>, but they're not actually identifiers in the global scope (nor properties of the global object). They're injected in the scope of every module, essentially a bit like the parameters listed in the <code>Module(..)</code> function declaration.</p>\n<p>The only way to do so is to add properties to another of Node's automatically provided \"globals,\" which is ironically called <code>global</code>.</p>\n<h3>Global This</h3>\n<p>As of ES2020, JS has finally defined a standardized reference to the global scope object, called <code>globalThis</code>. So, subject to the recency of the JS engines your code runs in, you can use <code>globalThis</code> in place of any of those other approaches.</p>\n<h1>Chapter 5 - The (Not So) Secret Lifecycle of Variables</h1>\n<h2>When Can I Use a Variable?</h2>\n<p>Recall Chapter 1 points out that all identifiers are registered to their respective scopes during compile time. Moreover, every identifier is <em>created</em> at the beginning of the scope it belongs to, <strong>every time that scope is entered</strong>.</p>\n<p>The term most commonly used for a variable being visible from the beginning of its enclosing scope, even though its declaration may appear further down in the scope, is called <strong>hoisting</strong>.</p>\n<p>The answer is a special characteristic of formal <code>function</code> declarations, called <em>function hoisting</em>. When a <code>function</code> declaration's name identifier is registered at the top of its scope, it's additionally auto-initialized to that function's reference.</p>\n<p>One key detail is that both <em>function hoisting</em> and <code>var</code>-flavored <em>variable hoisting</em> attach their name identifiers to the nearest enclosing <strong>function scope</strong> (or, if none, the global scope), not a block scope.</p>\n<p>Declarations with <code>let</code> and <code>const</code> still hoist. But these two declaration forms attach to their enclosing block rather than just an enclosing function as with <code>var</code> and <code>function</code> declarations. </p>\n<h3>Hoisting: Declaration vs. Expression</h3>\n<p><em>Function hoisting</em> only applies to formal <code>function</code> declarations</p>\n<p>In addition to being hoisted, variables declared with <code>var</code> are also automatically initialized to <code>undefined</code> at the beginning of their scope—again, the nearest enclosing function, or the global. Once initialized, they're available to be used (assigned to, retrieved from, etc.) throughout the whole scope.</p>\n<p>A <code>function</code> declaration is hoisted <strong>and initialized to its function value</strong> (again, called <em>function hoisting</em>). A <code>var</code> variable is also hoisted, and then auto-initialized to <code>undefined</code>. Any subsequent <code>function</code> expression assignments to that variable don't happen until that assignment is processed during runtime execution.</p>\n<h3>Variable Hoisting</h3>\n<p>There's two necessary parts to the explanation:</p>\n<ul>\n<li>the identifier is hoisted,</li>\n<li><strong>and</strong> it's automatically initialized to the value <code>undefined</code> from the top of the scope.</li>\n</ul>\n<h2>Hoisting: Yet Another Metaphor</h2>\n<p>Rather than hoisting being a concrete execution step the JS engine performs, it's more useful to think of hoisting as a visualization of various actions JS takes in setting up the program <strong>before execution</strong>.</p>\n<p>The typical assertion of what hoisting means: <em>lifting</em>—like lifting a heavy weight upward—any identifiers all the way to the top of a scope</p>\n<p>I assert that hoisting <em>should</em> be used to refer to the <strong>compile-time operation</strong> of generating runtime instructions for the automatic registration of a variable at the beginning of its scope, each time that scope is entered.</p>\n<h2>Re-declaration?</h2>\n<p>A repeated <code>var</code> declaration of the same identifier name in a scope is effectively a do-nothing operation. </p>\n<p>It's not just that two declarations involving <code>let</code> will throw this error. If either declaration uses <code>let</code>, the other can be either <code>let</code> or <code>var</code>, and the error will still occur</p>\n<p>In other words, the only way to \"re-declare\" a variable is to use <code>var</code> for all (two or more) of its declarations.</p>\n<h3>Constants?</h3>\n<p>So if <code>const</code> declarations cannot be re-assigned, and <code>const</code> declarations always require assignments, then we have a clear technical reason why <code>const</code> must disallow any \"re-declarations\": any <code>const</code> \"re-declaration\" would also necessarily be a <code>const</code> re-assignment, which can't be allowed!</p>\n<h3>Loops</h3>\n<p>All the rules of scope (including \"re-declaration\" of <code>let</code>-created variables) are applied <em>per scope instance</em>. In other words, each time a scope is entered during execution, everything resets.</p>\n<p>Each loop iteration is its own new scope instance, and within each scope instance, <code>value</code> is only being declared once. So there's no attempted \"re-declaration,\" and thus no error. </p>\n<p>Is <code>value</code> being \"re-declared\" here, especially since we know <code>var</code> allows it? No. Because <code>var</code> is not treated as a block-scoping declaration (see Chapter 6), it attaches itself to the global scope</p>\n<p>The straightforward answer is: <code>const</code> can't be used with the classic <code>for</code>-loop form because of the required re-assignment.</p>\n<h2>Uninitialized Variables (aka, TDZ)</h2>\n<p>With <code>var</code> declarations, the variable is \"hoisted\" to the top of its scope. But it's also automatically initialized to the <code>undefined</code> value, so that the variable can be used throughout the entire scope.</p>\n<p>However, <code>let</code> and <code>const</code> declarations are not quite the same in this respect.</p>\n<p>the <strong>only way</strong> to do so is with an assignment attached to a declaration statement. An assignment by itself is insufficient! </p>\n<p>The term coined by TC39 to refer to this <em>period of time</em> from the entering of a scope to where the auto-initialization of the variable occurs is: Temporal Dead Zone (TDZ).</p>\n<p>The TDZ is the time window where a variable exists but is still uninitialized, and therefore cannot be accessed in any way. Only the execution of the instructions left by <em>Compiler</em> at the point of the original declaration can do that initialization. After that moment, the TDZ is done, and the variable is free to be used for the rest of the scope.</p>\n<p>\"temporal\" in TDZ does indeed refer to <em>time</em> not <em>position in code</em></p>\n<p>The actual difference is that <code>let</code>/<code>const</code> declarations do not automatically initialize at the beginning of the scope, the way <code>var</code> does. The <em>debate</em> then is if the auto-initialization is <em>part of</em> hoisting, or not? I think auto-registration of a variable at the top of the scope (i.e., what I call \"hoisting\") and auto-initialization at the top of the scope (to <code>undefined</code>) are distinct operations and shouldn't be lumped together under the single term \"hoisting.\"</p>\n<p>So to summarize, TDZ errors occur because <code>let</code>/<code>const</code> declarations <em>do</em> hoist their declarations to the top of their scopes, but unlike <code>var</code>, they defer the auto-initialization of their variables until the moment in the code's sequencing where the original declaration appeared. This window of time (hint: temporal), whatever its length, is the TDZ.</p>\n<p>always put your <code>let</code> and <code>const</code> declarations at the top of any scope. Shrink the TDZ window to zero (or near zero) length, and then it'll be moot.</p>\n<h1>Chapter 6 - Limiting Scope Exposure</h1>\n<h2>Least Exposure</h2>\n<p>Principle of Least Privilege expresses a defensive posture to software architecture: components of the system should be designed to function with least privilege, least access, least exposure. If each piece is connected with minimum-necessary capabilities, the overall system is stronger from a security standpoint, because a compromise or failure of one piece has a minimized impact on the rest of the system.</p>\n<p><strong>Naming Collisions</strong>: if you use a common and useful variable/function name in two different parts of the program, but the identifier comes from one shared scope (like the global scope), then name collision occurs, and it's very likely that bugs will occur as one part uses the variable/function in a way the other part doesn't expect.</p>\n<p><strong>Unexpected Behavior</strong>: if you expose variables/functions whose usage is otherwise <em>private</em> to a piece of the program, it allows other developers to use them in ways you didn't intend, which can violate expected behavior and cause bugs.</p>\n<p><strong>Unintended Dependency</strong>: if you expose variables/functions unnecessarily, it invites other developers to use and depend on those otherwise <em>private</em> pieces. </p>\n<p>Declare variables in as small and deeply nested of scopes as possible, rather than placing everything in the global (or even outer function) scope.</p>\n<h2>Hiding in Plain (Function) Scope</h2>\n<p>That means we can name every single occurrence of such a function expression the exact same name, and never have any collision. More appropriately, we can name each occurrence semantically based on whatever it is we're trying to hide, and not worry that whatever name we choose is going to collide with any other <code>function</code> expression scope in the program.</p>\n<h3>Invoking Function Expressions Immediately</h3>\n<p>we're defining a <code>function</code> expression that's then immediately invoked. This common pattern has a (very creative!) name: Immediately Invoked Function Expression (IIFE).</p>\n<p>Unlike earlier with <code>hideTheCache()</code>, where the outer surrounding <code>(..)</code> were noted as being an optional stylistic choice, for a standalone IIFE they're <strong>required</strong>; they distinguish the <code>function</code> as an expression, not a statement. For consistency, however, always surround an IIFE <code>function</code> with <code>( .. )</code>.</p>\n<h4>Function Boundaries</h4>\n<p>So, if the code you need to wrap a scope around has <code>return</code>, <code>this</code>, <code>break</code>, or <code>continue</code> in it, an IIFE is probably not the best approach. In that case, you might look to create the scope with a block instead of a function.</p>\n<h2>Scoping with Blocks</h2>\n<p>In general, any <code>{ .. }</code> curly-brace pair which is a statement will act as a block, but <strong>not necessarily</strong> as a scope.</p>\n<p>A block only becomes a scope if necessary, to contain its block-scoped declarations (i.e., <code>let</code> or <code>const</code>).</p>\n<p>Not all <code>{ .. }</code> curly-brace pairs create blocks (and thus are eligible to become scopes):</p>\n<ul>\n<li>Object literals use <code>{ .. }</code> curly-brace pairs to delimit their key-value lists, but such object values are <strong>not</strong> scopes.</li>\n<li><code>class</code> uses <code>{ .. }</code> curly-braces around its body definition, but this is not a block or scope.</li>\n<li>A <code>function</code> uses <code>{ .. }</code>around its body, but this is not technically a block—it's a single statement for the function body. It <em>is</em>, however, a (function) scope.</li>\n<li>The <code>{ .. }</code> curly-brace pair on a <code>switch</code> statement (around the set of <code>case</code> clauses) does not define a block/scope.</li>\n</ul>\n<p>In most languages that support block scoping, an explicit block scope is an extremely common pattern for creating a narrow slice of scope for one or a few variables. So following the POLE principle, we should embrace this pattern more widespread in JS as well; use (explicit) block scoping to narrow the exposure of identifiers to the minimum practical.</p>\n<p>If you find yourself placing a <code>let</code> declaration in the middle of a scope, first think, \"Oh, no! TDZ alert!\" If this <code>let</code> declaration isn't needed in the first half of that block, you should use an inner explicit block scope to further narrow its exposure!</p>\n<p>But the benefits of the POLE principle are best achieved when you adopt the mindset of minimizing scope exposure by default, as a habit. If you follow the principle consistently even in the small cases, it will serve you more as your programs grow.</p>\n<h3><code>var</code> <em>and</em> <code>let</code></h3>\n<p> That variable is used across the entire function (except the final <code>return</code> statement). Any variable that is needed across all (or even most) of a function should be declared so that such usage is obvious. <code>var</code></p>\n<p><code>var</code> should be reserved for use in the top-level scope of a function.</p>\n<p>Why not just use <code>let</code> in that same location? Because <code>var</code> is visually distinct from <code>let</code> and therefore signals clearly, \"this variable is function-scoped.\" </p>\n<p>Using <code>let</code> in the top-level scope, especially if not in the first few lines of a function, and when all the other declarations in blocks use <code>let</code>, does not visually draw attention to the difference with the function-scoped declaration.</p>\n<p>As long as your programs are going to need both function-scoped and block-scoped variables, the most sensible and readable approach is to use both <code>var</code> <em>and</em> <code>let</code> together, each for their own best purpose.</p>\n<h3>Where To <code>let</code>?</h3>\n<p>\"What is the most minimal scope exposure that's sufficient for this variable?\" Once that is answered, you'll know if a variable belongs in a block scope or the function scope. </p>\n<p>Declaration belongs in a block scope, use <code>let</code>. If it belongs in the function scope, use <code>var</code> (again, just my opinion).</p>\n<p>Placing the <code>var</code> declaration for <code>tmp</code> inside the <code>if</code> statement signals to the reader of the code that <code>tmp</code> belongs to that block. Even though JS doesn't enforce that scoping, the semantic signal still has benefit for the reader of your code.</p>\n<h3>What's the Catch?</h3>\n<p>The <code>err</code> variable declared by the <code>catch</code> clause is block-scoped to that block. This <code>catch</code> clause block can hold other block-scoped declarations via <code>let</code>. But a <code>var</code> declaration inside this block still attaches to the outer function/global scope.</p>\n<h2>Function Declarations in Blocks (FiB)</h2>\n<p>So what about <code>function</code> declarations that appear directly inside blocks? As a feature, this is called \"FiB.\"</p>\n<p>The JS specification says that <code>function</code> declarations inside of blocks are block-scoped, so the answer should be (1).</p>\n<p>As far as I'm concerned, the only practical answer to avoiding the vagaries of FiB is to simply avoid FiB entirely. In other words, never place a <code>function</code> declaration directly inside any block.</p>\n<p>Always place <code>function</code> declarations anywhere in the top-level scope of a function (or in the global scope).</p>\n<p>It's important to notice that here I'm placing a <code>function</code> <strong>expression</strong>, not a declaration, inside the <code>if</code> statement. That's perfectly fine and valid, for <code>function</code> expressions to appear inside blocks. Our discussion about FiB is about avoiding <code>function</code> <strong>declarations</strong> in blocks.</p>\n<p>FiB is not worth it, and should be avoided.</p>\n<h1>Chapter 7 - Using closures</h1>\n<p>Closure builds on this approach: for variables we need to use over time, instead of placing them in larger outer scopes, we can encapsulate (more narrowly scope) them but still preserve access from inside functions, for broader use. Functions <em>remember</em> these referenced scoped variables via closure.</p>\n<h2>See the closure</h2>\n<p>Closure is a behavior of functions and only functions. </p>\n<p>For closure to be observed, a function must be invoked, and specifically it must be invoked in a different branch of the scope chain from where it was originally defined. </p>\n<p>While <code>greetStudent(..)</code> does receive a single argument as the parameter named <code>greeting</code>, it also makes reference to both <code>students</code> and <code>studentID</code>, identifiers which come from the enclosing scope of <code>lookupStudent(..)</code>. Each of those references from the inner function to the variable in an outer scope is called a <em>closure</em>.</p>\n<p>In academic terms, each instance of <code>greetStudent(..)</code> <em>closes over</em> the outer variables <code>students</code> and <code>studentID</code>.</p>\n<p>Closure allows <code>greetStudent(..)</code> to continue to access those outer variables even after the outer scope is finished </p>\n<h3>Pointed closure</h3>\n<p>It's just important not to skip over the fact that even tiny arrow functions can get in on the closure party.</p>\n<h3>Adding Up Closures</h3>\n<p>closure is associated with an instance of a function, rather than its single lexical definition.</p>\n<p>every time the outer <code>adder(..)</code> function runs, a <em>new</em> inner <code>addTo(..)</code> function instance is created, and for each new instance, a new closure.</p>\n<h3>Live Link, Not a Snapshot</h3>\n<p>Closure is actually a live link, preserving access to the full variable itself. We're not limited to merely reading a value;</p>\n<p>the closed-over variable can be updated (re-assigned) as well! </p>\n<p>But <code>greeting()</code> is closed over the variable <code>studentName</code>, not its value. The classic illustration of this mistake is defining functions inside a loop</p>\n<p>Each of the three functions in the <code>keeps</code> array do have individual closures, but they're all closed over that same shared <code>i</code> variable.</p>\n<p>Of course, a single variable can only ever hold one value at any given moment. So if you want to preserve multiple values, you need a different variable for each.</p>\n<p>Each function is now closed over a separate (new) variable from each iteration, even though all of them are named <code>j</code>. And each <code>j</code> gets a copy of the value of <code>i</code> at that point in the loop iteration; that <code>j</code> never gets re-assigned. </p>\n<h3>What if I Can't see It?</h3>\n<p>In fact, global scope variables essentially cannot be (observably) closed over, because they're always accessible from everywhere. No function can ever be invoked in any part of the scope chain that is not a descendant of the global scope.</p>\n<p>All function invocations can access global variables, regardless of whether closure is supported by the language or not. Global variables don't need to be closed over.</p>\n<p>Variables that are merely present but never accessed don't result in closure:</p>\n<h3>Observable Definition</h3>\n<p>Closure is observed when a function uses variable(s) from outer scope(s) even while running in a scope where those variable(s) wouldn't be accessible.</p>\n<p>The key parts of this definition are:</p>\n<ul>\n<li>Must be a function involved</li>\n<li>Must reference at least one variable from an outer scope</li>\n<li>Must be invoked in a different branch of the scope chain from the variable(s)</li>\n</ul>\n<h2>The Closure Lifecycle and Garbage Collection (GC)</h2>\n<p>Since closure is inherently tied to a function instance, its closure over a variable lasts as long as there is still a reference to that function.</p>\n<p>Closure can unexpectedly prevent the GC of a variable that you're otherwise done with, which leads to run-away memory usage over time. </p>\n<h3>Per Variable or Per Scope?</h3>\n<p>Conceptually, closure is <strong>per variable</strong> rather than <em>per scope</em>. </p>\n<p>Closure must be <em>per scope</em>, implementation wise, and then an optional optimization trims down the scope to only what was closed over (a similar outcome as <em>per variable</em> closure).</p>\n<p>In cases where a variable holds a large value (like an object or array) and that variable is present in a closure scope, if you don't need that value anymore and don't want that memory held, it's safer (memory usage) to manually discard the value rather than relying on closure optimization/GC.</p>\n<h2>An Alternative Perspective</h2>\n<p>Closure is the link-association that connects that function to the scope/variables outside of itself, no matter where that function goes.</p>\n<p>Closure instead describes the <em>magic</em> of <strong>keeping alive a function instance</strong>, along with its whole scope environment and chain, for as long as there's at least one reference to that function instance floating around in any other part of the program.</p>\n<h1>Chapter 8 - The Module Pattern</h1>\n<h2>Encapsulation and Least Exposure (POLE)</h2>\n<p>The goal of encapsulation is the bundling or co-location of information (data) and behavior (functions) that together serve a common purpose.</p>\n<p>The recent trend in modern front-end programming to organize applications around Component architecture pushes encapsulation even further.</p>\n<p>It's easier to build and maintain software when we know where things are, with clear and obvious boundaries and connection points. </p>\n<h2>What Is a Module?</h2>\n<p>A module is a collection of related data and functions (often referred to as methods in this context), characterized by a division between hidden <em>private</em> details and <em>public</em> accessible details, usually called the \"public AP</p>\n<h3>Namespaces (Stateless Grouping)</h3>\n<p><code>Utils</code> here is a useful collection of utilities, yet they're all state-independent functions. Gathering functionality together is generally good practice, but that doesn't make this a module.</p>\n<h3>Data Structures (Stateful Grouping)</h3>\n<p>Even if you bundle data and stateful functions together, if you're not limiting the visibility of any of it, then you're stopping short of the POLE aspect of encapsulation; it's not particularly helpful to label that a module.</p>\n<h3>Modules (Stateful Access Control)</h3>\n<p>classic module,\" which was originally referred to as the \"revealing module\" when it first emerged in the early 2000s.</p>\n<p>The use of an IIFE implies that our program only ever needs a single central instance of the module, commonly referred to as a \"singleton.\"</p>\n<h4>Classic Module Definition</h4>\n<p>So to clarify what makes something a classic module:</p>\n<ul>\n<li>There must be an outer scope, typically from a module factory function running at least once.</li>\n<li>The module's inner scope must have at least one piece of hidden information that represents state for the module.</li>\n<li>The module must return on its public API a reference to at least one function that has closure over the hidden module state (so that this state is actually preserved).</li>\n</ul>\n<h2>Node CommonJS Modules</h2>\n<p>In some older legacy code, you may run across references to just a bare <code>exports</code>, but for code clarity you should always fully qualify that reference with the <code>module.</code> prefix.</p>\n<p>CommonJS modules behave as singleton instances, similar to the IIFE module definition style presented before. No matter how many times you <code>require(..)</code> the same module, you just get additional references to the single shared module instance.</p>\n<h2>Modern ES Modules (ESM)</h2>\n<p>One notable difference is that ESM files are assumed to be strict-mode, without needing a <code>\"use strict\"</code> pragma at the top. There's no way to define an ESM as non-strict-mode.</p>\n<p>Even though <code>export</code> appears before the <code>function</code> keyword here, this form is still a <code>function</code> declaration that also happens to be exported. </p>\n<p>This is a so-called \"default export,\" which has different semantics from other exports. In essence, a \"default export\" is a shorthand for consumers of the module when they <code>import</code>, giving them a terser syntax when they only need this single default API member.</p>\n<p>A named import can also be <em>renamed</em> with the <code>as</code> keyword:</p>\n<p>As is likely obvious, the <code>*</code> imports everything exported to the API, default and named, and stores it all under the single namespace identifier as specified. </p>\n<h1>Apendix A - Exploring Further</h1>\n<h2>Implied Scopes</h2>\n<h3>Parameter Scope</h3>\n<p>Here, <code>studentID</code> is a considered a \"simple\" parameter, so it does behave as a member of the BLUE(2) function scope. But if we change it to be a non-simple parameter, that's no longer technically the case. Parameter forms considered non-simple include parameters with default values, rest parameters (using <code>...</code>), and destructured parameters.</p>\n<p>My advice to avoid getting bitten by these weird nuances:</p>\n<ul>\n<li>Never shadow parameters with local variables</li>\n<li>Avoid using a default parameter function that closes over any of the parameters</li>\n</ul>\n<h3>Function Name Scope</h3>\n<p>The name identifier of a function expression is in its own implied scope, nested between the outer enclosing scope and the main inner function scope.</p>\n<h2>Anonymous vs. Named Functions</h2>\n<p>As you contemplate naming your functions, consider:</p>\n<ul>\n<li>Name inference is incomplete</li>\n<li>Lexical names allow self-reference</li>\n<li>Names are useful descriptions</li>\n<li>Arrow functions have no lexical names</li>\n<li>IIFEs also need names</li>\n</ul>\n<h3>Explicit or Inferred Names?</h3>\n<p>Every function in your program has a purpose. If it doesn't have a purpose, take it out, because you're just wasting space. If it <em>does</em> have a purpose, there <em>is</em> a name for that purpose.</p>\n<p>These are referred to as <em>inferred</em> names. Inferred names are fine, but they don't really address the full concern I'm discussing.</p>\n<h3>Missing Names?</h3>\n<p>The vast majority of all <code>function</code> expressions, especially anonymous ones, are used as callback arguments; none of these get a name. So relying on name inference is incomplete, at best.</p>\n<p>Any assignment of a <code>function</code> expression that's not a <em>simple assignment</em> will also fail name inferencing</p>\n<h3>Who am I?</h3>\n<p>Leaving off the lexical name from your callback makes it harder to reliably self-reference the function. You <em>could</em> declare a variable in an enclosing scope that references the function, but this variable is <em>controlled</em> by that enclosing scope—it could be re-assigned, etc.—so it's not as reliable as the function having its own internal self-reference.</p>\n<h3>Names are Descriptors</h3>\n<p>There's just no reasonable argument to be made that <strong>omitting</strong> the name <code>keepOnlyOdds</code> from the first callback more effectively communicates to the reader the purpose of this callback. You saved 13 characters, but lost important readability information</p>\n<p> If you can't figure out a good name, you likely don't understand the function and its purpose yet. The function is perhaps poorly designed, or it does too many things, and should be re-worked. Once you have a well-designed, single-purpose function, its proper name should become evident.</p>\n<h3>Arrow Functions</h3>\n<p>Don't use them as a general replacement for regular functions. They're more concise, yes, but that brevity comes at the cost of omitting key visual delimiters that help our brains quickly parse out what we're reading. And, to the point of this discussion, they're anonymous, which makes them worse for readability from that angle as well.</p>\n<p>Briefly: arrow functions don't define a <code>this</code> identifier keyword at all. If you use a <code>this</code> inside an arrow function, it behaves exactly as any other variable reference, which is that the scope chain is consulted to find a function scope (non-arrow function) where it <em>is</em> defined, and to use that one.</p>\n<p>So, in the rare cases you need <em>lexical this</em>, use an arrow function. It's the best tool for that job. But just be aware that in doing so, you're accepting the downsides of an anonymous function.</p>\n<h3>IIFE Variations</h3>\n<p>The <code>!</code>, <code>+</code>, <code>~</code>, and several other unary operators (operators with one operand) can all be placed in front of <code>function</code> to turn it into an expression. Then the final <code>()</code> call is valid, which makes it an IIFE.</p>\n<h2>Hoisting: Functions and Variables</h2>\n<ul>\n<li>Executable code first, function declarations last</li>\n<li>Semantic placement of variable declarations</li>\n</ul>\n<h3>Function Hoisting</h3>\n<p>The reason I prefer to take advantage of <em>function hoisting</em> is that it puts the <em>executable</em> code in any scope at the top, and any further declarations (functions) below. </p>\n<p>I think <em>function hoisting</em> makes code more readable through a flowing, progressive reading order, from top to bottom.</p>\n<h3>Variable Hoisting</h3>\n<p>While that kind of inverted ordering was helpful for <em>function hoisting</em>, here I think it usually makes code harder to reason about.</p>\n<h3><code>const</code>-antly Confused</h3>\n<p>The only time I ever use <code>const</code> is when I'm assigning an already-immutable value (like <code>42</code> or <code>\"Hello, friends!\"</code>), and when it's clearly a \"constant\" in the sense of being a named placeholder for a literal value, for semantic purposes. That's what <code>const</code> is best used for. That's pretty rare in my code, though.</p>\n<h2>Are Synchronous Callbacks Still Closures?</h2>\n<p>In this context, \"calling back\" makes a lot of sense. The JS engine is resuming our suspended program by <em>calling back in</em> at a specific location. OK, so a callback is asynchronous.</p>\n<h3>Synchronous Callback?</h3>\n<p>There's nothing to <em>call back into</em> per se, because the program hasn't paused or exited. We're passing a function (reference) from one part of the program to another part of the program, and then it's immediately invoked.</p>\n<p>There's other established terms that might match what we're doing—passing in a function (reference) so that another part of the program can invoke it on our behalf. You might think of this as <em>Dependency Injection</em> (DI) or <em>Inversion of Control</em> (IoC).</p>\n<p>Notably, Martin Fowler cites IoC as the difference between a framework and a library: with a library, you call its functions; with a framework, it calls your functions.</p>\n<h3>Synchronous Closure?</h3>","frontmatter":{"title":"You Dont know JS - Scope & Closures","language":"en-US","coverPath":"you-dont-know-js-scope-closures","status":"Read","date":"2021-01-01"}}},{"node":{"html":"<h1>I - Introdução</h1>\n<p>A introdução é focada em explicar a diferença entre fazer funcionar e fazer direito.</p>\n<blockquote>\n<p>Funciona por que fazer algo funcionar - uma vez - não é tão difícil</p>\n<p>Fazer direito é outra questão. Criar software de maneira correta é difícil. Requer conhecimento e habilidades que a maioria dos jovens programadores ainda não adquiriu. Requer um grau de raciocínio e insight que a maioria dos programadores não se empenha em desenvolver. Requer um nível de disciplina e dedicação que a maioria dos programadores nunca sonhou que precisaria. Principalmente, requer paixão pela programação e o desejo de se tornar um profissional.</p>\n</blockquote>\n<p>Com essas definições de fazer funcionar e fazer direito, seria possível, por exemplo, diferenciar engenheiros de software e programadores/desenvolvedores.</p>\n<p>Ele termina a introdução explicando qual seria o impacto de criar software da maneira certo.</p>\n<blockquote>\n<p>Quando o software é feito da maneira certa, ele exige só uma fração dos recursos humanos para ser criado e mantido. As mudanças são simples e rápidas. Os poucos defeitos surgem distantes uns dos outros. O esforço é minimizado enquanto a funcionalidade e a flexibilidade são maximizados.</p>\n</blockquote>\n<h2>1 - O que são design e arquitetura?</h2>\n<p>Nesse primeiro capitulo ele reforça a ideia apresentada no prefácio:</p>\n<blockquote>\n<p>O objetivo da arquitetura de software é minimizar os recursos humanos necessários para construir e manter um determinado sistema.</p>\n</blockquote>\n<p>Para o autor não existe diferença entre design e arquitetura. Ele pontua que no geral o termo arquitetura é usado em um contexto de mais alto nível enquanto o termo design é aplicado em um contexto de mais baixo nível. Porém, o objetivo de ambos é o mesmo (citação acima).</p>\n<blockquote>\n<p>Há simplesmente uma linha constante de decisões que se estende dos níveis mais altos para os mais baixos.</p>\n</blockquote>\n<p>Em seguida o autor demonstra com um estudo de caso um cenário onde uma empresa não se preocupou com a qualidade do design e a produtividade por release teve uma queda drástica, mesmo com um acréscimo de engenheiros de software, o que diga-se de passagem aumentou muito o valor do produto.</p>\n<p>Com a simples reflexão da lebre e da tartaruga o autor explica quais foram os erros dessa empresa. Comparando os desenvolvedores atuais com a lebre, que tem um excesso de confiança na sua capacidade de produção e que acreditam que podem assim que liberarem o produto ao mercado, voltar e limpar a bagunça.</p>\n<p>Por fim, conclui o capitulo explicando que </p>\n<blockquote>\n<p>A única forma de seguir rápido é seguir bem.</p>\n</blockquote>\n<p>p.s. Ver exemplo de Jason Gorman sobre Kata de numerais romanos com TDD e sem TDD.</p>\n<h2>2 - Um conto de dois valores</h2>\n<p>O autor inicia explicando que existem dois tipos de valores em um sistema de software:</p>\n<ul>\n<li>Comportamento: Implementação de uma especificação funcional.</li>\n<li>Arquitetura: Software. <em>soft</em> (suave) e <em>ware</em> (produto). Um produto que deve ser fácil de mudar.</li>\n</ul>\n<p>Segundo o autor, entre os dois valores em um sistema de software, a arquitetura é o valor maior. Ou seja, é mais importante um sistema que é flexível para mudanças do que seja um sistema que funciona com os comportamentos esperados.</p>\n<blockquote>\n<p> [...] os gerentes de negócios não estão equipados para avaliar a importância da arquitetura. <em>É para isso que são contratados desenvolvedores de software</em>. Portanto, é responsabilidade da equipe de desenvolvimento de software garantir a importância da arquitetura sobre a urgência dos recursos.</p>\n</blockquote>\n<p>O autor conclui, que times de desenvolvimento de software são stakeholders assim como os demais stakeholders. Portanto, é seu dever expor a importância da arquitetura e garantir que ela seja respeitada.</p>\n<h1>II - Começando com os tijolos: paradigmas da programação</h1>\n<h2>3 - Panorama do paradigma</h2>\n<p>Nesse capítulo o autor introduz os três tipos de paradigmas de programação existentes: estruturado, orientado a objetos e funcional.</p>\n<p>Como reflexão o autor sugere que cada um desses paradigmas impõe restrições na maneira de desenvolver o software. E que possivelmente esses três serão os três únicos paradigmas que existem, pois eles já restringem todas possibilidades do desenvolvimento de software.</p>\n<p>Os próximos capítulos explicam dessa seção explicam mais a fundo as restrições dos paradigmas.</p>\n<h2>4 - Programação estruturada</h2>\n<blockquote>\n<p>A programação era difícil e os programadores não eram muito competentes.</p>\n</blockquote>\n<p>Dado que o cérebro humano era mais limitado que o computador e que os ciclos de interação de feedback para os desenvolvedores eram muito grandes. Dijkstra começou a trabalhar com a ideia de provar matematicamente os programas.</p>\n<p>Enquanto tentava aplicar o uso da prova matemática, ele se deparou com a complexidade das declarações <em>goto</em> e como elas impediam a abordagem dividir para conquistar.</p>\n<p>Baseado então no <a href=\"https://en.wikipedia.org/wiki/Structured_program_theorem\">Structured program theorem</a> de Bohm e Jacopini que define que três tipos de controle são necessários para processar um função computável. Dijkstra enviou uma carta aberta <a href=\"https://web.archive.org/web/20070703050443/http://www.acm.org/classics/oct95/\">Go To statements considered harmful</a> que é possivelmente o começo do paradigma estruturado.</p>\n<p>Ou seja, no paradigma estruturado os saltos (declarações <em>goto</em>) foram substituídos por declarações <code>if/then/else</code> e <code>do/while/until</code>. </p>\n<p>O autor então introduz o conceito de decomposição funcional, que é possível com o paradigma estruturado.</p>\n<blockquote>\n<p>Você pode pegar uma declaração de um problema de larga escala e decompô-la em funções de alto nível. Cada uma dessas funções pode então ser decomposta em funções de níveis mais baixos até o infinito. Além do mais, cada função decomposta pode ser representada por meio de estruturas de controle restritas da programação estruturada.</p>\n</blockquote>\n<p>Embora o paradigma estruturado se consolidasse as provas formais propostas por Dijkstra não se consolidaram.</p>\n<p>Por fim, o autor diferencia os modelos de comprovação da matemática e da ciência.</p>\n<blockquote>\n<p>[...] a matemática é a disciplina que prova as declarações como verdadeiras. Por outro lado, a ciência é a disciplina que prova as declarações como falsas.</p>\n</blockquote>\n<p>E conclui que com testes podemos comprovar a presença de erros e não a ausência de erros. Assim como a ciência consegue provar que uma declaração é falsa mas não consegue provar que é verdadeira. A programação estruturada, e portanto a programação que restringe uso de declarações <em>goto</em>, permite comprovar a presença de erros e por isso é tão relevante.</p>\n<h2>5 - Programação orientada a objetos</h2>\n<blockquote>\n<p>A base de uma boa arquitetura é a compreensão e aplicação dos princípios do design orientado a objeto.</p>\n</blockquote>\n<p>O autor começa o capítulo sugerindo que a programação orientada a objetos é a base para uma boa arquitetura. Porém, é muito difícil definir o paradigma de orientação a objetos. O autor testa se encapsulamento, herança e polimorfismo que são normalmente sugeridos como as características do paradigma OO realmente fazem definem o paradigma.</p>\n<blockquote>\n<p>O encapsulamento é citado como parte da definição da OO porque linguagens OO possibilitam um encapsulamento fácil e eficaz de dados e funções.</p>\n</blockquote>\n<p>O autor desmente a ideia de que encapsulamento deveria ser usado para definir o paradigma de OO. Dado que na linguagem de programação C, que não é OO, existia um encapsulamento perfeito com arquivos header. Esse encapsulamento foi enfraquecido em linguagens OO como C++ (onde precisa declarar os atributos no arquivo header - embora possam ser declarados como privados) e Java (onde não existe arquivo header).</p>\n<p>O autor então questiona.  A herança pode ser usado para definir OO? E ele mesmo responde:</p>\n<blockquote>\n<p>A herança é simplesmente a redeclaração de um grupo de variáveis e funções dentro de um escopo fechado.</p>\n</blockquote>\n<p>Novamente, isso já era possível em C. Quando uma estrutura é um super conjunto de outra, pode-se usar um <em>cast</em> para o sub conjunto. E enquanto ao polimorfismo:</p>\n<blockquote>\n<p>Podemos dizer, portanto, que o polimorfismo é uma aplicação de ponteiros em funções.</p>\n</blockquote>\n<p>Entretanto, em linguagens como C é necessário um conjunto de convenções para que a aplicação de ponteiros em funções funcione corretamente. O paradigma OO elimina essas convenções e facilita o uso de polimorfismo, por isso o autor sugere:</p>\n<blockquote>\n<p>OO impõe disciplina na transferência indireta de controle. </p>\n</blockquote>\n<p>e continua</p>\n<blockquote>\n<p>A OO permite a arquitetura de plug-in seja usada em qualquer lugar e para qualquer coisa.</p>\n</blockquote>\n<p>Então o autor acrescenta que OO permite o uso de inversão de dependência e que isso gera uma flexibilidade para os arquitetos de solução. Inversão de dependência também proporciona o uso de <em>implementação independente</em>, ou seja, alterações na implementação de um componente não impactam outros. E <em>desenvolvimento independente</em>, onde equipes diferentes podem trabalhar independentemente.</p>\n<h2>6 - Programação funcional</h2>\n<blockquote>\n<p>Esse paradigma se baseia essencialmente no calculo-lambda, inventado por Alonzo Church na década de 1930.</p>\n</blockquote>\n<p>O autor inicia o capítulo demonstrando como calcular o quadrado de um numero em Closure e Java. E então pontua a diferença que no exemplo em Java existe uma variável mutável enquanto no exemplo de Closure não existem variáveis mutáveis.</p>\n<blockquote>\n<p>Variáveis em linguagens funcionais <em>não variam</em>.</p>\n</blockquote>\n<p>O autor então discorre sobre a importância da imutabilidade na arquitetura. Em uma arquitetura imutável não existirão problemas de concorrência como <em>race conditions</em> ou <em>deadlock conditions</em>. </p>\n<p>Entretanto, para alcançar imutabilidade total seria preciso de recursos infinitos. Como isso, atualmente, é impraticável é necessário aplicar segregações de mutabilidade.</p>\n<blockquote>\n<p>Os componentes imutáveis realizam suas tarefas de maneira puramente funcional, sem usar nenhuma variável mutável. Os componentes mutáveis se comunicam com um ou mais dos outros componentes que não são puramente funcionais e permitem que o estado das variáveis seja modificado.</p>\n</blockquote>\n<p>Nesses componentes mutáveis é importante usar algum mecanismo, por exemplo memória transacional, para evitar os problemas de concorrência.</p>\n<blockquote>\n<p>[...] Aplicações bem estruturadas devem ser segregadas entre componentes que mudam e que não mudam variáveis.</p>\n</blockquote>\n<p>Por fim, o autor fala sobre <em>event sourcing</em>. Dado que os recursos computacionais estão aumentando, cada vez menos precisaremos de estados mutáveis. E portanto estratégias como <em>event sourcing</em> se tornam bastante atraentes.</p>\n<blockquote>\n<p><em>Event sourcing</em> é uma estratégia em que armazenamos as transações, mas não o estado. Quando o estado for solicitado, simplesmente aplicamos todas as transações desde o inicio.</p>\n</blockquote>\n<p>Nesse caso as APIs oferecem somente um Create and Read, ao invés de todo o CRUD. E a vantagem é ter uma aplicação que não modifica nem elimina dados, uma aplicação inteiramente imutável.</p>\n<h1>III - Princípios de design</h1>\n<blockquote>\n<p>Bons sistemas de software começam com um código limpo. Por um lado, se os tijolos não são bem feitos, a arquitetura da construção perde a importância. Por outro lado, você pode fazer uma bagunça considerável com tijolos bem-feitos.</p>\n</blockquote>\n<p>Os princípios SOLID são aplicados para que obtenhamos software que tolere mudanças, sejam fáceis de entender e sejam reutilizáveis. O intuito é a aplicação desses princípios em níveis de componentes.</p>\n<p>A história dos princípios começa na década de 80, embora os princípios como conhecemos hoje foram estabilizados nos anos 2000.</p>\n<p>Os princípios são:</p>\n<ul>\n<li>Single Responsibility Principle (SRP): cada módulo de software tenha uma, e apenas uma, razão para mudar.</li>\n<li>Open-Closed Principle (OCP): Permitirem que o comportamento do sistema mude pela adição de novo código, e não pela alteração do código existente.</li>\n<li>Liskov Substitution Principle (LSP): Os módulos devem aderir um contrato que assim permite que esses módulos sejam substituídos por outros que aderiram o mesmo contrato.</li>\n<li>Interface Segregation Principle (ISP): Módulos devem evitar depender de coisas que não precisam.</li>\n<li>Dependency Inversion Principle (DIP): Um código de nível mais alto não deve depender de detalhes de implementação de códigos de nível mais baixo.</li>\n</ul>\n<h2>7 - SRP: O Princípio da responsabilidade única</h2>\n<p>No começo do capítulo o autor explica que esse principio é possivelmente o que as pessoas mais se confundem dado o nome errôneo.</p>\n<blockquote>\n<p>Um modulo deve ser responsável por um, e apenas um, ator.</p>\n</blockquote>\n<p>Onde modulo pode ser representado como um arquivo fonte, por exemplo uma classe ou um conjunto de funções e dados. E o ator representa um conjunto de stakeholders que exigem que o sistema mude da mesma forma.</p>\n<p>O autor então demonstra dois sintomas que exemplificam a violação. O sintoma de duplicação acidental e o sintoma de fusão.</p>\n<p>No sintoma de duplicação acidental a classe <code>Employee</code> tem três métodos <code>calculatePay</code>, <code>reportHours</code> e <code>save</code> onde cada um reflete as necessidades de atores diferentes, respectivamente, CFO, COO e CTO. Nesse exemplo, os métodos podem utilizar um algoritmo compartilhado, digamos <code>regularHours</code>, que ao ser alterado pelos interesses do CFO também impactarão os demais atores que também utilizam o método nas seus respectivos métodos.</p>\n<p>No sintoma de fusão, cada ator requer mudanças no mesmo arquivo <code>Employee</code>. Desenvolvedores de times diferentes trabalham nessas mudanças e quando forem sincronizar o código terão problemas de fusão (conflitos no merge). A resolução dos conflitos adiciona risco para todos atores, mesmo algum ator que não pediu mudanças poderia ser afetado.</p>\n<p>Ambos sintomas podem ser solucionados <em>separando o código que dá suporte a atores diferentes</em>.</p>\n<p>O autor sugere duas soluções para o problema, a primeira separando a lógica de negocio e os dados, e a segunda agregando a lógica de negócio com os dados mas delegando a execução para outras classes.</p>\n<p>A primeira solução, uma <em>Facade</em> é criada <code>EmployeeFacade</code> que expõe os métodos <code>calculatePay</code>, <code>reportHours</code> e <code>save</code>. A <code>EmployeeFacade</code> é responsável por criar e delegar cada uma das classes reesposáveis por cada método <code>PayCalculator</code>, <code>HourReporter</code> e <code>EmployeeSaver</code>. Essas classes tem acesso ao <code>EmployeeData</code> que é uma estrutura de dados.</p>\n<p>Na segunda solução, a classe <code>Employee</code> ainda expõem os métodos <code>calculatePay</code>, <code>reportHours</code> e <code>save</code>, porem, a execução desses métodos é delegada para <code>PayCalculator</code>, <code>HourReporter</code> e <code>EmployeeSaver</code>.</p>\n<h2>8 - OCP: O Princípio do aberto/fechado</h2>\n<blockquote>\n<p>Um artefato de software deve ser aberto para extensão, mas fechado para modificação.</p>\n</blockquote>\n<p>Adicionar comportamentos não devem exigir mudanças massivas no software. O autor sugere que uma boa arquitetura de software deve alterar o menos possível o código para alterações, idealmente nenhuma mudança. Em seguida propõe um exercício mental.</p>\n<p>Nesse exercício, é necessário adicionar a um sistema que exibe um resumo financeiro em uma página web uma nova funcionalidade de exibir o resumo em um formato para impressão em PDF. Para completar a tarefa são necessários dois passos. Primeiro a aplicação do SRP e em seguida do DIP.</p>\n<p>Na aplicação do primeiro princípio as responsabilidades são separadas em duas. A primeira é a geração dos financeiros para o relatório e a segunda é a a apresentação desses dados.</p>\n<p>Assim que a separação é concluída, é necessário organizar as dependências para garantir que as mudanças de responsabilidade de uma classe não causem mudanças nas outras. Nesse exemplo, são adicionados classes que são partes de componentes. Os componentes se comunicam somente através de interfaces, ou seja, é feita a aplicação do DIP.</p>\n<blockquote>\n<p>[...] o código fonte da classe A menciona a classe B, mas a classe B não menciona nada sobre a classe A.</p>\n</blockquote>\n<p>Ou seja, as implementações conhecem suas interfaces, mas as interfaces não conhecem as implementações. Assim teremos componentes com relações unidirecionais, em que as flechas apontam para quem eles estão protegendo das mudanças. Por exemplo, a interface de relatório financeiro é protegida de mudanças no controlador de registro financeiro e na base de dados financeiras.</p>\n<blockquote>\n<p>Os arquitetos separam a funcionalidade com base no como, por que e quando da mudança e, em seguida, organizam essa funcionalidade separada em uma hierarquia de componentes. Os componentes de nível mais alto na hierarquia são protegidos das mudanças feitas em componentes nos níveis mais baixos.</p>\n</blockquote>\n<p>Por fim o autor explica que as interfaces também podem ser utilizadas para ocultar informações e evitar dependências transitivas.</p>\n<blockquote>\n<p>As dependências transitivas violam o princípio geral de que as entidades de software não dependem de coisas que não usam diretamente.</p>\n</blockquote>\n<h2>9 - LSP: O Princípio de substituição de liskov</h2>\n<p>O LSP foi definido em um texto de Barbara Liskov.</p>\n<blockquote>\n<p>O que queremos aqui é algo como a seguinte propriedade de substituição: se, para cada objeto <em>o1</em> de tipo <em>S</em>, houver um objeto <em>o2</em> de tipo <em>T</em>, de modo que, para todos os programas <em>P</em> definidos em termos <em>T</em>, o comportamento de <em>P</em> não seja modificado quando <em>o1</em> for substituído por <em>o2</em>, então <em>S</em> é um subtipo de <em>T</em>.</p>\n</blockquote>\n<p>O autor usa o exemplo de cobrança das taxas de uma licença. A aplicação <code>Billing</code> usa somente a interface <code>Licence</code> que define o método <code>calcFee</code>. As classes <code>PersonalLicense</code> e <code>BusinessLicense</code> implementam <code>License</code>. E portanto o comportamento da aplicação <code>Billing</code> não depende da utilização de qualquer subtipo.</p>\n<blockquote>\n<p>Ambos os subtipos são substituíveis pelo tipo <code>License</code>.</p>\n</blockquote>\n<p>Em seguida, o autor demonstra uma violação com o exemplo do quadrado e retângulo. Nesse exemplo, <code>Square</code> é um subtipo de <code>Rectangle</code>, esse que define os métodos <code>setHeight</code> e <code>setWeight</code>. O subtipo de <code>Rectangle</code>, <code>Square</code>, define o método <code>setSide</code>. A violação acontece pois quando a classe <code>User</code> utiliza os métodos de <code>Rectangle</code> a classe <code>User</code> não invoca o método <code>setSide</code> de <code>Square</code>. Ou seja, o código de <code>User</code> precisa de pelo menos um comando <code>if/else</code> para determinar quando for um <code>Rectangle</code> e usar sua interface e quando for um <code>Square</code> usar sua interface.</p>\n<p>Além disso, a interface da <code>Rectangle</code> classe permite determinar valores diferentes para largura e altura. Enquanto, um <code>Square</code> deveria ter todos os lados com o mesmo valor. Isso torna o código mais confuso de ler.</p>\n<pre><code>// Funciona\nRectangle r = new Rectangle();\nr.setWeight(5);\nr.setHeight(2);\nassert(r.area() == 10)\n\n// Não funciona - o método setSide não foi chamado e o calculo da área estará errado.\nRectangle r = new Square();\nr.setWeight(5);\nr.setHeight(2);\nassert(r.area() == 10)\n</code></pre>\n<p>Para evitar o comportamento errado, a classe <code>User</code> deveria utilizar comandos <code>if/else</code> para separar a lógica. </p>\n<blockquote>\n<p>Já que o comportamento do <em>User</em> depende dos tipos utilizados, esses tipos não são substituíveis.</p>\n</blockquote>\n<p>Após exemplificar em um nível de design de código, o autor explica que esse princípio se tornou mais genérico e hoje é aplicado em várias camadas do software. Desde o código, o primeiro exemplo, até a arquitetura, serviços REST que usam a mesma interface para se comunicar. Em seguida o autor apresenta um exemplo no nível de arquitetura.</p>\n<p>Nesse exemplo, uma companhia de táxi não leu corretamente a especificação e criou erroneamente um dos endpoints dos recursos REST. As demais aplicações que consomem essa especificação precisarão adaptar seu código, possivelmente adicionando clausulas <code>if/else</code> para lidar com esse serviço que implementou incorretamente.</p>\n<h2>10 -  ISP: O Princípio da segregação de interface</h2>\n<p>Em uma linguagem de programação como Java em que é necessário importar as classes que deseja consumir explicitamente. Em um cenário em que uma classe <code>OPS</code> define as operações <code>op1</code>, <code>op2</code> e <code>op3</code> e essas operações são consumidas respectivamente pelas classes <code>User1</code>, <code>User2</code> e <code>User3</code>. Qualquer alteração na em qualquer uma das operações de <code>OPS</code> vai impactar(recompilação) as 3 classes consumidoras.</p>\n<p>Esse problema pode ser resolvido com a segregação de interfaces.</p>\n<p>O mesmo exemplo aplicando segregação de interfaces, <code>OPS</code> implementa <code>U1OPS</code>, <code>U2OPS</code> e <code>U3OPS</code> que são interfaces que definem respectivamente <code>op1</code>, <code>op2</code> e <code>op3</code>. Agora as classes <code>User1</code>, <code>User2</code> e <code>User3</code> poderão depender exclusivamente das operações que elas usam.</p>\n<blockquote>\n<p>Linguagens estaticamente tipadas como Java forçam os programadores a criarem declarações que os usuários devem <code>import</code>, <code>use</code> ou <code>include</code>. São essas declarações <code>included</code> no código-fonte que criam as dependências de código-fonte que forçam a recompilação e a reimplementação.</p>\n</blockquote>\n<p>Em seguida o autor explica que em linguagens de programação como Ruby ou Python, essas declarações não existem no código fonte. Pois são inferidas em tempo de execução. E portanto, essas linguagens são mais flexíveis e menos fortemente acopladas.</p>\n<p>Abstraindo o conceito de ISP e aplicando em um nível de arquitetura, o autor propõe:</p>\n<blockquote>\n<p>Em geral, é prejudicial depender de módulos que  contenham mais elementos do que você precisa.</p>\n</blockquote>\n<h2>11 - DIP: O Princípio da inversão de dependência</h2>\n<blockquote>\n<p>[...] os sistemas mais flexíveis são aqueles em que as dependências de código-fonte se referem apenas as abstrações e não a itens concretos.</p>\n</blockquote>\n<p>Portanto, tanto as linguagens estaticamente tipadas como Java, que usam <code>import</code>, <code>use</code> e <code>include</code>, quanto as linguagens dinamicamente tipadas como Ruby e Python <strong>não</strong> deveriam se referir a módulos concretos. A única diferença entre essa afirmação é que nas linguagens dinamicamente tipadas é mais difícil definir o que é um módulo concreto, então o autor propõe:</p>\n<blockquote>\n<p>Nesse contexto especifico, podemos conceituá-lo como qualquer módulo em que as funções chamadas são implementadas.</p>\n</blockquote>\n<p>Em seguida, o autor explica que essa ideia é impossível de ser aplicada a risca. Pois dependemos de muitas classes de facilidade da linguagem, em Java por exemplo da classe <code>String</code>. Então, podemos depender das abstrações ou de classes que são consideradas muito estáveis (mudanças são raras e estritamente controladas).</p>\n<blockquote>\n<p>Por essas razões, tendemos a ignorar a estabilidade de segundo plano de sistema operacionais e facilidades de plataformas quando se trata do DIP. Toleramos essas dependências concretas porque sabemos e confiamos que elas não mudarão.</p>\n</blockquote>\n<p>Após essa distinção entre abstrações e módulos concretos e módulos instáveis e estáveis. E mostra o porque é importante ter abstrações estáveis.</p>\n<blockquote>\n<p>Em uma interface abstrata, toda mudança corresponde a uma mudança em suas implementações concretas. Por outro lado, as mudanças nas implementações concretas normalmente ou nem sempre requerem mudanças nas interfaces que implementam.</p>\n</blockquote>\n<p>Portanto, precisamos alcançar a estabilidade nas abstrações. Bons designers e arquitetos trabalham duro para alcançar isso. O autor propõe algumas regras para alcançar abstrações estáveis:</p>\n<ul>\n<li><strong>Não se refira a classes concretas voláteis:</strong> Refira-se a interfaces abstratas. Essa regra também estabelece restrições severas sobre a criação dos objetos e geralmente força o uso de <em>Abstract Factories</em>.</li>\n<li><strong>Não derive de classes concretas voláteis:</strong> A herança é o relacionamento de código-fonte mais forte e rígido de todos, e consequentemente, deve ser usado com muito cuidado.</li>\n<li><strong>Não sobrescreva funções concretas:</strong> Funções concretas muitas vezes exigem dependências de código fonte. Quando fazemos o <code>override</code> acabamos <em>herdando-as</em>. Como substituição, converta a função em abstrata e crie múltiplas implementações.</li>\n<li><strong>Nunca mencione o nome de algo que seja concreto e volátil:</strong> Reafirmação do próprio principio.</li>\n</ul>\n<p>Em seguida, o autor explica que a criação de objetos cria uma dependência de código-fonte, e que portanto, merece um tratamento especial. Uma solução é o uso das <em>Abstract Factories</em>. Ao aplicarmos o padrão de design, obtemos um limite arquitetural que separa o concreto do abstrato.</p>\n<p>O autor ainda menciona, que normalmente, as aplicações tem um método <code>main</code> onde as classes concretas são instanciadas e adicionadas a uma variável global do tipo da abstração. As classes consumidoras podem então consumir somente a abstração que a implementação é injetada.</p>\n<p>No fim do capítulo, o autor menciona que o DIP é um princípio extremamente citado durante o livro, pois é responsável por desenhar o limite arquitetural nas arquiteturas.</p>\n<h1>IV - Princípios dos componentes</h1>\n<h2>12 - Componentes</h2>\n<p>Esse capítulo começa com o autor falando sobre a definição de componentes.</p>\n<blockquote>\n<p>Componentes são unidades de implantação (deploy). Eles são as menores entidades que podem ser implantadas como parte de um sistema.</p>\n</blockquote>\n<p>Em seguida, ele fala sobre as vantagens de termos componentes bem projetados.</p>\n<blockquote>\n<p>[...] quando efetivamente implantados, componentes bem projetados sempre retêm a capacidade de serem implantados de forma independente e, portanto, desenvolvidos independentemente.</p>\n</blockquote>\n<p>Após a definição e vantagens o autor reconta a historia por trás do surgimento dos componentes.</p>\n<p>Ela começa com os desenvolvedores tendo que indicar no programa, o endereço de memória que seria alocado. Além disso, eles adicionavam ao código fonte da sua aplicação funções de biblioteca. Com essa abordagem, a compilação poderia levar horas.</p>\n<p>Para melhorar o tempo de compilação, a aplicação e bibliotecas de funções foram separadas e eram alocadas em diferentes áreas da memória. O aumento do tamanho das aplicações trouxe novos problemas. O que levou a arquivos binários relocáveis. Esses arquivos eram ligados através de um carregador de ligações.</p>\n<blockquote>\n<p>O carregador de ligações permitiu que os programadores dividissem seus programas em segmentos separados compiláveis e carregáveis.</p>\n</blockquote>\n<p>O autor então conclui que depois de 50 anos, a arquitetura de plugin de componente é o padrão.</p>\n<h2>13 - Coesão de componentes</h2>\n<p>Esse capítulo fala sobre os três princípios da coesão de componentes:</p>\n<ul>\n<li>Reuse/Release Equivalence Principle (REP): A granularidade do reuso é a granularidade do release.</li>\n<li>Common Closure Principle (CCP): Reúna tudo que muda ao mesmo tempo pelas mesmas razões. Separe tudo que muda em tempos diferentes por razões diferentes.</li>\n<li>Common Reuse Principle (CRP): Não dependa de coisas que você não precisa.</li>\n</ul>\n<h3>CRP: Reuse/Release Equivalence Principle</h3>\n<p>Então o autor começa aprofundando em cada um dos princípios. Ao se referir ao REP ele propõe:</p>\n<blockquote>\n<p>As pessoas que querem reutilizar componentes de software não podem, e não devem, fazê-lo a não ser que esses componentes sejam rastreados por meio de um processo de release e recebam números de release.</p>\n</blockquote>\n<p>Isso é importante, por que os desenvolvedores que usam o componente precisam saber quando vão chegar novos release do componente e quais mudanças o release trará. Assim o desenvolvedor poderá escolher se vai ou não atualizar aquele componente.</p>\n<p>Além disso, esse princípio significa que classes e módulos de um mesmo componente devem ser parte de um grupo coeso. Ou seja, deve haver um tema ou propósito abrangente que todos as classes e módulos compartilham.</p>\n<blockquote>\n<p>As classes e módulos agrupados em um componente devem ser passíveis de release em conjunto.</p>\n</blockquote>\n<p>Ou seja, as classes e módulos sendo alterados na release e sua documentação deve, fazer sentido tanto para o autor quanto para os usuários do componente.</p>\n<h3>CCP: Common Closure Principle</h3>\n<p>Em seguida o autor fala sobre CCP:</p>\n<blockquote>\n<p>Reúna tudo que muda ao mesmo tempo pelas mesmas razões. Separe tudo que muda em tempos diferentes por razões diferentes.</p>\n</blockquote>\n<p>É o Single responsibility principle (SRP) reescrito para componentes.</p>\n<blockquote>\n<p>Quando o código de uma aplicação tem que mudar, é preferível que todas as mudanças ocorram em um componente em vez de servem distribuídas por vários componentes.</p>\n</blockquote>\n<p>Isso limita a quantidade de implantações necessárias.</p>\n<p>O agrupamento das classes que mudam pelos mesmos motivos também reduz o trabalho relacionado a fazer release, revalidar e reimplantar o software.</p>\n<blockquote>\n<p>O CCP amplia essa lição (se referindo ao OCP) ao reunir no mesmo componente as classes fechadas para os mesmos tipos de mudanças. Assim, quando ocorre uma mudança nos requisitos, essa mudança te uma boa chance de se limitar a um número mínimo de componentes.</p>\n</blockquote>\n<h3>CRP: Common Reuse Principle</h3>\n<p>O terceiro e ultimo principio que o autor comenta é o CRP:</p>\n<blockquote>\n<p>Não force usuários de um componente a depender de coisas que eles não precisam.</p>\n</blockquote>\n<p>O CRP sugere que devemos manter no mesmo componentes classes e módulos que deverão ser reutilizados juntos. </p>\n<blockquote>\n<p>As classes raramente são reutilizadas isoladamente. É mais comum que as classes reutilizáveis colaborem com outras classes que fazem parte da abstração reutilizável.</p>\n</blockquote>\n<p>Ou seja, dado que essas classes são reutilizadas juntas são fortemente acopladas e devem ser mantidas juntas no mesmo componente.</p>\n<p>O CRP também sugere para somente depender de componentes se todas as classes desse componente são utilizadas. Caso contrário, qualquer alteração em uma classe que não é relevante para nós poderá nos impactar.</p>\n<blockquote>\n<p>Segundo o CRP, as classes que não têm uma forte ligação entre si não devem ficar no mesmo componente.</p>\n</blockquote>\n<p>Esses três princípios lutam entre si, enquanto REP e CCP tendem a aumentar os componentes, o CRP tende a diminui-lo.</p>\n<blockquote>\n<p>Um bom arquiteto deve descobrir uma posição nesse triângulo de tensão que satisfaça as demandas <em>atuais</em> da equipe de desenvolvimento.</p>\n</blockquote>\n<h2>14 - Acoplamento</h2>\n<h3>ADP: Acyclic Dependencies Principle</h3>\n<p>O autor começa o capítulo falando sobre a síndrome da manhã seguinte (Morning after syndrome). Em cenários onde muitos desenvolvedores alteram o mesmo arquivo-fonte, é comum esse problema ocorrer, você chega na manhã seguinte e seu código parou de funcionar pois alguma dependência foi alterada.</p>\n<p>Uma das tentativas de solucionar esse problema é usar build semanais. Onde os desenvolvedores trabalham sem sincronizar código durante quatro dias da semana, e no ultimo dia fazem a sincronização. Essa tentativa lembra um modelo cascata, e simplesmente aumenta e adia as dores para o dia da integração. Com o crescimento do projeto é preciso aumentar a quantidade de dias de integração e assim a produtividade da equipe começa a cair. Por causa disso, o feedback rápido se perde.</p>\n<blockquote>\n<p>A solução desse problema é particionar o ambiente de desenvolvimento em componentes passíveis de release. Os componentes se tornam unidades de trabalho que podem ser da responsabilidade de um único desenvolvedor ou de uma equipe de desenvolvedores.</p>\n</blockquote>\n<p>Quando um componente se torna funcional, uma release com uma versão deve ser feita. Os demais desenvolvedores usam a aquela versão daquele componente. Os desenvolvedores do componente podem continuar trabalhando no seu componente sem impactar os demais. Ao lançarem novas versões, os consumidores do componente devem analisar se querem aderir a nova versão ou não.</p>\n<blockquote>\n<p>As mudanças feitas em um componente não precisam ter efeito imediato sobre as outras equipes. Cada equipe pode decidir por conta própria quando adaptará seus componentes aos novos releases dos componentes.</p>\n</blockquote>\n<p>Outra vantagem é que a integração acontece em pequenos incrementos. Ainda que esse processo seja simples racional e amplamente usado ele precisa deve gerenciar a estrutura de dependência dos componentes para que funcione. <em>Não pode haver ciclos.</em></p>\n<p>Para tal é preciso que os componentes sigam uma estrutura de grafo acíclico direcionado (DAG - Directed Acyclic Graph).</p>\n<blockquote>\n<p>Começando em qualquer dos componentes, é impossível seguir as relações de dependência e voltar para o componente inicial.</p>\n</blockquote>\n<p>Dessa maneira é bastante simples descobrir quais componentes são dependentes dos outros. Basta seguir as flechas de dependência de trás para frente. Outra vantagem, é que no momento da release do sistema inteiro, o processo ocorre debaixo para cima. Primeiro os componentes que não tem dependências de outros e depois dos componentes que dependem daqueles componentes, e assim por diante.</p>\n<p>No exemplo do livro, a primeira classe seria <code>Entities</code>, depois <code>Interactors</code>, <code>Database</code>, <code>Presenters</code>, <code>Controllers</code>, <code>Authorizer</code>, <code>View</code> e <code>Main</code>.</p>\n<p>A violação desse princípio é quando temos um ciclo de dependências. Ao fazer alterações em um componente, esse componente depende do próximo que depende do primeiro. E assim, é difícil ou impossível estabelecer uma ordem lógica de mudanças. E o problema da síndrome do dia seguinte volta a acontecer. </p>\n<p>Outro problema com a violação desse princípio é que ele aumenta a dificuldade de preparar os testes, pois agora, é preciso preparar as dependências do seu componente e as dependências do componentes dele.</p>\n<p>Existem duas soluções para a violação.</p>\n<ol>\n<li>Aplicar DIP: Primeiro, cria-se uma interface com os métodos que a classe precisar no <em>componente que usa</em> o outro componente. Depois, no <em>componente usado</em>, cria-se uma classe que herda/implementa a classe do <em>componente que usa</em>.</li>\n<li>Criar um novo componente: Mova a(s) classe(s) das quais ambos dependem para esse novo componente.</li>\n</ol>\n<blockquote>\n<p>À medida que a aplicação cresce, a estrutura de dependência dos componentes varia e cresce. Assim, a estrutura de dependência deve sempre ser monitorada com relação a ciclos.</p>\n</blockquote>\n<p>É importante quando os ciclos aparecerem quebrar eles. E isso pode significar o acréscimo de componentes, no segundo caso.</p>\n<p>Isso leva a conclusão que a arquitetura deverá ter seu design feito de cima para baixo (top-down design). O que pode ser contra intuitivo, pois diagramas de dependência de componentes devem de alguma forma representar funções do sistema. O que não é verdade.</p>\n<blockquote>\n<p>Os diagramas de dependências de componentes têm pouco a ver com descrever a função da aplicação. Na verdade, eles são um mapa para a facilidade de se fazer o build e a manutenção da aplicação.</p>\n</blockquote>\n<p>A estrutura de dependências deve isolar volatilidade, ou seja, componentes que são estáveis devem ser protegidos dos que são voláteis.</p>\n<h3>SDP: Stable Dependency Principle</h3>\n<p>É esperado alguma volatilidade nos componentes.</p>\n<blockquote>\n<p>Um componente que seja difícil de mudar não deve ser dependente de qualquer componente que esperamos que seja volátil. Caso contrário, o componente volátil também será difícil de mudar.</p>\n</blockquote>\n<p>Assim evita-se que um componente se torne difícil de mudar por cause de uma dependência.</p>\n<blockquote>\n<p>Estável é a propriedade de algo que \"não é facilmente movido\". A estabilidade está relacionada com a quantidade de trabalho necessária para fazer uma mudança.</p>\n</blockquote>\n<p>Moeda em pé é facilmente derrubada, portanto, é instável. Uma mesa, precisa de muito trabalho para ser movida, portanto, é estável.</p>\n<blockquote>\n<p>Uma maneira segura de tornar um componente de software difícil de mudar é fazer com que vários componentes de software dependam dele. Um componente do qual muitos dependam é muito estável por que requer muito trabalho par reconciliar eventuais mudanças com todos os componentes dependentes.</p>\n</blockquote>\n<p>Três componentes dependem do componente X. Para cada componente que depende do componente X, o componente X tem uma razão a mais para não mudar. O componente X é dito <em>estável</em>.</p>\n<p>O componente X é <em>responsável</em> pelos componentes que dependem dele.</p>\n<p>O componente X é <em>independente</em> pois ele não depende de nenhum outro componente.</p>\n<p>O componente Y depende de três componentes. O componente Y é dito como muito <em>instável</em>.</p>\n<p>O componente Y é <em>irresponsável</em> pois nenhum componente depende dele.</p>\n<p>O componente Y é <em>dependente</em> pois qualquer mudança em algum dos componentes que ele depende pode afetar ele.</p>\n<p>Existem métricas para calcular a estabilidade de um componente.</p>\n<blockquote>\n<p><em>fan in</em>: Dependências que chegam. Número de classes fora do componente que dependem das classes contidas no componente.</p>\n<p><em>fan out</em>: Dependências que saem. Número de classes contidas nesse componente que dependem de classes de componentes externos.</p>\n<p><em>I</em>: Instabilidade. I = <em>fan-out</em> / (<em>fan in</em> + <em>fan out</em>).</p>\n</blockquote>\n<p>O SDP sugere que a métricas de instabilidade de componente deve ser maior que as métricas de instabilidade dos componentes dependentes.</p>\n<blockquote>\n<p>Os componentes mutáveis estão no topo e dependem do componente estável abaixo.</p>\n</blockquote>\n<p>Quando houver violações do SDP é novamente possível usar o DIP, para inverter o fluxo de dependência e evitar a violação.</p>\n<h3>SAP: Stable Abstraction Principle</h3>\n<blockquote>\n<p>Estabelece uma relação entre estabilidade e abstração. Por um lado, ele diz que um componente estável deve também ser abstrato para que essa estabilidade não impeça a sua extensão. Por outro lado, ele afirma que um componente instável deve ser concreto, já que a sua instabilidade permite que o código concreto dentro dele seja facilmente modificado.</p>\n</blockquote>\n<p>A aplicação do SAP e do SDP implica no conceito do DIP a nível de componentes. </p>\n<p>Existem métricas para calcular a abstração de um componente.</p>\n<blockquote>\n<p>Nc: número de classes concretas</p>\n<p>Na: número de classes abstratas.</p>\n<p>A: Abstração. Na/Nc.</p>\n</blockquote>\n<p>Por fim é possível estabelecer relação entre Instabilidade e Abstração. Baseado na projeção dos pontos <em>A</em> e <em>I</em> fica claro onde fica a sequencia principal, área onde os componentes deveriam habitar. Também é possível identificar as zonas de <em>inutilidade</em> e de <em>dor</em>.</p>\n<blockquote>\n<p>(Zona da dor) Esse componente é altamente estável, concreto e, por ser rígido, indesejável. Não pode ser estendido por que não é abstrato e é muito difícil de muar por causa da sua estabilidade.</p>\n</blockquote>\n<blockquote>\n<p>(Zona da inutilidade) Essa posição é indesejável porque indica abstração máxima sem dependentes. Componentes como esse são inúteis.</p>\n</blockquote>\n<blockquote>\n<p>A posição mais recomendável para um componente é em uma das duas extremidades da Sequencia Principal. Os bons arquitetos lutam para colocar a maioria dos componentes nessas extremidades.</p>\n</blockquote>\n<p>Por fim, ainda é possível calcular a Distancia dos componentes da Sequencia Principal. E usar essa métrica para determinar quando um componente precisa ser reexaminado e reestruturado.</p>\n<h1>V - Arquitetura</h1>\n<h2>15 - O que é arquitetura?</h2>\n<p>É a forma dada a um sistema pelo seus criadores.</p>\n<blockquote>\n<p>Primeiro, um arquiteto de software é e continua a ser um programador. [...] Os arquitetos de software são os melhores programadores, e continuam a assumir tarefas de programação enquanto orientam o resto da equipe para um design que maximize a produtividade.</p>\n</blockquote>\n<blockquote>\n<p>Eles não podem fazer o seu trabalho de maneira adequada se não vivenciarem os problemas que criam para os demais programadores.</p>\n</blockquote>\n<p>É crucial para uma boa arquitetura deixar o maior numero de opções abertas pelo maior tempo possível.</p>\n<p>A arquitetura é menos sobre o funcionamento esperado pelo sistema e mais sobre suportar o ciclo de vida do sistema.</p>\n<blockquote>\n<p>Uma boa arquitetura torna o sistema fácil de entender, fácil de desenvolver, fácil de manter e fácil de implantar. O objetivo final é minimizar o custo da vida útil do sistema e não maximizar a produtividade do programador.</p>\n</blockquote>\n<h3>Desenvolvimento</h3>\n<p>Um sistema que não seja fácil de desenvolver não terá um ciclo de vida longo.</p>\n<blockquote>\n<p>Equipes com estruturas diferentes exigem decisões arquiteturais diferentes.</p>\n</blockquote>\n<p>A arquitetura de um componente por equipe não é a melhor para implantação(deploy).</p>\n<h3>Implantação</h3>\n<blockquote>\n<p> O objetivo da arquitetura de software deve ser criar um sistema que seja facilmente implantado <em>com uma única ação</em>.</p>\n</blockquote>\n<p>Como é incomum dos arquitetos considerarem a implantação durante o desenvolvimento inicial. Quando chega o momento de fazer deploy o resultado é um deploy complexo e sofrível.</p>\n<blockquote>\n<p>Se os arquitetos tivessem considerados as questões de implantação no inicio do processo,poderiam ter optado por menos serviços, um hibrido de serviços e componentes in-process e um meio mais integrado para lidar com as interconexões.</p>\n</blockquote>\n<h3>Operação</h3>\n<p>É o ciclo que menos tem impacto da arquitetura.</p>\n<blockquote>\n<p>Quase qualquer dificuldade operacional pode ser resolvida pela inclusão de mais hardware no sistema sem que ocorram impactos drásticos sobrea a arquitetura de software.</p>\n</blockquote>\n<p>Porém, uma boa arquitetura comunica as necessidades do operacionais do sistema.</p>\n<h3>Manutenção</h3>\n<p>A manutenção é o mais caro de todos.</p>\n<blockquote>\n<p>O conjunto sem fim de novos recursos e o rastro inevitável de feitos e correções consome enormes quantidades de recursos humanos.</p>\n</blockquote>\n<p>Durante a manutenção é muito provável que novos erros sejam criados. Uma boa arquitetura isola os componentes para que evite que novos erros sejam inseridos acidentalmente.</p>\n<p>Com esses quatro ciclos de vida do software em mente. Enfim podemos esclarecer o objetivo do arquiteto:</p>\n<blockquote>\n<p>O objetivo do arquiteto é criar uma forma para o sistema que reconheça a política como o elemento mais essencial do sistema e torne os detalhes <em>irrelevantes</em> para essa política. Isso permite que as decisões para esses detalhes sejam <em>adiadas</em> e <em>deferidas</em>.</p>\n</blockquote>\n<p>Com isso conclui-se que bancos de dados, modelo de apresentação, estratégia de exposição não são decisões que precisam ser tomadas no começo do projeto.</p>\n<p>O autor traz exemplos da década de 60, em que o código estava completamente acoplado ao dispositivo IO. A estratégia deles funcionou bem por algum tempo, porem quando o leitor de cartões foi substituído por uma fita magnética todo o código foi impactado.</p>\n<p>Num segundo exemplo, o autor tinha um código em que era desacoplado ao dispositivo IO. Quando foi necessário trocar o dispositivo IO, as regras de negócios ficaram intactas e só foi necessário implementar o novo dispositivo IO.</p>\n<blockquote>\n<p>Bons arquitetos projetam a política de modo que as decisões sobre os detalhes possam ser adiadas e diferidas pelo maior tempo possível.</p>\n</blockquote>\n<h2>16 - Independência</h2>\n<h3>Casos de uso</h3>\n<p>Suportar casos de uso é a primeira preocupação do arquiteto e a prioridade da arquitetura. Um sistema de carrinho de compras vai ter uma estrutura transparente mostrando os casos de uso do sistema.</p>\n<h3>Operação</h3>\n<p>A arquitetura te um papel substancial na operação. Por exemplo, se a aplicação vai rodar vários processos dentro de uma maquina, em várias maquinas independentes.</p>\n<p>A decisão de como operar é uma das opções que um bom arquiteto deixa em aberto.</p>\n<blockquote>\n<p>Um sistema escrito como monólito, e dependente dessa estrutura monolítica, não pode ser atualizado facilmente para múltiplos processos, múltiplas threads ou microserviços caso surja a necessidade.</p>\n</blockquote>\n<h3>Desenvolvimento</h3>\n<p>Quando a organização for grande, com muitas equipes e muitas organizações. É preciso dividir o desenvolvimento em componentes desenvolvíveis e bem isolados para cada equipe.</p>\n<h3>Implantação</h3>\n<blockquote>\n<p>Uma boa arquitetura não depende de dúzias de pequenos scripts de configuração e ajustes em arquivos de propriedades.</p>\n</blockquote>\n<h3>Deixando opções abertas</h3>\n<p>Uma boa arquitetura equilibra os pontos acima. Não é fácil, por que não sabemos todos casos de usos, requisitos de implantação e estrutura da equipe, e inevitavelmente essas informações mudam.</p>\n<h3>Desacoplando camadas</h3>\n<blockquote>\n<p>Separar coisas que mudam por razões diferentes e reunir coisas que mudam pelas mesmas razões - de acordo com o contexto da intenção do sistema.</p>\n</blockquote>\n<p>O que muda por razões diferentes?</p>\n<blockquote>\n<p>A validação do campo de entrada é uma regra de negócios fortemente ligada à própria aplicação.</p>\n</blockquote>\n<p>Banco de dados, UI, e demais também mudam por razões diferentes</p>\n<h3>Desacoplando os casos de uso</h3>\n<p>Os casos de usos usam e são usados por seus respectivos componentes de outras camadas como UI, e banco de dados.</p>\n<h3>Modo de desacoplamento</h3>\n<p>Quando o desacoplamento é feito, ele permite que os diferentes caso de usos sejam executados em diferentes ambientes com diferentes requisitos de implantação. Para tal, é preciso que os componentes sejam executados servidores separados, e sem depender um dos outros, sendo a comunicação feita pela rede.</p>\n<blockquote>\n<p>as vezes temos que separar os nossos componentes até o nível do serviço.</p>\n</blockquote>\n<h3>Desenvolvimento independente</h3>\n<blockquote>\n<p>Quando os componentes são fortemente desacoplados, a interferência entre as equipes é mitigada.</p>\n</blockquote>\n<h3>Implantação independente</h3>\n<blockquote>\n<p>Adicionar um novo caso de uso, então seria tão simples quanto adicionar um novo arquivo jar ou serviços ao sistema sem mexer no resto.</p>\n</blockquote>\n<h3>Duplicação</h3>\n<p>Existem as duplicações verdadeiras que temos que remover.</p>\n<p>Existem as duplicações falsas ou acidentais. Aparentemente duplicadas, mas evoluem por caminhos diferentes e por razões diferentes.</p>\n<blockquote>\n<p>Quando separar verticalmente os casos de uso uns dos outros, você encontrará esse problema, e a tentação aqui será acoplar os casos de uso por eles conterem estruturas de tela similares, algoritmos similares, ou consultas de banco de dados e/ou esquemas similares. Tenha cuidado resista à tentação de cometer o pecado da eliminação automática da duplicação. Certifique-se que a duplicação é real.</p>\n</blockquote>\n<h3>Modos de desacoplamento (novamente)</h3>\n<p>Três níveis:</p>\n<ol>\n<li>Nível de fonte: Controlar as dependências entre módulos.</li>\n<li>Nível de implementação: Controlar as dependências entre unidades implantáveis (jar, DLL, ...). Os componentes precisam ser particionados em unidade independentemente implantáveis.</li>\n<li>Nível de serviço: Controlar as dependências através das estruturas de dados que enviamos pela rede.</li>\n</ol>\n<p>Escolher o 3° modo de desacoplamento diretamente traz um alto custo. Além do que, caso for um serviço desnecessário, ele acarreta em desperdício de esforço, memória e ciclos.</p>\n<p>A evolução natural entre os 3 níveis é o que agrega mais valor, e garante tempo para entender as necessidades dos sistemas e evoluir conforme as necessidades evoluem.</p>\n<blockquote>\n<p>Pessoalmente, prefiro adiar o desacoplamento até o ponto em que um serviço possa ser formado, se necessário, e então, deixar os componentes no mesmo espaço de endereço pelo máximo de tempo possível. Isso deixa aberta a opção de um serviço.</p>\n</blockquote>\n<blockquote>\n<p>Uma boa arquitetura deve possibilitar que o sistema nasça como monolito, seja implantado em um único arquivo e, então, cresça como um conjunto de unidades independentemente implantáveis, incluindo serviços e microserviços.</p>\n</blockquote>\n<h2>17 - Fronteiras: Estabelecendo Limites</h2>\n<blockquote>\n<p>A arquitetura de software é a arte de estabelecer limites que eu chamo de fronteiras. </p>\n</blockquote>\n<p>As fronteiras vão ajudar na separação de responsabilidade, e a diminuir o acoplamento. E estabelecer quem quais classes e módulos ficam em quais lugares.</p>\n<p>Decisões que não estão relacionadas ao negócio, deveriam ser adiadas, como escolher qual framework, banco de dados, servidores web. Uma boa arquitetura de sistema permite adiar essas decisões até o ultimo minuto.</p>\n<h3>FitNesse</h3>\n<p>Estabelecer um limite entre o banco de dados e as regras de negócios, permite que a decisão de acesso aos dados seja adiada. E com isso os problemas relacionados a isso também (consistência de schema, . </p>\n<h3>Quais limites você deve estabelecer e quando?</h3>\n<blockquote>\n<p>O banco de dados é uma ferramenta que as regras de negócio podem usar <em>indiretamente</em>. As regras de negócio não precisam conhecer o esquema, a linguagem de consulta, ou qualquer outro detalhe sobre o banco de dados.</p>\n</blockquote>\n<p>As regras de negócio consultam o banco de dados através de uma Interface. As regras de negócio só precisam saber da interface. Dessa maneira, as regras de negócios ficam independentes dos bancos de dados, permitindo adiar a decisão de qual tecnologia usar.</p>\n<blockquote>\n<p>De fato, a história da tecnologia de desenvolvimento de software é a história de como criar plug-ins de maneira conveniente para estabelecer uma arquitetura de sistema escalonável. As regras centrais de negócio são mantidas separadas e independentes desses componentes, que podem ser opcionais ou implementados de muitas formas diferentes.</p>\n</blockquote>\n<h3>O argumento sobre plug-in</h3>\n<p>Considerando que os demais componentes que são dependentes das regras de negócio. As fronteiras garantem que mudanças em componentes como UI ou banco de dados não vão impactar as regras de negócio.</p>\n<blockquote>\n<p>Esse é simplesmente o principio da responsabilidade única de novo. O SRP nos diz onde devemos estabelecer limites.</p>\n</blockquote>\n<h2>18 - Anatomia do limite</h2>\n<p>Existem vários tipos de limites.</p>\n<blockquote>\n<p>O truque para criar um cruzamento de limites adequado é lidar com as dependências de código-fonte.</p>\n</blockquote>\n<h3>Cruzando limites</h3>\n<blockquote>\n<p>O cruzamento de limites não é nada mais do que uma função de um lado do limite chamando uma função do outro lado e transmitindo alguns dados.</p>\n</blockquote>\n<p>Quando o código fonte muda, as suas dependências precisam mudar e ser recompiladas. Por isso é importante estabelecer um sentido para as mudanças, não queremos que quando o banco de dados mude, isso tenha impacto nas regras de negócio.</p>\n<h3>O temido monolito</h3>\n<p>O limite mais simples, pode ser encontrado nos monolitos. Pois nele não tem uma restrição física. Mesmo em um monolito é possível e importante que se estabeleça os limites.</p>\n<p>Dentro do monolito existem dois exemplos de cruzamentos de limites.</p>\n<p>Exemplo: Cliente mais baixo chamando um serviço mais alto. (Framework chamando um use case).\nNesse caso, o cliente mais baixo pode simplesmente invocar os métodos do serviço mais alto. As dependências ficam do lado do Framework. Mudar o framework não altera o serviço.</p>\n<p>Exemplo: Cliente mais alto chamando um serviço mais baixo. ( Use case chamando um banco de dados ).\nNesse caso, o cliente alto invoca os métodos de uma interface que estão no mesmo nível que o seu. O serviço mais baixo implementa a interface de nível mais alto.</p>\n<h3>Componentes de implantação</h3>\n<p>A implantação não envolve compilação. Somente o uso de binários agrupados em um único arquivo ou diretório executável.</p>\n<p>Mesmas estrategias para segregar componentes que dos monolitos.</p>\n<h3>Processos locais</h3>\n<p>Apartir daqui existe um limite arquitetural físico, a memória. Ou o fato, de se evitar compartilhamento dela. </p>\n<blockquote>\n<p>A estratégia de segregação entre processos locais é a mesma aplicável a monolitos e componentes binários.</p>\n</blockquote>\n<p>O processos locais de nível mais alto, não devem conter nomes, endereços físicos ou qualquer informação sobre os processos locais de níveis mais baixos.</p>\n<h3>Serviços</h3>\n<p>O limite mais forte é um serviço. Roda em processos separados e até maquinas separadas.</p>\n<blockquote>\n<p>As mesmas regras se aplicam aos serviços e processos locais. Os serviços de nível mais baixos devem ser plug-ins para serviços de nível mais alto.</p>\n</blockquote>\n<h2>19 - Política e Nível</h2>\n<p>Programa de software é um conjunto de politicas que descrevem uma entrada e uma saída.</p>\n<p>Parte da arquitetura é agrupar as politicas que mudam pelos mesmas razões, no mesmo momento e que estão no mesmo nível e separar as que não compartilham essas carácteristicas devem ser separadas.</p>\n<blockquote>\n<p>A palavra nível pode ser definida de forma restrita como \"distância das entradas e saídas\". Quanto mais longe uma política está das entradas e saídas do sistema, maior é o seu nível. As políticas que lidam com entradas e saídas são as políticas de nível mais baixo no sistema.</p>\n</blockquote>\n<p>Queremos que as dependências de código-fonte sejam desacopladas do fluxo de dados e <em>acopladas ao nível</em>.</p>","frontmatter":{"title":"Arquitetura Limpa","language":"pt-BR","coverPath":"arquitetura-limpa","status":"Reading","date":"2021-01-01"}}},{"node":{"html":"<h2>1. Serverless in five minutes</h2>\n<blockquote>\n<p>Serverless applications are, at the most basic technical level, software that runs in an environment where the hosting provider is fully responsible for infrastructural and operational tasks such as receiving network requests, scaling the computing infrastructure on demand, monitoring and recovery.</p>\n</blockquote>\n<blockquote>\n<p>Serverless applications only need to provide the code that should run when an event happens and configure the triggers in the execution environment to call that code.</p>\n</blockquote>\n<blockquote>\n<p>The right comparison to think about is WiFi. When you browse the internet using a ‘wireless’ connection in a coffee shop, your device talks to a router just a few feet away, and there is a wire coming out of that router and taking your packets to the internet. But you don’t need to care about that wire or manage it actively. AWS Lambda is serverless in the same way WiFi is wireless. here are network servers, virtual and physical machines running in the background, but you don’t really need to care about them any more.</p>\n</blockquote>\n<p>Two main benefits:</p>\n<ol>\n<li>Short time to market</li>\n<li>Reduced operational costs</li>\n</ol>\n<h3>The pricing model</h3>\n<blockquote>\n<p> (AWS Lambda) provides standardized execution environments ... and algorithms to automatically scale containers according to the workload.</p>\n</blockquote>\n<blockquote>\n<p>The serverless pricing model is a lot more important than the technology for application developers.</p>\n</blockquote>\n<blockquote>\n<p>... you pay for actual usage ... You never pay for idle infrastructure or when tasks are waiting on user requests.</p>\n</blockquote>\n<blockquote>\n<p>The maximum memory allowed for a task, and the time it spent executing</p>\n</blockquote>\n<blockquote>\n<p>When companies pay for reserved capacity, copies of the production environment usually multiply the operational costs, even though they are idle most of the time. That is why staging and acceptance testing environments usually end up being slimmed-down versions of the real thing.</p>\n</blockquote>\n<blockquote>\n<p>With billing based on actual usage, environments don’t cost anything if they are idle.</p>\n</blockquote>\n<h4>How request pricing affects deployment architecture</h4>\n<blockquote>\n<p>serverless is effectively platform-as-a-service - Simon wardley</p>\n</blockquote>\n<blockquote>\n<p>The three critical aspects of a serverless application:\n• Infrastructure providers are responsible for handling incoming network requests.\n• Operational cost is based on actual utilization, broken down by individual requests.\n• Technical operations (deployment, scaling, security and monitoring) are included in the price.</p>\n</blockquote>\n<blockquote>\n<p>The number of requests matters, not the number of environments.</p>\n</blockquote>\n<h4>How request pricing affects security</h4>\n<blockquote>\n<p>each Lambda function can do a focused task and work under significantly restricted access privileges. It becomes easy to apply the principle of least privilege, allowing a task access only to resources and information required for its legitimate purpose</p>\n</blockquote>\n<blockquote>\n<p>Because the service provider takes care of containers, it was able easily to drain traffic from old versions, send new requests to patched containers and just manage the whole thing for us.</p>\n</blockquote>\n<h4>How request pricing affects product decisions</h4>\n<blockquote>\n<p>With Lambda, you can launch a new version for the specific customer as soon as the feature is ready, and keep everyone else on the old version until the feature is fully developed. Running two environments doesn’t really cost any more than a single one.</p>\n</blockquote>\n<blockquote>\n<p>Just create an experimental version of the application and send % of the traffic there. he number of users and requests is still the same, so running two versions costs the same as working with a single version.</p>\n</blockquote>\n<h3>Important AWS Lambda technical constraints</h3>\n<blockquote>\n<p>The four important technical limitations that you needed to consider when evaluating whether something should run in Lambda:</p>\n<ul>\n<li>No session affinity</li>\n<li>Non-deterministic latency</li>\n<li>Execution time limited to 15 minutes</li>\n<li>No direct control over processing power</li>\n</ul>\n</blockquote>\n<h4>No session affinity</h4>\n<blockquote>\n<p>Purely on Lambda, there is no way to control request routing or somehow ensure that requests from the same source arrive in sequence to the same destination.</p>\n</blockquote>\n<blockquote>\n<p>There are no guarantees about preserving state across requests, and application developers have no control over the routing.</p>\n</blockquote>\n<blockquote>\n<p>Don’t design for stateless execution, design for a share-nothing architecture.</p>\n</blockquote>\n<blockquote>\n<p>You can still cache or pre-calculate things that do not depend on a particular user, but user sessions and state have to be somewhere else</p>\n</blockquote>\n<h4>Non-deterministic latency</h4>\n<blockquote>\n<p>Lambda is optimized for maximizing throughput, not for minimizing latency.</p>\n</blockquote>\n<blockquote>\n<p>The latency of processing a single request isn’t really deterministic.</p>\n</blockquote>\n<blockquote>\n<p>A cold start, in the serverless jargon, is when an incoming request needs to wait for a new Lambda instance for processing</p>\n</blockquote>\n<blockquote>\n<p>empirical tests suggest that with JavaScript or Python, the cold start is less than half a second. With Java and C#, it still may take a bit longer depending on the application size,</p>\n</blockquote>\n<h4>Limited execution time</h4>\n<blockquote>\n<p>Currently, a Lambda function is allowed to run for three seconds by default, and you can configure it to allow up to 15 minutes.</p>\n</blockquote>\n<blockquote>\n<p>instead of using Lambda to start a remote task and then wait for it to complete, split that into two Lambdas</p>\n</blockquote>\n<h4>No direction control over processing power</h4>\n<blockquote>\n<p>The only container choice you can make is the amount of memory, from 128MB to about 3 GB. Lambda is not good for tasks that require GPUs.</p>\n</blockquote>\n<blockquote>\n<p>The memory configuration has an indirect impact on processing power.Lambda allocates CPU power in proportion to memory, so that at 1792 MB a function has access to one full virtual CPU.</p>\n</blockquote>\n<blockquote>\n<p>With Node.js, all tasks run through a single core anyway, so with JavaScript you won’t get any further processing speed improvements if you ask for more than 1.75 GB. With Java or other languages that can take advantage of multiple cores, asking for the maximum allowed memory might give you faster responses and lower cost for CPU-intensive tasks.</p>\n</blockquote>\n<blockquote>\n<p>the best way to optimise costs and performance is to explore various parameter combinations</p>\n</blockquote>\n<h3>When to use Lambda</h3>\n<blockquote>\n<p>Lambda is great for use cases where throughput is critical and the tasks parallelise nicely.</p>\n</blockquote>\n<blockquote>\n<p>Any single request taking a few hundred milliseconds more than average won’t be noticeable to typical users, and Lambda will ensure that everyone gets served relatively quickly regardless of traffic spikes.</p>\n</blockquote>\n<blockquote>\n<p>Longer on-demand computational tasks that can execute in less than 15 minutes, or could be split into independent segments</p>\n</blockquote>\n<blockquote>\n<p>Tasks that need high availability and good operational infrastructure, but do not need to guarantee latency</p>\n</blockquote>\n<blockquote>\n<p>Lambda is currently not suitable for tasks that require guaranteed latency</p>\n</blockquote>\n<blockquote>\n<p>(Lambda is currently not suitable) ...tasks that could potentially run for longer than 15 minutes.</p>\n</blockquote>\n<blockquote>\n<p>(Lambda is currently not suitable)... tasks that require a huge amount of processing power and coordination</p>\n</blockquote>\n<h1>Part I - Basic development tasks</h1>\n<h2>2. Setup tools for local development</h2>\n<blockquote>\n<p>The AWS Serverless Application 0odel (SA0) is a set of products that simplify developing, testing and de\nploying applications using AWS Lambda.</p>\n</blockquote>\n<h2>3. Create a web service</h2>\n<blockquote>\n<p>Lambda has pre-packaged execution environments, called runtimes, for many popular language</p>\n</blockquote>\n<h3>Infrastructure as a code</h3>\n<blockquote>\n<p>CloudFormation converts a source file describing an application infrastructure (called template) into a set of running, configured cloud resources (called stack).</p>\n</blockquote>\n<blockquote>\n<p>We can modify the template file, and CloudFormation will reconfigure or delete only the resources that actually need to change. If a resource update fails for whatever reason, CloudFormation will reset all the other resources to the previous configuration, managing a whole set of infrastructural components as a single unit.</p>\n</blockquote>\n<h3>The lambda programming model</h3>\n<blockquote>\n<p>The Lambda function is asynchronous in JavaScript, meaning that it has to either be marked as async (line in the previous listing) or return a Promise .</p>\n</blockquote>\n<blockquote>\n<p>In case of HTTP requests, the Lambda function needs to respond with an object containing the status code and the body of the HTTP response.</p>\n</blockquote>\n<h3>Deploying SAM applications</h3>\n<blockquote>\n<p>In order to create a Lambda function, CloudFormation expects a fully self-contained ZIP archive.</p>\n</blockquote>\n<blockquote>\n<p>In general, turning a SAM application on your disk into resources running in AWS requires three steps:</p>\n<ol>\n<li>Build: create a clean copy of all Lambda functions, remove test and development resources, and download third-party dependencies.</li>\n<li>Package: bundle each function into a self-contained ZIP archive and up load to S3 , and produce a copy of the source application template that points to remote resources instead of local directories.</li>\n<li>Deploy: upload the packaged template to CloudFormation, and execute the changes to create a running infrastructure.</li>\n</ol>\n</blockquote>\n<h4>Step 2: Package</h4>\n<blockquote>\n<p>we will first need an S3 bucket to host our function packages. In the continuous delivery jargon, this will be our binary artefact storage.</p>\n</blockquote>\n<h3>Inspecting a Stack from the command line</h3>\n<blockquote>\n<p>AWS command line tools use the JMESPath query syntax</p>\n</blockquote>\n<h2>4. Development and Troubleshooting</h2>\n<blockquote>\n<p>Reliable centralised logging for a highly distributed system is a huge technical challenge, but\nwith Lambda that comes with the basic setup and is included in the price.</p>\n</blockquote>\n<h3>Retrieving logs from the command line</h3>\n<h2>5. Safe Deployment</h2>\n<blockquote>\n<p>a Lambda deployment does not create or destroy any containers. It only creates a new function configuration.</p>\n</blockquote>\n<h3>Function configuration</h3>\n<blockquote>\n<p>With AWS Lambda in particular, events do not target a particular function. hey target a particular version of a function, or, more precisely, events are connected to a version of the function configuration.</p>\n</blockquote>\n<blockquote>\n<p>Function configuration describes all the properties of a runtime environment necessary to spin up a new container.</p>\n<ul>\n<li>The runtime type and </li>\n<li>How much memory and time the function can use</li>\n<li>The URL of the function code package</li>\n<li>The IAM role specifying what this function can access in AWS and who can call this function</li>\n<li>Error recovery, logging and environment parameters of the function</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Published versions are read-only copies of function configurations, and they are not wiped out after a subsequent update.</p>\n</blockquote>\n<blockquote>\n<p>Lambda allows us to declare configuration aliases, meaningful names pointing to a numerical version. For example, we can create an alias called live to represent a published configuration version, and set up all event sources to request that alias. After an application update, we do not need to rewire event sources.</p>\n</blockquote>\n<h1>Part II - Working with AWS Services</h1>\n<h2>6. Handling HTTP Requests</h2>\n<blockquote>\n<p>API Gateway is a service for publishing and managing REST and Websocket APIs.</p>\n</blockquote>\n<h1>To read</h1>\n<ol>\n<li>Serverless Computing: Economic and Architectural impact</li>\n</ol>","frontmatter":{"title":"Running Serverless","language":"en-US","coverPath":"running-serverless","status":"Reading","date":"2021-01-01"}}},{"node":{"html":"","frontmatter":{"title":"Scrum guide","language":"en-US","coverPath":"/images/cover/scrum-guide.jpg","status":"Read","date":"2021-01-01"}}},{"node":{"html":"<h1>Foreword - Jim Highsmith</h1>\n<blockquote>\n<p>Agile planning is deceptive. At one level, it's pretty easy - create a few story cards, prioritize them, allocate them to release iterations, then add additional detail to get a next iteration plan.</p>\n</blockquote>\n<blockquote>\n<p>First, agile teams do a lot of planning, but it spread out much more evenly over the entire project. Second, agile teams squarely face the critical factor that many non-agile teams ignore - uncertainty.</p>\n</blockquote>\n<blockquote>\n<p>Is planning important? absolutely\nIs adjusting the plan as knowledge is gained and uncertainty reduced important? absolutely</p>\n</blockquote>\n<blockquote>\n<p>The agile approach is focused on actually delivering value and not on making outrageous and unachievable plans and commitments.</p>\n</blockquote>\n<blockquote>\n<p>Agile approaches are \"plan-do-adapt\", \"plan-do-adapt\".</p>\n</blockquote>\n<blockquote>\n<p>But in reality, you are Agile, Extreme or otherwise when you know enough about the practices to adapt them to the reality of your own specific situation. Continuous learning and adaptation are core to agile development.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Planning seems easy, but this is a deception. </p>\n<p>Planning is spread across the project life cycle, at each planning we use the information gathered in the last cycle.</p>\n<p>Agile planning focus on value not on plans.</p>\n<h1>Foreword - Gabrielle Benefield</h1>\n<blockquote>\n<p>Giving a product or project manager sage like qualities to be able to foresee what others, who are experts in their own work, can really deliver is business suicide. Often this approach is really a way of saying yes to the business when asked to deliver on unrealistic goals.</p>\n</blockquote>\n<blockquote>\n<p>Teams can still be flexible and change course as desired, but it's important to have a roadmap to follow. It's not enough to go fast if you are heading in the wrong direction.</p>\n</blockquote>\n<blockquote>\n<p>Keeping everything highly transparent, and letting the business know of any changes as they come up, means that the business can adapt quickly to make the best decisions.</p>\n</blockquote>\n<h2>My Summary</h2>\n<p>Saying yes for everything is a business suicide by creating unrealistic goals.</p>\n<h1>Introduction</h1>\n<blockquote>\n<p>Without agile estimating and planning, we cannot have agile projects.</p>\n</blockquote>\n<h1>Part I - The problem and the goal</h1>\n<h2>Chapter 1 - The purpose of planning</h2>\n<blockquote>\n<p>The team that does no planning cannot answer the most basic questions, such \"When will you be done?\" and \"Can we schedule the product release for June?\"</p>\n</blockquote>\n<blockquote>\n<p>The team that overplans deludes themselves into thinking that any plan can be \"right\".</p>\n</blockquote>\n<blockquote>\n<p>The cone of uncertainty shows that during the feasibility phase of a project a schedule estimate is typically as far off as 60% to 160%. That is, a project expected to take 20 weeks could take anywhere from 12 to 32 weeks. After the requirements are written, the estimate might still be off +/- 15% in either direction. So an estimate of 20 weeks means work that takes 17 to 23 weeks.</p>\n</blockquote>\n<h3>Why Do It?</h3>\n<blockquote>\n<p>Plans and schedules may be needed for a variety of legitimate reasons, such as planning marketing campaigns, scheduling product release activities, training internal users, and so on.</p>\n</blockquote>\n<blockquote>\n<p>Planning - especially an ongoing iterative approach to planning - is a quest for value.</p>\n</blockquote>\n<blockquote>\n<p>At the start of a project we may decide that a product should contain a specific set of features and be released on August 31. But in June we may decide that a slightly later date with more features will be better  Or we may decide that slightly sooner with slightly fewer features will be better.</p>\n</blockquote>\n<blockquote>\n<p>A good planning process supports this by</p>\n<ul>\n<li>Reducing risk</li>\n<li>Reducing uncertainty</li>\n<li>Supporting better decision making</li>\n<li>Establishing trust</li>\n<li>Conveying information</li>\n</ul>\n</blockquote>\n<h3>Reducing Risk</h3>\n<blockquote>\n<p>Some projects are so risky that we may choose not to start once we've learned about the risks. Other projects may contain features whose risks can be contained by early attention.</p>\n</blockquote>\n<blockquote>\n<p>The project team can opt to eliminate the risk right then by spending time learning about the legacy system. Or risk can be noted and the estimate for the work either made larger or expressed as a range to account for the greater uncertainty and risk.</p>\n</blockquote>\n<h3>Reducing Uncertainty</h3>\n<blockquote>\n<p>It is critical that this new knowledge be acknowledged and factored into an iterative planning process that is designed to help a team refine their vision of the product.</p>\n</blockquote>\n<blockquote>\n<p>The most critical risk facing most projects is the risk of developing the wrong product.</p>\n</blockquote>\n<blockquote>\n<p>The often-cited CHAOS studies ( Standish 2001 ) define a successful project as on time, on budget, and with all features as initially specified. This is a dangerous definition because it fails to acknowledge that a feature that looked good when the project was started may not be worth its development cost once the team begins on the project.</p>\n</blockquote>\n<blockquote>\n<p>We want to encourage projects on which investments, schedule and feature decisions are periodically reassessed.</p>\n</blockquote>\n<blockquote>\n<p>The product's users and customer would probably not be satisfied wonderful new feature ideas had been rejected in favor of mediocre ones simple because the mediocre features were in the inital plan.</p>\n</blockquote>\n<h3>Supporting Better Decision Making</h3>\n<blockquote>\n<p>Organizations need estimates in order to make decisions beyond whether or not to start a project.</p>\n</blockquote>\n<blockquote>\n<p>We are constantly making decisions between functionality and effort, cost and time.</p>\n</blockquote>\n<h3>Establishing Trust</h3>\n<blockquote>\n<p>Frequent reliable delivery of promised features builds trust between the developers of a product and the customers of that product. </p>\n</blockquote>\n<blockquote>\n<p>Reliable estimates enable reliable delivery.</p>\n</blockquote>\n<blockquote>\n<p>Reliable estimates benefit developers by allowing them to work at a sustainable pace. This leads to higher-quality code and fewer bugs.</p>\n</blockquote>\n<h3>Conveying Information</h3>\n<blockquote>\n<p>A plan does not guarantee an exact set of features on an exact date at a specified cost. A plan does, however, communicates and establish a set of baseline expectations.</p>\n</blockquote>\n<h3>What Makes a Good Plan?</h3>\n<blockquote>\n<p>A good plan is one that stakeholders find sufficiently reliable that they can use it as the basis for making decisions.</p>\n</blockquote>\n<blockquote>\n<p>You determine that the new version will be ready for release in six months. You create a plan that describes a set of features that are certain to be in the new version and another set of features that may or may not be included, depending on how well things progress.</p>\n<p>[...] If development takes twelve months instead of the planned six months, that was not a good plan.</p>\n<p>[...] However, if the project takes seven months instead of six, the plan was probably still useful. Yes, the plan was incorrect, and yes, it may have led to some slightly mistimed decisions.</p>\n<p>[...]The plan, although inaccurate, was even more likely useful if we consider that it should have been updated regularly throughout the course of the project.</p>\n</blockquote>\n<h3>What Makes Planning Agile?</h3>\n<blockquote>\n<p>Agile planning shifts the emphasis from the plan to the planning.</p>\n</blockquote>\n<blockquote>\n<p>An agile plan is one that we are not only willing , but also eager to change. We don't want to change the plan just for the sake of changing, but we want to change because change means we've learned something or that we've avoided a mistake.</p>\n</blockquote>\n<blockquote>\n<p>[...] we need plans that are easily changed. [...] An agile plan is one that is easy to change.</p>\n</blockquote>\n<blockquote>\n<p>There are many ways we can change the plan without changing the date. We can drop a feature, we can reduce the scope of a feature, we can possibly add people to the project, and so on.</p>\n</blockquote>\n<blockquote>\n<p>An agile planning:</p>\n<ul>\n<li>Is focused more on the planning than on the plan</li>\n<li>Encourages change</li>\n<li>Results in plans that are easily changed</li>\n<li>Is spread throughout the project</li>\n</ul>\n</blockquote>\n<h3>My Summary</h3>\n<p>The purpose of planning is to answer questions like, \"When it will be done?\", \"Can we schedule the release for June?\". If you don't plan you cannot answer, if you plan to much you become delusional that the plan will work.</p>\n<p>Planning is a quest for value.</p>\n<p>A good planning process reduces risk and uncertainty, establishes trust and convey information, supports better decision making.</p>\n<p>A good planning reduces risk either by taking action to reduce it, or increasing the work estimate or estimating it as a range.</p>\n<p>A good planning reduces uncertainty, uncertainty of building the wrong product. The definition of a successful project doesn't account the value it delivered, only if it was delivered on time and with the expected scope.  A good planning may change once the uncertainty is reduced and the initial plan is validated.</p>\n<p>A good planning provides a better decision making by considering functionality, effort, time and cost.</p>\n<p>A good planning establishes trust by establishing frequent releases of promised software.</p>\n<p>A good planning conveys information, not a commitment of a set of expectation on a date, rather a plan conveys a set of baseline expectations.</p>\n<blockquote>\n<p>A good plan is one that stakeholders find sufficiently reliable that they can use it as the basis for making decisions.</p>\n</blockquote>\n<p>Agile planning changes the focus to the planning instead of focusing on the plan. This means our plans may change, and we are okay with that.</p>\n<p>Agile planning is focused on planning and not on the plan. It's spread through all the project, it encourages changes, and therefore plans are easy to change.</p>\n<h2>Chapter 2 - Why planning fails</h2>\n<blockquote>\n<ul>\n<li>Nearly two-thirds of projects significantly overrun their cost estimates ( Lederer and Prasad 1992 )</li>\n<li>Sixty-four percent of the features included in products are rarely or never used ( Johnson 2002 )</li>\n<li>The average project exceeds its schedule by 100% ( Standish 2001 )</li>\n</ul>\n</blockquote>\n<h3>Planning Is By Activity Rather Than Feature</h3>\n<blockquote>\n<p>A critical problem with traditional approaches to planning is that they focus the completion of activities rather than on the delivery of features. [ ... ] This becomes how we measure the progress of the team.</p>\n</blockquote>\n<blockquote>\n<p>A first problem with activity-based planning is that customers get no value from the completion of activities.</p>\n</blockquote>\n<blockquote>\n<p>When we review a schedule showing activities, we do so looking for forgotten activities rather than for missing features.</p>\n</blockquote>\n<blockquote>\n<p>When faced with overrunning a schedule, some teams attempt to save time by inappropriately reducing quality.</p>\n</blockquote>\n<blockquote>\n<p>Other teams institute change-control policies designed to constrain product changes, even highly valuable changes.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Activities don't finish early</li>\n<li>Lateness is passed down the schedule</li>\n<li>Activities are not independent</li>\n</ul>\n</blockquote>\n<h3>Activities Don't Finish Early</h3>\n<blockquote>\n<p>Parkinson's law ( 1993 ), which states:\nWork expands so as to fill the time available for its completion.</p>\n</blockquote>\n<blockquote>\n<p>If there's a Gant chart hanging on the wall that says an activity is expected to take five days, the programmer assigned to that activity will generally make sure the activity takes the full five days.</p>\n</blockquote>\n<blockquote>\n<p>It is human nature when ahead of that schedule to fill the extra time with other work that we, but perhaps not others, value.</p>\n</blockquote>\n<h3>Lateness Is Passed Down the Schedule</h3>\n<blockquote>\n<p>An early start requires a combination of things to go well; a late start can be caused by one thing going wrong.</p>\n</blockquote>\n<blockquote>\n<p>The problem is compounded because we've already established that activities will rarely finish early. This means that activities will start late and that the lateness will get passed down the schedule.</p>\n</blockquote>\n<h3>Activities Are Not Independent</h3>\n<blockquote>\n<p>Activities are said to be independent if the duration of one activity does not influence the duration of another activity.</p>\n</blockquote>\n<blockquote>\n<p>The real knowledge we should gain in a situation like this is that when an activity takes longer than planned, all similar activities are also like to take longer than planned.</p>\n</blockquote>\n<h3>Multitasking Causes Further Delays</h3>\n<blockquote>\n<p>Clark and Whelwright (1993) studied the effects of multitasking and found that the time an individual spends on value-adding work drops rapidly when the individual is working on more than two tasks.</p>\n</blockquote>\n<blockquote>\n<p>Additionally, in this example, each of the desire units of work remains in process for twenty days rather than ten, as was the case when the work was done serially.</p>\n</blockquote>\n<blockquote>\n<p>[...] assumes that I am not slowed by switching among these activities more frequently. The Clark and Whelwright study indicates that a a loss in productivity will occur.</p>\n</blockquote>\n<h3>Features Are Not Developed by Priority</h3>\n<blockquote>\n<p>Many traditional plans are created with the assumption that all identified activities will be completed. This means that work is typically prioritized an sequenced for the convenience of the development team.</p>\n</blockquote>\n<blockquote>\n<p>Then, with the end of the project approaching,  the team scrambles to meet the schedule by dropping features. Because there was no attempt to work on features in order of priority, some of the features dropped are of greater value than those that are delivered</p>\n</blockquote>\n<h3>We Ignore Uncertainty</h3>\n<blockquote>\n<p>We ignore uncertainty about the product and assume that the initial requirements analysis led to a complete and perfect specification of the product.</p>\n</blockquote>\n<blockquote>\n<p>Similarly, we ignore uncertainty about how we will build the product and pretend we can assign precise estimates (\"two weeks\") to imprecise work.</p>\n</blockquote>\n<blockquote>\n<p>Even with all this uncertainty, schedules are often expressed as a single, unqualified date: \"We will ship on June 30\".</p>\n</blockquote>\n<blockquote>\n<p>As the project progress and as uncertainty and risk are removed from the project, estimates can be refined and made more precise.</p>\n</blockquote>\n<blockquote>\n<p>To reduce uncertainty about what the product should be, work in short iterations, and show (or, ideally give) working software to users every few weeks.</p>\n</blockquote>\n<h3>Estimates Become Commitments</h3>\n<blockquote>\n<p>Each estimate between the end of the week and ten years from now comes with its own probability between 0% and 100% (Armour 2002).</p>\n</blockquote>\n<blockquote>\n<p>A problem with traditional planning can arise if the project team or its stakeholders equate estimating with committing.</p>\n</blockquote>\n<blockquote>\n<p>Commitments are made to dates. Normally the date that a team is asked (or told) to commit to is one to which they would assign a less-than-100% probability</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Planning fails because it focus on the completion of activities rather than of delivery of features. Customers get no value when activities are done.</p>\n<p>When we review a a project schedule, we look for incomplete activities not incomplete features.</p>\n<p>When a schedule is late, quality is sacrificed.</p>\n<p>Because the schedule is usually tight, there is no room for change in the plans, even if there is a high valuable change.</p>\n<p>Planning fails because, activities don't finish early, lateness is passed down the schedule, activities are not independent.</p>\n<p>Planning fails because activities don't finish early. Parkinson's law proposes that work expands to fulfill the time available for its completion. If we have 5 days to finish a task, it will take more or less 5 days. People will start to look for some hot new technology before starting the new task, adding bells and whistles to the development, or they might even be accused of giving a padded estimate.</p>\n<p>Planning fails because lateness is passed down the schedule. Due to the activity based format used in the traditional plans. All activities must finish early in order to the project finish early. However, if one activity delays, the lateness is passed down the schedule. Therefore, it only requires one activity to go wrong to have a later start in the next activity.</p>\n<p>Because, the Parkinson's law already defines that we don't finish the task earlier, the work tends to expand,  once a project goes late, there is rare change of early completion.</p>\n<p>An activity is considered independent if its duration do not influence the duration of another activity. Independent activities might balance the overall duration.</p>\n<p>Planning fails because activities are not independent. Most activities are not independent. Therefore, most of the time when one activity delays most of the other activity will delay as well. We should learn with the first activity that we might take a similar time and that we will not balance out.</p>\n<p>Multitasking delays the delivery. Multitasking generates waste and reduces. Two examples that show multitasking. Is possible to demonstrate the delay by having 3 activities each during 10 days, focusing one at each time, you deliver each at 10 days, focusing 5 days at each, you delay 10 days the first delivery ( demonstrate the delay ). Counting from A-Z and 1 to 24 . First the letters and after the numbers, or counting one of each ( demonstrate the cognitive waste ).</p>\n<p>Planning fails because we focus on activities and not on the most priority feature. Because, the plans assume most activities will be finished by the end, you do not start working on the most important priority activities. When the schedule delays, and you have to drop features, you end up having to drop features more important than the features delivered.</p>\n<p>Planning fails because we ignore uncertainty. We believe that the initial work we specified will be the complete and perfect amount of work to be done. Plans that ignore uncertainty and provide a single date are most likely going to fail. As a project progresses, the estimate can be refined and a more precise date can be achieved. We reduce uncertainty, by working in short iterations and delivering working software at the end of it.</p>\n<p>Planning fails because we consider estimates as commitment. Each estimate comes with a probability of success. An estimate is a probability, not a commitment. Commitment are made to dates, usually a date where the team has a less than 100% probability of delivering the software.</p>\n<h2>Chapter 3 - An Agile Approach</h2>\n<blockquote>\n<p>[The agile manifest] Their document both gave a name to how they were developing software and provided a list of value statement.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Individuals and interactions over processes and tools</li>\n<li>Working software over comprehensive documentation</li>\n<li>Customer collaboration over contract negotiation</li>\n<li>Responding to change over to following a plan</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>... a well functioning team of great individuals with mediocre tools will always outperform a dysfunctional team of mediocre individuals with great tools and process.</p>\n</blockquote>\n<blockquote>\n<p>Agile processes acknowledge the unique strengths ( and weakness) of individuals and capitalize on these rather than attempting to make everyone homogeneous.</p>\n</blockquote>\n<blockquote>\n<p>[Working software] ... it leads them to have a stable, incrementally enhanced version of the product at the end of each iteration.</p>\n</blockquote>\n<blockquote>\n<p> ... collect early and frequent feedback on both the product and the process.</p>\n</blockquote>\n<blockquote>\n<p>... we'd like software teams and customers to approach projects with this same attitude of collaboration and shared goals.</p>\n</blockquote>\n<blockquote>\n<p>Agile teams value responding to change over following a plan because their ultimate focus is on delivering as much value as possible to the project's customer and users.</p>\n</blockquote>\n<blockquote>\n<p>it's impossible for users to know every detail of every feature they want.</p>\n</blockquote>\n<h3>An Agile Approach to Projects</h3>\n<blockquote>\n<p>Taken collectively, the four value statements lead to software development processes that are highly iterative and incremental and that deliver coded and tested software at the end of each iteration.</p>\n</blockquote>\n<h3>An Agile Team Works As One</h3>\n<blockquote>\n<p>Critical to the success of a project is that all project participants view themselves as one team aimed at a common goal. There is no room for a \"throw it over the wall\" mentality on an agile project.</p>\n</blockquote>\n<blockquote>\n<p>The primary duties of the product owner include making sure that all team members are pursing a common vision for the project, establishing priorities so that the highest-valued functionality is always being worked on, and making decision that lead to a good return on the investment in the project.</p>\n</blockquote>\n<blockquote>\n<p>The second role is the customer. The customer is the person who has made the decision to fund the project or to buy the software.</p>\n</blockquote>\n<blockquote>\n<p>I use developer very generally to refer to anyone developing software. That includes programmers, testers, analysts, database engineers, usability experts, technical writers, architects, designers and so on.</p>\n</blockquote>\n<blockquote>\n<p>Agile project management focus more on leadership than on management.</p>\n</blockquote>\n<h3>An Agile Team Works in Short Iterations</h3>\n<blockquote>\n<p>Once the project has begun in earnest, all work ( analysis, design, coding, testing and so on) happens concurrently within each iteration.</p>\n</blockquote>\n<blockquote>\n<p>Iterations are timeboxed, meaning they finish on time even if functionality is dropped. Timeboxes are often very short.</p>\n</blockquote>\n<h3>An Agile Team Delivers Something Each Iteration</h3>\n<blockquote>\n<p>... during the iteration they transform one or more imprecise requirements statements into coded, tested, and potentially shippable software.</p>\n</blockquote>\n<blockquote>\n<p>... teams make progress by adding one or more small features in each iteration but that each added feature is coded, tested, and of releasable quality.</p>\n</blockquote>\n<blockquote>\n<p>Because a single iteration does not usually provide sufficient time to complete enough new functionality to satisfy user or customer desires, the broader concept of a release is introduced.</p>\n</blockquote>\n<blockquote>\n<p>Releases may occur on varying intervals.</p>\n</blockquote>\n<h3>An Agile Team Focuses on Business Priorities</h3>\n<blockquote>\n<p>First, they deliver features in the order specified by the product owner, who is expected to prioritize an combine features into a release that optimizes the return on the organization's investment in the project.</p>\n</blockquote>\n<blockquote>\n<p>For the product owner to have the most flexibility in prioritizing, features must be written so as to minimize the technical dependencies among then.</p>\n</blockquote>\n<blockquote>\n<p>Second, agile teams focus on completing and delivering user-valued features rather than on completing isolated tasks ( that eventually combine into a user-valued feature).</p>\n</blockquote>\n<blockquote>\n<p>A user story is a brief description of functionality as viewed by a user or customer of the system.</p>\n</blockquote>\n<blockquote>\n<p>User stories are free-form, and there is no mandatory syntax.</p>\n</blockquote>\n<blockquote>\n<p>As a <user>, I want <capability>, so that <business value> </p>\n</blockquote>\n<blockquote>\n<p>The story card is just the beginning, though, and each user story is accompanied by as many conversations between the developers and the product owner as needed.</p>\n</blockquote>\n<blockquote>\n<p>the focus is shifted drastically from written to verbal communication.</p>\n</blockquote>\n<h3>An Agile Team Inspects and Adapts</h3>\n<blockquote>\n<p>The plan created at the start of any project is not a guarantee of what will occur. </p>\n</blockquote>\n<blockquote>\n<p>Agile teams view every such change as presenting both the opportunity and need to update the plan to better reflect the reality of the current situation.</p>\n</blockquote>\n<blockquote>\n<p>At the start of each new iteration, an agile team incorporates all new knowledge gained in the preceding iteration and adapts accordingly.</p>\n</blockquote>\n<blockquote>\n<p>Perhaps, based on feedback from seeing the software from an earlier iteration, the product owner has learned that user would like to see more of one type of feature and that they don't value another feature as much as was previously thought.</p>\n</blockquote>\n<blockquote>\n<p>Priorities do tend to be relatively stable from one iteration to the next. However, the opportunity to alter priorities between iterations is a powerful contributor to the ability to maximize the return on the project investment.</p>\n</blockquote>\n<h3>An Agile Approach to Planning</h3>\n<blockquote>\n<p>... we should not view a project solely as the execution of a series of steps. Instead, it is important that we view a project as rapidly and reliably generating a flow of useful new capabilities and new knowledge.</p>\n</blockquote>\n<blockquote>\n<p>The new capabilities are delivered in the product; the new knowledge is used to make the product the best that it can be.</p>\n</blockquote>\n<blockquote>\n<p>New product knowledge helps us know more about what the product should be. New project knowledge is information about the team, the technologies in use, the risks, and so on.</p>\n</blockquote>\n<blockquote>\n<p>On an agile project, we don't know exactly where the finish line is, but we often know we need to get to it or as close as we can by a know date.</p>\n</blockquote>\n<blockquote>\n<p>An agile project is more like a timed race than a 10-kilometer race: run as far as possible in sixty minutes. In this way, the agile project team knows when they will finish but not what they will deliver.</p>\n</blockquote>\n<blockquote>\n<p>When we acknowledge that the result is both somewhat unknown as well as unknowable in advance, planning becomes a process of setting and revising goals that leads to a longer-term objective.</p>\n</blockquote>\n<h3>Multiple Levels of Planning</h3>\n<blockquote>\n<p>When setting and revising goals, it is important to remember that we cannot see past the horizon and that the accuracy of a plan decreases rapidly the further we attempt to plan beyond where we can see.</p>\n</blockquote>\n<blockquote>\n<p>Agile teams achieve this by planning at three distinct horizons. The three horizons are the release, the iteration and the current day.</p>\n</blockquote>\n<blockquote>\n<p>The goal of release planning is to determine an appropriate answer to the questions of scope, schedule, and resources for a project.</p>\n</blockquote>\n<blockquote>\n<p>Release planning occurs at the start of a project but is not an isolated effort. A good release plan is updated throughout the project (usually at the start of each iteration) so that it always reflects the current expectations about what will be included in the release.</p>\n</blockquote>\n<blockquote>\n<p>Based on the work accomplished in the just-finished iteration, the product owner identifies high-priority work the team should address in the new iteration.</p>\n</blockquote>\n<blockquote>\n<p>... components of the iteration plan can be smaller.</p>\n</blockquote>\n<blockquote>\n<p>During iteration planning we talk about tasks that will transform a feature request into a working and tested software.</p>\n</blockquote>\n<blockquote>\n<p>Most agile teams use some form of daily stand-up meeting to coordinate work and synchronize daily efforts.</p>\n</blockquote>\n<blockquote>\n<p>Product planning involves a product owner's looking further ahead than the immediate release and planning for the evolution of the released product or system.</p>\n</blockquote>\n<h3>Conditions of Satisfaction</h3>\n<blockquote>\n<p>Every project is initiated with a set of objectives.</p>\n</blockquote>\n<blockquote>\n<p>These objectives can be thought of as the customer or product owner's conditions of satisfaction - that is, the criteria that will be used to gauge the success of the project.</p>\n</blockquote>\n<blockquote>\n<p>At the start of release planning, the team and product owner collaboratively explore the product owner's condition of satisfaction. These include the usual items - scope, schedule, budget, and quality - although agile teams typically prefer to treat quality as non-negotiable.</p>\n</blockquote>\n<blockquote>\n<p>When no feasible solution can be found, the conditions of satisfaction must change.</p>\n</blockquote>\n<blockquote>\n<p>Once a release plan covering approximately the next three to six months are established, it is used as input into the planning of the first iteration.</p>\n</blockquote>\n<blockquote>\n<p>For an iteration, the product owner conditions of satisfaction are typically the features she'd like developed next and some high level test about each features.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Agile focuses on individuals more than processes. A great team with mediocre tools will outperform a mediocre team with great tools.</p>\n<p>Agile focus on working software more than comprehensive documentation. This value regards to increase the functionalities incrementally at the end of each iteration, enabling to get feedback earlier and adding this feedback to the next planning.</p>\n<p>Agile focus on customer collaboration over contract negotiation. Everyone working on a project must have the same shared goal, and work together to achieve it.</p>\n<p>Agile focus on responding to change over following a plan. At the end of each iteration we add the knowledge we acquired to the next planning.</p>\n<p>A very good description:</p>\n<blockquote>\n<p>Taken collectively, the four value statements lead to software development processes that are highly iterative and incremental and that deliver coded and tested software at the end of each iteration.</p>\n</blockquote>\n<p>Although a team has several roles, there is no space for \"throw over the wall\" mentality. Everyone that is part of the process of developing software participates together creating a \"we are all in this together mindset\".</p>\n<p>The PO ensures the team's prioritizes the most high value features and making decision that lead to a good ROI. The customer is the one that funded or bought the software. Third we have developers (any one that participates in process of developing software). At last, agile project manager executes a leadership role.</p>\n<p>Teams works in iterations that are timeboxed. All activities occur concurrently within an iteration.</p>\n<p>During the iteration the team transforms specification into a developed and tested functionality. This piece of functionality might be shippable. Because we might not want to ship every sprint, we need the a new concept, the concept of a release. Releases frequency vary.</p>\n<p>The team always focus on the highest value feature. The team must try to have features as independent technically as possible. Therefore, the PO will have more flexibility.</p>\n<p>The teams focus on features, not activities. The features are usually described as a user story describes the user, the problem, and the value. A user story, although lightweight, goes back and forward between the PO and Dev team until it reaches a development maturity.</p>\n<p>Agile teams embraces changes on plans. Most of the times the iterations are stable, and teams have the in between iterations to change the priority of the next iteration based on what you learned in the current iteration. </p>\n<p>A project is not a execution of steps. A project is the generation of capabilities and of knowledge. At each iteration the team acquires knowledge about itself, technology, risk the product. There are two types of knowledges acquired at each iteration, technical and business knowledge. The technical knowledge regards to reassessing after the implementation time the difficulties with the technology, usually you also learn about the risks and reduce uncertainties. Business knowledge regards to what we learned about the product in the last iteration.</p>\n<p>In agile project, you usually don't know where the finish line is. Is more like a race against time, that you know that you'll finish, but you don't know what you will deliver. Therefore, because we understand the finish line is unknown and unknowable, planning become an exercise of revisiting goals and the next steps.</p>\n<p>The accuracy of plans reduce when looking to much further in the future. Agile teams plan releases, iterations and daily. </p>\n<p>The release planning answers the scope, schedule and resources necessary to deliver. The release plan starts at the beginning of a project, and is updated after each sprint.</p>\n<p>Iteration planning answers the question of what is the next highest valuable item the team have to work. Iteration items are small components that will be transformed from specification to developed and tested feature.</p>\n<p>In the daily planning, the team synchronizes and plan the tasks until the next day always looking to complete the most important tasks in the iteration.</p>\n<p>There are other levels of planning, for example, product planning which is more about discovery.</p>\n<p>Every project has a set of objectives. We use these objectives to measure if the project was successful or not. At the release planning the team discusses the project's condition of satisfaction (cost, schedule, scope). Agile teams prefer to not negotiate quality.</p>\n<p>If the team does not finds an agreement the satisfaction condition must be changed. Once the release planning is established we used that information to fulfill the iteration planning.</p>\n<h1>Part II - Estimating Size</h1>\n<blockquote>\n<p>agile teams separates estimation of size from estimates of duration.</p>\n</blockquote>\n<h2>Chapter 4 - Estimating Size with Story Points</h2>\n<blockquote>\n<p>[Restaurants] In cases such as these, you are ordering by relative rather than measured size.</p>\n</blockquote>\n<blockquote>\n<p>All I need to know is whether a particular story or feature is larger or smaller than other stories and features.</p>\n</blockquote>\n<h3>Story Points are relative</h3>\n<blockquote>\n<p>Story Points are a unit of measure for expressing the overall size of a user story.</p>\n</blockquote>\n<blockquote>\n<p>The raw values we assign are unimportant. What matters are the relative values.</p>\n</blockquote>\n<blockquote>\n<p>[SP] ... is an amalgamation of the amount of effort involved in developing the feature, the complexity of developing it, the risk inherent in it, and so on.</p>\n</blockquote>\n<blockquote>\n<p>Once you've fairly arbitrarily assigned a story-point value to the first story, each additional story is estimated by comparing it with the first story or with any others that have been estimated.</p>\n</blockquote>\n<blockquote>\n<p>On an agile project it is not uncommon to begin an iteration with incompletely specified requirements, the details of which will b e discovered during the iteration.</p>\n</blockquote>\n<blockquote>\n<p>When you're given a loosely defined user story (or dog), you make some assumptions, take a guess, and move on.</p>\n</blockquote>\n<h3>Velocity</h3>\n<blockquote>\n<p>Velocity is a measure of a team's rate of progress.</p>\n</blockquote>\n<blockquote>\n<p>It's calculated by summing the number of story points assigned to each user story that the team completed during the iteration.</p>\n</blockquote>\n<blockquote>\n<p>Desired Features -> Estimate size -> estimate duration -> Schedule.</p>\n</blockquote>\n<blockquote>\n<p>If we sum the story-points estimates for all desired features we come up with a total size estimate for the project. If we know team's velocity we can divide size by velocity to arrive at an estimated number of iterations.</p>\n</blockquote>\n<blockquote>\n<p>A key tenet of agile estimating and planning is that we estimate size but derive duration.</p>\n</blockquote>\n<h3>Velocity Corrects Estimation Errors</h3>\n<blockquote>\n<p>[About lower velocity than expected] Without re-estimating any work they will have correctly identified that the project ill take ten iterations rather than eight.</p>\n</blockquote>\n<blockquote>\n<p>2In fact, your estimates remain useful because they are estimates of the relative effort.</p>\n</blockquote>\n<blockquote>\n<p>The beauty of this is that estimating in story points completely separates the <em>estimation of effort</em> from the <em>estimation of duration</em>.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Story Points are a relative unit of measure. It represents the effort, complexity, risks and so on of a user stories. </p>\n<p>We say it's a relative unit of measure because it's always relative to other story points. An User Story with 4 story points is twice big than an story points with 2.</p>\n<p>The Story points are a good match for agile projects, where you might not have all the requirements beforehand.</p>\n<p>We use the velocity to understand the team's rate of progress. We calculate it by summing all SP completed from an iteration. You can also create an average from several iterations.</p>\n<p>To come up with a schedule, we list all the desired features, we estimate the effort and derive the duration, therefore we can come up with an expected range of date.</p>\n<p>Even though we fail to estimate the duration of a development ( velocity is lower than expected ). The estimate holds true, because it's relative.</p>\n<h2>Chapter 5 - Estimating in Ideal Days</h2>\n<blockquote>\n<p>Ideal time is the amount of time that something takes when stripped of all peripheral activities.</p>\n</blockquote>\n<blockquote>\n<p>Elapsed time is the amount of time that passes on a clock ( or perhaps a calendar).</p>\n</blockquote>\n<blockquote>\n<p>It's almost always far easier and more accurate to predict the duration of an event in ideal time than in elapsed time.</p>\n</blockquote>\n<blockquote>\n<p>[Football analogy] To come up with an estimate that will be as accurate as your off-the-cuff ideal time estimates, you would need to consider each of these factors [amount of faults, injuries, weather, team's strategy].</p>\n</blockquote>\n<blockquote>\n<p>Finally, after decades of televising football games, the network have accustomed us not to expect a precise finish time.</p>\n</blockquote>\n<h3>Ideal Time and Software Development</h3>\n<blockquote>\n<p>Why ideal time does not equal elapsed time are:</p>\n<ul>\n<li>Supporting the current release</li>\n<li>Sick time</li>\n<li>Meetings</li>\n<li>Demonstrations</li>\n<li>Personnel Issues</li>\n<li>Phone calls</li>\n<li>Special projects</li>\n<li>Training</li>\n<li>Email</li>\n<li>Reviews and walk-through</li>\n<li>Interviewing candidates</li>\n<li>Task switching</li>\n<li>Bug fixing in current releases</li>\n<li>Management reviews</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Problems can arise when a manager asks a team member the inevitable question: \"How long will this take?\" The team member responds, \"Five days\", so the manager counts off five days on her calendar and marks the day with a big red X. The team member, however, really meant to say, \"Five days if that's all I do, but I do a lot of other things, so probably two weeks.\".</p>\n</blockquote>\n<blockquote>\n<p>A software developer who is told to multitask loses a great deal of efficiency while switching between two (or more) tasks.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>The story being estimated is the only thing you'll work on.</li>\n<li>Everything you need will be on hand when you start.</li>\n<li>There will be no interruptions.</li>\n</ul>\n</blockquote>\n<h3>Ideal Days as a Measure of Size</h3>\n<blockquote>\n<p>When we estimate the number of ideal days that a user story will take to develop, test, and accept, it is not necessary to consider the impact of the overhead of the environment</p>\n</blockquote>\n<blockquote>\n<p>The amount of time that elapses on a clock (or calendar) will differ of course.</p>\n</blockquote>\n<blockquote>\n<p>Then an estimate of size expressed as a number of ideal days can be converted into an estimate of duration using velocity in exactly the same way as with story points.</p>\n</blockquote>\n<h3>One Estimate, Not Many</h3>\n<blockquote>\n<p>If you choose to estimate in ideal days, assign one aggregate estimate to each user story.</p>\n</blockquote>\n<blockquote>\n<p>This level of focus on individual roles on a team shifts team thinking away from the \"we're all in this together\" mentality we'd like to exist on an agile team.</p>\n</blockquote>\n<blockquote>\n<p>Further, it vastly increases the amount of work necessary to plan a release.</p>\n</blockquote>\n<blockquote>\n<p>A team in this situation [developers for specific platforms] may want to estimate the ideal time for each role on each story. They should, however, be aware of the extra administrative burden this will require.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Ideal time is different from elapsed time. We often give ideal time estimates and these are used as elapsed time. Ideal time is the 90 minutes from a US football game, elapsed time is the time on the watch or on the calendar varying depending on several aspects of the game.</p>\n<p>The daily routine impacts the ideal time. Slack messages, support, e-mails, trainings, and so on...</p>\n<p>Multitask is a second factor that impacts the ideal time.</p>\n<p>When we estimate with ideal time. We consider we'll be working only in the story being estimated. That everything we'll need to begin will be on hand. and there'll be no interruptions.</p>\n<p>When we estimate with ideal time, we don't consider the company's bureaucracy. Therefore, ideal time should be the same on a start up or enterprise company. The elapsed time may change.</p>\n<p>We should give one estimate for the whole team, to avoid not follow the \"we're all in this together\" mentality.</p>\n<h2>Chapter 6 - Techniques for Estimating</h2>\n<blockquote>\n<p>We want to remain aware, too, of the diminishing return on time spent estimating. We can often spend a little time thinking about an estimate and come up with  a number that is nearly as good as if we had spent a lot of time thinking about it.</p>\n</blockquote>\n<blockquote>\n<p>First, no matter how much effort is invested, the estimate is never at the top of the accuracy axis.</p>\n</blockquote>\n<blockquote>\n<p>Little effort is required to move the accuracy up dramatically from the baseline.</p>\n</blockquote>\n<blockquote>\n<p>Finally, notice that eventually, the accuracy of the estimate declines.</p>\n</blockquote>\n<blockquote>\n<p>When starting starting to plan a project, it is useful to think about where on the curve we wish to be. Many projects try to be very high up the accuracy axis, forcing teams far out on the effort axis even though the benefits diminish rapidly. Often, this is the result of the simplistic view that we can lock down budgets, schedules and scopes and that projects success equates to on-time, on-budget delivery of an up-front, precisely planned set of features. This type of thinking leads to a desire for extensive signed requirements documents, lots of up-front analysis work, and detailed project plans that show every task a team can think. Then, even after all this additional up-front work, the estimates still aren't perfect.</p>\n</blockquote>\n<blockquote>\n<p>Agile teams, however, choose to be closer to the left (in the curve). They acknowledge that we cannot eliminate uncertainty from estimates, but they embrace the idea that small efforts are regarded with big gains.</p>\n</blockquote>\n<h3>Estimates Are Shared</h3>\n<blockquote>\n<p>Estimates are not created by a single individual on the team. Agile teams do not rely on a single expert to estimate. Despite well-known evidence that estimates prepared by those who will do the work are better than estimates prepared by anyone else (Lederer and Prasad 1992).</p>\n</blockquote>\n<blockquote>\n<p>On an agile project we tend not to know specifically who will perform a given task.</p>\n</blockquote>\n<blockquote>\n<p>others may have something to say about her estimate. (Example: Considering previous experience someone had )</p>\n</blockquote>\n<h3>The Estimation Scale</h3>\n<blockquote>\n<p>we are best at estimating things that fall within one order of magnitude.</p>\n</blockquote>\n<blockquote>\n<p>Two estimation scales I've had good success with are</p>\n<p>1,2,3,5 and 8</p>\n<p>1,2,4 and 8</p>\n</blockquote>\n<blockquote>\n<p>These nonlinear sequences work well because they reflect the greater uncertainty associated with estimates for larger units of work.</p>\n</blockquote>\n<blockquote>\n<p>Each of these numbers should be thought of as a bucket into which items of the appropriate size are poured. Rather than thinking of work as water being poured into the buckets, think of the work as sand.</p>\n</blockquote>\n<blockquote>\n<p>Assigning a nonzero values to tiny features will limit the size of largest features.</p>\n</blockquote>\n<blockquote>\n<p>Secondly, if the work truly is closer to 0 than 1, the team may not want the completion of the feature to contribute to its velocity calculations.</p>\n</blockquote>\n<blockquote>\n<p>I've never had the slightest problem explaining this to product owners, who realize that a 0-point story is the equivalent of a free lunch. However, they also realize there's a limit to the number of free lunches they can get in a single iteration.</p>\n</blockquote>\n<h3>User Stories, Epics and Themes</h3>\n<blockquote>\n<p>For features that we're not sure we want ( a preliminary cost estimate is desired before too much investment is put int them) or for feature that may not happen in the near future, it is often desirable to write one much larger user story. A large user stories is sometimes called an <em>epic</em>.</p>\n</blockquote>\n<blockquote>\n<p>Additionally, a set of related user stories may be combined ( usually by a paper clip if working with note cards) and treated as a single entity for either estimating or release planning. Such a set of user stories is referred to as a theme.</p>\n</blockquote>\n<blockquote>\n<p>However, it's important that  they realize that estimates of themes and epics will be more uncertain than estimates of the more specific, smaller user stories.</p>\n</blockquote>\n<h3>Deriving an Estimate</h3>\n<blockquote>\n<ul>\n<li>Expert opinion</li>\n<li>Analogy</li>\n<li>Disaggregation</li>\n</ul>\n</blockquote>\n<h3>Expert Opinion</h3>\n<blockquote>\n<p>The expert relies on her intuition or gut feel and provides an estimate.</p>\n</blockquote>\n<blockquote>\n<p>This approach is less useful on agile projects than on traditional projects. On an agile project, estimates are assigned to user stories or other user-valued functionality.</p>\n</blockquote>\n<blockquote>\n<p>A nice benefit of estimating by expert opinion is that it usually doesn't take very long.</p>\n</blockquote>\n<h3>Analogy</h3>\n<blockquote>\n<p>If the story is twice the size, it is given an estimate twice as large.</p>\n</blockquote>\n<blockquote>\n<p>Instead, you want to estimate each new story against an assortment of those that have already been estimated.</p>\n</blockquote>\n<h3>Disaggregation</h3>\n<blockquote>\n<p>Disaggregation refers to splitting a story or feature into smaller, easier-to-estimate pieces.</p>\n</blockquote>\n<blockquote>\n<p>However, you need to be careful not to go too far with this approach.</p>\n</blockquote>\n<h3>Planning Poker</h3>\n<blockquote>\n<p>Planning poker combines expert opinion, analogy and disaggregation into an enjoyable approach to estimating that results in quick but reliable estimates.</p>\n</blockquote>\n<h3>The Right Amount of Discussion</h3>\n<blockquote>\n<p>Buy a two-minute sand timer, and place it in the middle of the table where planning poker is being played. Anyone in the meeting can turn the timer over at any time.</p>\n</blockquote>\n<h3>Smaller Sessions</h3>\n<blockquote>\n<p>This isn't ideal but may be a reasonable option, especially if there are many items to be estimated, as can happen at the start of a new project.</p>\n</blockquote>\n<h3>When to Play Planning Poker</h3>\n<blockquote>\n<p>First, there will usually be an effort to estimate a large number of items before the project officially begins or during its first iterations.</p>\n</blockquote>\n<blockquote>\n<p>Second, teams will need to put forth some ongoing effort to estimate new stories that are identified during an iteration.</p>\n</blockquote>\n<blockquote>\n<p>Kent Beck suggests hanging an envelope on the wall with all new stories placed in the envelope.</p>\n</blockquote>\n<h3>Why Planning Poker Works</h3>\n<blockquote>\n<p>First, planning poker brings together multiple expert opinions to do the estimating. </p>\n</blockquote>\n<blockquote>\n<p>Second, a lively dialogue ensues during planning poker, and estimators are called upon by their peers to justify their estimates.</p>\n</blockquote>\n<blockquote>\n<p>Third, studies have shown that averaging individual estimates leads to better results ( Hoest and Wohlin 1998)</p>\n</blockquote>\n<h3>My summary</h3>\n<p>Considering two axis X that represents effort and Y that represents estimate accuracy. In the start of the axis X with little effort you can get a lot of accuracy. As we progress in the axis X we reduce the amount of accuracy we get, until eventually we start to get an decline in the accuracy. Therefore, we can never have an estimate that is 100% accurate. </p>\n<p>With that in mind, is preferable to invest effort until you reach a good enough estimate. And avoid wasting effort to get an worse result. Agile teams are usually in the beginning of the axis X.</p>\n<p>Even though there are evidence that experts have a better estimate, in agile teams the whole team participate in the estimation. That's because we don't know who will do the task and because team members can also help the expert with information from similar tasks from the past.</p>\n<p>When estimating we want to keep things in the same order of magnitude. Avoid comparing the distance going to the shop and the distance going to the moon.</p>\n<p>The most common sequences are Fibonacci and 0,1,2,4,8. These sequences are good because the gap between the number increases exponentially which is a good representation for the uncertainty. The bigger the story more uncertain it is.</p>\n<p>Assigning zero to your scale reduce the largest value of the scale, which is good. It's also good to document the tasks that we have to do that individually the effort is so small it won't impact the velocity. Just be careful because there's a limit of free lunches. Many 0 user stories can be grouped in a unique user story with a bigger size.</p>\n<p>Consider the values in the scale as a sand, you can put a little more sand than the bucket max size.</p>\n<p>There are three ways to get an estimate. Through an expert, doing analogies or disaggregating user stories.</p>\n<p>An estimate based on the expert relies on its experience and gut feel. It's less valuable in agile projects that we don't assign the tasks for each person. It's fast to do.</p>\n<p>An estimate based on analogy to reach an estimate you compare with a set of other user stories. If this user story is twice asH big as that one, then you have your estimate.</p>\n<p>An estimate based on disaggregation you start with a big story and start to break it into smaller ones. Be careful to not break to much, otherwise you might end up with a broader estimate.</p>\n<p>Planning poker is the combines the three estimation techniques. To limit the amount of discussion in a planning poker, you can put a sand timer in the table an anyone can start it at any time of a discussion.</p>\n<p>When to estimate, in beginning of a project you might have to expend a extra effort estimating user stories. Then, during each iteration of a the project, you need to estimate the new items that come up.</p>\n<p>Planning poker works because it uses the three techniques described above, it provides discussion about the topic and it is based on individuals average estimates.</p>\n<h2>Chapter 7 - Re-estimating</h2>\n<blockquote>\n<p>... velocity is the great equalizer.</p>\n</blockquote>\n<blockquote>\n<p>as long as we are consistent with our estimates, measuring velocity over the first few iterations will allow us to hone in on a reliable schedule.</p>\n</blockquote>\n<h3>My summary</h3>\n<p>If you break the consistency between the user story you estimate and the user stories you completed, the velocity will not reflect the reality.</p>\n<p>When you learn something about the estimates you should re-estimate the sprint velocity and the backlog user stories relative to it.</p>\n<p>Consider that you have the following user stories and its story points:</p>\n<ul>\n<li>Chart A - 3</li>\n<li>Chart B - 5</li>\n<li>Chart C - 3</li>\n<li>Text Report - 3</li>\n<li>Upload - 3</li>\n<li>Recommendation - 5</li>\n</ul>\n<p>In the first iteration the teams pulls Chart A, Chart B and Recommendation, adding to 11 Story points.</p>\n<p>At the end of the first iteration the team completed only Chart A and Recommendation, therefore, team's velocity is 8. Chart A took more effort than expected.</p>\n<p>The team has 3 scenarios:</p>\n<ol>\n<li>Team do not re-estimate: The next iteration the team pulls Chart B and Chart C. The next iteration will most likely fail.</li>\n<li>Team re-estimate the finished story: The team re-estimate Chart A to 6, twice as big as they firstly estimated. Teams velocity increases to 11. The next iteration the teams pulls 11 story points (Chart B, Chart C and Text Report). The next iteration will most likely fail.</li>\n<li>Re-estimating when relative size changes: The team re-estimate Chart A to 6 and all the user stories that are relative to it like Chart B and Chart C. Therefore, the Chart B changes its estimate to 10 and Chart C to 6. Team's velocity increases to 11. The next iteration the team pull only the Chart B.</li>\n</ol>\n<p>In the third scenario the team learned with the Chart A user story and applied the knowledge to the other user stories that are related to it.</p>\n<p><img src=\"../src/images/when-re-estimate.png\" alt=\"When re estimate\"></p>\n<p>Usually, if the team didn't complete the whole user story they do not add its size to the velocity calculation. An exception would occur if the team completes half of a user story, and during the next iteration they will not work on the other half. In this case, you could split the user stories in two, and add the story points to from the completed part to the velocity.</p>\n<h2>Chapter 8 - Choosing between Story Points and Ideal Days</h2>\n<h3>Story Points Help Drive Cross-functional Behavior</h3>\n<blockquote>\n<p>Story points estimate needs to be a single number that represents all the work for the whole team, estimating story points initiates high-level discussions about everything that will be involved.</p>\n</blockquote>\n<h3>My summary</h3>\n<p>There are 5 points in favor of Story points:</p>\n<ul>\n<li>Story-points help drive cross-functional behavior. There is only one estimate for the whole team. In opposition, in ideal time each role calculates the time they require.</li>\n<li>Story-points estimates do not decay. Because story points do not consider any time, the estimate remains the same even though the velocity might change. Ideal Points need to re-estimate if the team evolves technically.</li>\n<li>Story points are a pure measure of size. You can estimate by analogy only (which has evidence that is better). No temptation to compare then with the reality.</li>\n<li>Estimating in story points is typically faster. Estimates in story points are more high-level and therefore consume less time.</li>\n<li>My ideal days are not your ideal days. Even though you can agree on the effort required you might disagree with the time necessary to complete the task.</li>\n</ul>\n<p>There are 2 points in favor of ideal days:</p>\n<ul>\n<li>Ideal days are easier to explain outside the team. Everyone understands that not every minute of the work day is spent programming, testing, designing or otherwise making progress towards new features. Considering the story points, you have a longer path to explain to everyone, though it's an excellent opportunity to teach about the cone of uncertainty , progressive refinement and how velocity impacts the schedule.</li>\n<li>Ideal days are easier to estimate at first. Similarly, most people are used to the concept of estimating time to tasks. With Story points again you have a longer path.</li>\n</ul>\n<h1>Part III - Planning for Value</h1>\n<blockquote>\n<p>Before planning a project, we must consider what it is that our users need. Simply generating a list of things we think they want and then scheduling the development of those features is not enough. Achieving the best combination of product features(scope), schedule, and cost requires deliberate consideration of the cost and value of the user stories and themes that will comprise the release.</p>\n</blockquote>\n<h2>Chapter 9 - Prioritizing Themes</h2>\n<blockquote>\n<p>There is rarely, if ever, enough time to do everything. So we prioritize.</p>\n</blockquote>\n<h3>Factors in Prioritization</h3>\n<blockquote>\n<ol>\n<li>Financial value of having the features</li>\n<li>The cost of developing (and perhaps supporting) the new feature.</li>\n<li>The amount of significance of learning and new knowledge created by developing the features.</li>\n<li>The amount of risk removed by developing the features.</li>\n</ol>\n</blockquote>\n<h3>Cost</h3>\n<blockquote>\n<p>An important, yet often overlooked, aspect of cost is that the cost can change over time.</p>\n</blockquote>\n<blockquote>\n<p>The best way to reduce the cost of change is to implement a feature as late as possible - effectively when there is no more time for change.</p>\n</blockquote>\n<h3>New Knowledge</h3>\n<blockquote>\n<p>On many projects, much of the overall effort is spent in the pursuit of new knowledge.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Knowledge about the product</li>\n<li>Knowledge about the project</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>The flip side of acquiring knowledge is reducing uncertainty.</p>\n</blockquote>\n<blockquote>\n<p>End uncertainty is reduced by acquiring more knowledge about the product.</p>\n</blockquote>\n<blockquote>\n<p>Means uncertainty is reduced through acquiring more knowledge about the project.</p>\n</blockquote>\n<blockquote>\n<p>A project following a waterfall process tires to eliminate all the uncertainty about what is being built before tackling the uncertainty of how it will be built.</p>\n</blockquote>\n<blockquote>\n<p>...the complete upfront elimination of all end uncertainty is unachievable.</p>\n</blockquote>\n<blockquote>\n<p>...one of the greatest risks to most projects is the risk of building the wrong product. This risk can be dramatically reduced by developing early those features that will best allow us to get working software in front of or in the hands of the actual users.</p>\n</blockquote>\n<h3>Risk</h3>\n<blockquote>\n<p>is anything that has not yet happened but might and that would jeopardize or limit the success of the project.</p>\n</blockquote>\n<blockquote>\n<p>To prioritize work optimally, it is important to consider both risk and value.</p>\n</blockquote>\n<blockquote>\n<p>The high-value, high-risk features should be developed first. These features deliver the most value, and working on them eliminates significant risks.</p>\n</blockquote>\n<blockquote>\n<p>Next are the high-value, low-risk features. These features offer as much value as the first set, but they are less risky. Therefore, they can be done later in the schedule.</p>\n</blockquote>\n<blockquote>\n<p>Next are the low-value, low-risk features. These are sequenced third because they will have less impact on the total value of the product if they are dropped, and because they are low risk.</p>\n</blockquote>\n<blockquote>\n<p>features that deliver low value, but are high risk are best avoided.</p>\n</blockquote>\n<h3>My summary</h3>\n<p>Because we have limited money and time we need to prioritize.</p>\n<p>There are 4 things to consider:</p>\n<ol>\n<li><strong>Value</strong>. How much money the organization will make or save?</li>\n<li>\n<p><strong>Cost</strong>. How much money it will cost to develop (time vs team salary)? How much money it will cost to maintain? How much it will cost if will do it later </p>\n<p>Cost may change with time.</p>\n</li>\n<li>\n<p><strong>New knowledge</strong>. The overall effort of projects is in pursuit of new knowledge. There are 2 types:</p>\n<ol>\n<li>Knowledge about the product (What) or End uncertainty</li>\n<li>Knowledge about the project (How) or Means uncertainty</li>\n</ol>\n<p>Acquiring knowledge reduces uncertainty. It's not possible to reduce all the uncertainty. Waterfall projects try to remove all the What and How before they start the project. Agile projects reduces the risk as it goes. At the beginning of the project there's a higher effort to reduce the End uncertainty to be sure we are not building the wrong product.</p>\n</li>\n<li><strong>Risk</strong>. Anything that might happen during the project that might put it on jeopardize or limit the success of the project.</li>\n</ol>\n<p>To prioritize work optionally we must consider both value and risk.</p>\n<ol>\n<li>High value, high risk</li>\n<li>High value, low risk</li>\n<li>low value, low risk</li>\n</ol>\n<p>Avoid low value, high risk.</p>\n<p>You can start considering the value it will deliver and the cost it will have. Also consider the cost in the long term. Then start to move forward or backwards to see the difference between the outcomes.</p>\n<h2>Chapter 10 - Financial Prioritization</h2>\n<blockquote>\n<p><em>\"As a general rule of thumb when benefits are not quantified at all assume there aren't any.\" - Tom DeMarco and Timothy Lister</em></p>\n</blockquote>\n<blockquote>\n<p>At a minimum , whoever requested the theme should be able to quantify the reasons for developing it.</p>\n</blockquote>\n<blockquote>\n<p>(About the Theme-Return Worksheet) We cannot compare projects and make priorization decisions simply by summing the total row of a worksheet like Table 10.2 for each theme.</p>\n</blockquote>\n<h3>Sources of Return</h3>\n<blockquote>\n<p>The return on a project can come from a variety of sources. For convenience, we can categorize these as new revenue, incremental revenue, retained revenue and operational efficiencies.</p>\n</blockquote>\n<blockquote>\n<p>(Incremental revenue) incremental revenue from existing customers.</p>\n</blockquote>\n<blockquote>\n<p>Retained revenue refers to the revenue an organization will lose if the project or theme is not developed.</p>\n</blockquote>\n<blockquote>\n<p>No organization is ever as efficient as it could be. There's always some task that could be streamlined or eliminated.</p>\n</blockquote>\n<blockquote>\n<p>Often, the drive to improve operational efficiencies comes from anticipated growth. An inefficiency that may not be a problem today rapidly becomes a problem when the company becomes much larger.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Anything that takes a long time or that would take a long time if the company grew.</li>\n<li>Better integration or communication between departments</li>\n<li>Reduced employee turnover</li>\n<li>Shorter training time for new employees</li>\n<li>Any time-sensitive process</li>\n<li>Combining multiple processes</li>\n<li>Anything that improves accuracy and reduces rework</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>(unless we can find a way to deliver partial solution, which should always be a goal)</p>\n</blockquote>\n<h3>Financial Measures</h3>\n<blockquote>\n<p>The amount I have to invest today to have a known amount in the future is called the <em>present value</em>.</p>\n</blockquote>\n<blockquote>\n<p>The process of moving future amounts back into their present value is known as <em>discounting</em>.</p>\n</blockquote>\n<blockquote>\n<p>The rate at which organization discount future money is known as their <em>opportunity cost</em> and reflects the percentage return that is passed up to make this investment.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>You should at minimum be able to quantify the benefits of developing something, otherwise you cannot prioritize based on facts.</p>\n<p>You can held a meeting with your team members for each theme to categorize the types of return you are getting from a theme. There are several types of return, but the book presents four:</p>\n<ul>\n<li>New revenue: Revenue from new customers.</li>\n<li>Incremental revenue: Revenue from existing customers.</li>\n<li>Retained revenue: Revenue that is not lost because of the project.</li>\n<li>\n<p>Operational efficiency: Revenue from improving the system or the process. For example:</p>\n<ul>\n<li>Long, manual, tedious and error prone tasks that don't scale</li>\n<li>Improve of accuracy and reduces rework</li>\n<li>...</li>\n</ul>\n</li>\n</ul>\n<p>There are financial measures that can be used to prioritize the theme:</p>\n<ul>\n<li>Net Present Value </li>\n<li>Internal Rate of Return ( IRR or ROI )</li>\n<li>Payback period</li>\n<li>Discounted Payback period</li>\n</ul>\n<h2>Chapter 11 - Prioritizing Desirability</h2>\n<h3>Kano Model of Customer Satisfaction</h3>\n<blockquote>\n<p>The process of doing so was originated by Noriaki Kano, whose approach gives us a way to separate feature into three categories:</p>\n<ul>\n<li>Threshold, or must-have, features</li>\n<li>Linear features</li>\n<li>Exciters and delighter</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>In fact, exciters ad delighters are often called unknown needs because customers or users do not know they need these features until they see them.</p>\n</blockquote>\n<blockquote>\n<p>... once some amount of a must-have feature has been implemented, customer satisfaction cannot be increased by adding more of that feature. Also, no matter how much of a must-have feature is added, customer satisfaction never rises above the midpoint.</p>\n</blockquote>\n<blockquote>\n<p>customer satisfaction rises drastically based on even a partial implementation of an exciter or delighter feature.</p>\n</blockquote>\n<blockquote>\n<p>direct relationship between the inclusion of linear features and customer satisfaction.</p>\n</blockquote>\n<blockquote>\n<p>emphasis should be placed on prioritizing the development of all threshold features.</p>\n</blockquote>\n<blockquote>\n<p>A product's must-have features do not need to be developed in the first iterations of a release. However, because users consider these features to be mandatory, they need to be available before the product is released.</p>\n</blockquote>\n<blockquote>\n<p>Secondary emphasis should be placed on completing as many linear features as possible. Because, each of these features leads directly to greater customer satisfaction, the more of these features that can be included the better (excluding, of course, such situations as a product that is already bloated with too man features).</p>\n</blockquote>\n<blockquote>\n<p>Keep in mind that features tend to migrate down the Kano diagram over time.</p>\n</blockquote>\n<blockquote>\n<p>determining the category of a feature by asking two questions: one question regarding how the user would feel if the feature were present in the product and one question about how the user would feel if it were absent.</p>\n</blockquote>\n<blockquote>\n<p>(about features with high values for two responses) This indicates that different types of customers and users have a different expectations.</p>\n</blockquote>\n<h3>Relative Weighting: Another Approach</h3>\n<blockquote>\n<p>Rather than use questionnaires, this approach relies on expert judgment.</p>\n</blockquote>\n<blockquote>\n<p>estimates of benefits and penalties are relative.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Proposes two methods for prioritizing considering user's desiribility:</p>\n<ul>\n<li>\n<p>Kano Model</p>\n<ul>\n<li>Three categories</li>\n<li>Must-have: Its absence causes low customer satisfaction. Its presence guarantees at maximum a neutral customer. </li>\n<li>Linear features: Its absence causes low customer satisfaction. Its presence guarantees customer satisfaction. The amount of features is linear to the customer satisfaction.</li>\n<li>Exciters / delighter: Its absence do not causes low customer satisfaction. Its presence guarantees customer satisfaction. This kind of feature has a exponential impact in the customer satisfaction.</li>\n<li>Priorization must follow must-have, then linear features and at last exciters / delighter.</li>\n<li>Use a questionnary with users to access the Kano Model. You should ask two questions:</li>\n<li>How do you feel with the feature present?</li>\n<li>How do you feel with the feature absent?</li>\n<li>Use the following scale for the answers:</li>\n<li>I like it</li>\n<li>I expect it to be that way</li>\n<li>I am neutral</li>\n<li>I can live with it that way</li>\n<li>I dislike it that way</li>\n</ul>\n</li>\n<li>\n<p>Relative Weighting</p>\n<ul>\n<li>Relies on experts</li>\n<li>For each theme estimate benefits and penalties.</li>\n<li>Sum the Relative benefits and penalties to reach the feature Total Value.</li>\n<li>Divide each item by the total value to reach the % Value of each theme.</li>\n<li>Add an estimate for each theme. And sum the estimate.</li>\n<li>Divide the each estimate by the Sum of estimate to understand the % Cost of each theme.</li>\n<li>At last, divide the % Value by the % Cost and you'll reach to a Priority for each theme.</li>\n</ul>\n</li>\n</ul>\n<h2>Chapter 12 - Splitting User Stories</h2>\n<h3>When to Split a User Story</h3>\n<blockquote>\n<p>First, a user story should be split when it is too large to fit within a single iteration.</p>\n</blockquote>\n<blockquote>\n<p>Second, it can be useful to split a large user story (an epic) if a more accurate estimate is necessary.</p>\n</blockquote>\n<h3>Splitting across Data Boundaries</h3>\n<blockquote>\n<p>Split large stories along the boundaries of the data supported by the story.</p>\n</blockquote>\n<blockquote>\n<p>In some cases a large story can be made much smaller by removing the handling of exceptional or error conditions from the main story.</p>\n</blockquote>\n<h3>Splitting on Operational Boundaries</h3>\n<blockquote>\n<p>Split large stories based on the operations that are performed within the story.</p>\n</blockquote>\n<blockquote>\n<p>A common approach to doing this is to split a story along the boundaries of the common CRUD operations - Create, Read, Update and Delete.</p>\n</blockquote>\n<h3>Removing Cross-Cutting Concerns</h3>\n<blockquote>\n<p>Consider removing cross-cutting concerns (such as security, logging, error handling and so on) and creating two versions of the story one with an one without support for the cross cutting concern.</p>\n</blockquote>\n<h3>Don't Meet Performance Constraints</h3>\n<blockquote>\n<p>Consider splitting a large story by separating the functional and nonfunctional aspects into separate stories.</p>\n</blockquote>\n<h3>Split Stories of Mixed Priority</h3>\n<blockquote>\n<p>Split a large story into smaller stories if the smaller stories have different priorities.</p>\n</blockquote>\n<h3>Don't Split a Story into Tasks</h3>\n<blockquote>\n<p>Delivering a cohesive subset of all layers of a feature is almost always better than delivering all of one layer.</p>\n</blockquote>\n<blockquote>\n<p>Don't split a large story into tasks. Instead, try to find a way to fire a tracer bullet through the story.</p>\n</blockquote>\n<h3>Avoid the Temptation of Related Changes</h3>\n<blockquote>\n<p>Avoid making things worse by adding related changes to an appropriately sized feature unless the related changes are of equivalent priority.</p>\n</blockquote>\n<h3>Combining Stories</h3>\n<blockquote>\n<p>Just as we may need to split large stories, we may need to combine multiple tiny stories.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>You should split a story when won't fit in the iteration. Either because it's too big or because you won't be able to finish it. You should split it based on:</p>\n<ul>\n<li>The data</li>\n<li>The operations</li>\n<li>Cross-Cutting concerns (security, logging, error handling, ...)</li>\n<li>Non-functional requirements</li>\n<li>Different priorities within the story</li>\n</ul>\n<p>You shouldn't:</p>\n<ul>\n<li>Split a story into tasks. It's better to do the tracer bullet.</li>\n<li>Split and than add more work to the User story</li>\n</ul>\n<p>You may as well combine tiny user stories into a larger user story. This approach is commonly used for bugs.</p>\n<h2>Chapter 13 - Release Planning Essentials</h2>\n<blockquote>\n<p>Release planning is the process of creating a very high-level plan that covers a period longer than an iteration.</p>\n</blockquote>\n<blockquote>\n<p>The sooner the product can be released( and the better it is when it's released), the sooner the organization will begin earning a return on its investment in the project.</p>\n</blockquote>\n<blockquote>\n<p>Second, a release plan conveys expectations about what is likely to be developed and in what timeframe.</p>\n</blockquote>\n<blockquote>\n<p>Third, a release plan serves as a guidepost toward which the project team can progress.</p>\n</blockquote>\n<h3>The Release Plan</h3>\n<blockquote>\n<p>Multiplying the planned number of iterations by either the expected or known velocity of the team gives us the total amount of work  that can be performed.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Determine conditions of satisfaction</li>\n<li>Estimate the user stories</li>\n<li>\n<p>Do in any sequence</p>\n<ol>\n<li>Select an iteration length</li>\n<li>Estimate velocity</li>\n<li>Prioritize user stories</li>\n</ol>\n</li>\n<li>Select stories and a release date</li>\n</ol>\n</blockquote>\n<h3>Determine the Conditions of Satisfaction</h3>\n<blockquote>\n<p>it is important to know the criteria by which the project will be evaluated as a success or a failure</p>\n</blockquote>\n<blockquote>\n<p>Projects are typically either <em>date-driven</em> or <em>feature-driven</em>. A <em>date-driven</em> project is one that must be released by a certain date but for which the feature set is negotiable. A <em>feature-driven</em> project is one that we would probably like to release as son as possible but for which we consider the completion of a set of features to be more important.</p>\n</blockquote>\n<h3>Estimate the User Stories</h3>\n<blockquote>\n<p>an estimate represents the cost of developing a user story.</p>\n</blockquote>\n<h3>Prioritize User Stories</h3>\n<blockquote>\n<p>A good product owner will accept ultimate responsibility for prioritizing but will listen to advice from the development team, especially about the sequencing.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>A release plan is a high-level plan bigger than a iteration and serves as a guidance for a set of iterations. The release plan conveys the expectations for a timeframe. The earlier the release goes to production earlier you have return from it. </p>\n<p>A project must have a set of conditions that determine whether it will fail or succeed, usually there are three: schedule, scope, resources.</p>\n<p>You can have either date-driven or feature-driven projects:</p>\n<ul>\n<li>Date-driven: The date is fixed, but the scope is negotiable. Calculate the number of iterations until the end date. Multiply the velocity by the number of iterations to achieve the amount of effort you can deliver.</li>\n<li>Feature-driven: The scope is fixed, but the date is negotiable. Sum the amount of effort for each user story. Divide the total effort by the velocity, and you'll get the total number of iterations.</li>\n</ul>\n<p>Once you have the release plan, you must update it as the project proceeds. A release plan must not be static.</p>\n<h2>Chapter 14 - Iteration Planning</h2>\n<blockquote>\n<p>With an iteration plan, a team takes a more focused, detailed look at what will be necessary to implement completely only those user stories selected for the new iteration.</p>\n</blockquote>\n<blockquote>\n<p>( about the iteration planning meeting) Anyone involved in taking a raw idea an turning it into a functioning product should be present.</p>\n</blockquote>\n<blockquote>\n<p>One of the most significant advantages to using note cards during iteration planning is that it allows everyone to participate in the process.</p>\n</blockquote>\n<h3>Tasks Are Not Allocated During Iteration Planning</h3>\n<blockquote>\n<p>Individuals do not sign up for tasks until the iteration begins and generally sign up for only one or two related tasks at a time. New tasks are not begun until previously selected ones are completed.</p>\n</blockquote>\n<blockquote>\n<p>The primary purpose of iteration planning is to refine suppositions made in the ore coarse-grained release-plan.</p>\n</blockquote>\n<blockquote>\n<p>agile planning becomes a two-stage process. The first stage is the release plan, with its rough edges and general uncertainties. The second stage is the iteration plan. An iteration plan still has some rough edges and continues to be uncertain. However, because it is created concurrent with the start of a new iteration, an iteration plan is more detailed than a release plan.</p>\n</blockquote>\n<h3>Velocity-Driven Iteration Planning</h3>\n<blockquote>\n<ol>\n<li>\n<p>Do in any sequence</p>\n<ol>\n<li>Adjust priorities</li>\n<li>Determine target velocity</li>\n</ol>\n</li>\n<li>Identify an iteration goal</li>\n<li>Select user stories</li>\n<li>Split user stories into tasks</li>\n<li>Estimate tasks</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>The default assumption by most teams is that their velocity in the next iteration will equal the velocity of the most recent iteration. Beck and Fowler(2000) call this yesterday's weather.</p>\n</blockquote>\n<blockquote>\n<p>Other teams prefer to use a moving average over perhaps the last three iterations.</p>\n</blockquote>\n<blockquote>\n<p>The goal succinctly describes what they would like to accomplish during that period.</p>\n</blockquote>\n<blockquote>\n<p>Next, the product owner and team select stories that combine to meet the iteration goal.</p>\n</blockquote>\n<blockquote>\n<p>A common question around iteration planning is what should be included. All tasks necessary to go from a user story to a functioning, finished product should be identified.</p>\n</blockquote>\n<blockquote>\n<p>An agile teams has the goal of fixing all bugs in the iteration in which they are discovered.</p>\n</blockquote>\n<blockquote>\n<p>A spike is a task included in an iteration plan that is being undertaken specifically to gain knowledge or answer a question.</p>\n</blockquote>\n<blockquote>\n<p>The tasks you create should be of an approximate size so that each developer is able to finish an average of one per day.</p>\n</blockquote>\n<blockquote>\n<p>larger tasks should be generally understood to be placeholders for one or more additional tasks that will be added as soon as they are understood.</p>\n</blockquote>\n<h3>Commitment-Driven Iteration Planning</h3>\n<blockquote>\n<ol>\n<li>Adjust priorities</li>\n<li>Identify an iteration goal</li>\n<li>\n<p>Repeatedly while commitment is not full</p>\n<ol>\n<li>Select a story to add</li>\n<li>Expand the story into tasks</li>\n<li>Estimate tasks</li>\n<li>Team commits to story</li>\n</ol>\n</li>\n<li>Iteration planning is done</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>summing the estimates still gives some indication of the overall size of the work</p>\n</blockquote>\n<blockquote>\n<p>The key is that everyone on the team is accountable for contributing whatever is within their capabilities, regardless of whether it is their specialty.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Iteration plan describes the work expected for one iteration of the release plan. This is a more detail and focused plan. </p>\n<p>The iteration plan is created during a iteration planning meeting, There are some steps expected in this meetings that vary depending on the type of plan you are doing. There are two types:</p>\n<ul>\n<li>Velocity-driven: You plan based on your team's last velocity. </li>\n<li>Commitment-driven: You plan based on the team's commitment.</li>\n</ul>\n<p>The first step for either type of plan requires that the team prioritize the work to come. The team can use the iteration review meeting to do so.</p>\n<p>During the Iteration review meeting everyone interested in the project will be see the progress of that iteration, and they can provide valuable feedback.  This feedback might change the priority for the next iteration. Because a iteration review plus a iteration planning can be very tiring, you may choose to have a priorization meeting few days before the end of the iteration. This way you have a close enough knowledge whether the team will complete their iteration goal, and you are fresh to discuss the priorization for the next iteration.</p>\n<p>The second step is to identify a goal for your iteration. The work you will select in the next steps should represent what the teams need to do to accomplish the goal.</p>\n<p>The third step depends on the type of plan:</p>\n<p>For velocity-driven, select a user story with the highest priority, expand its task and estimate it. Repeat until you have reached the team's velocity from previous iterations.</p>\n<p>For commitment-drive, select a user story with the highest priority, expand its task and estimate it. Then, the teams decides whether it can commit to that user story for this iteration. Repeat until the team cannot commit to more user stories.</p>\n<p>When expanding the tasks, you should consider all work necessary to put the user story to production.</p>\n<h2>Chapter 15 - Selecting an Iteration Length</h2>\n<h3>Factors in Selecting an Iteration Length</h3>\n<blockquote>\n<ul>\n<li>The length of the release being worked on</li>\n<li>The amount of uncertainty</li>\n<li>The ease of getting feedback</li>\n<li>How long priorities can remain unchanged</li>\n<li>Willingness to go without outside feedback</li>\n<li>The overhead of iterating</li>\n<li>How soon a feeling of urgency is established</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Iteration length should be chosen to maximize the amount, frequency, and timeliness of feedback to the whole team.</p>\n</blockquote>\n<blockquote>\n<p>Naturally, one of the goals of a successful agile team is to reduce(or nearly eliminate) the overhead associated with each iteration.</p>\n</blockquote>\n<h3>Making a decision</h3>\n<blockquote>\n<p>One of the main goals in selecting an iteration length is finding one that encourages everyone to work at a consistent pace throughout the iteration. If the duration is too long, there is a natural tendency to relax a bit at the start of the iteration, which leads to panic and longer hours at the end of the iteration.</p>\n</blockquote>\n<blockquote>\n<p>My favorite technique to help reduce this strain is follow a macro-cycle of six two-week iterations followed by a one-week iteration. I refer to this cycle as \"6 x 2 + 1\". During the two-week iterations, the team works on items prioritized by the product owner. During the one-week iteration, however, the team chooses its own work.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>There are several reasons to why you should change the length of an iteration (see the bullets above). The length of the iteration will vary accordingly to your context. A project with a lot of uncertainty you might want shorter iterations. A project where you can only access your customer or users monthly you might choose a longer iteration.</p>\n<p>At the end of the iteration, you don't want to add overhead work (like manual tests and so on) to your next iteration. Focus on reducing it or eliminate.</p>\n<p>When making a decision focus on finding a consistent pace. A pace where the team can follow indefinitely and sustainably. You may also apply a \"6 x 2 + 1\" technique. After every 6 iterations of two week, the team has 1 iteration free to focus on work they can prioritize. This helps to reduce technical debt, and continuously improve the system.</p>\n<h2>Chapter 16 - Estimating Velocity</h2>\n<blockquote>\n<ul>\n<li>Use historical value</li>\n<li>Run an iteration</li>\n<li>Make a forecast</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>if you need to estimate velocity you should consider expressing the estimate as a range.</p>\n</blockquote>\n<h3>Use Historical Values</h3>\n<blockquote>\n<p>Any personnel or significant technology change will reduce the usefulness of historical measures of velocity.</p>\n</blockquote>\n<blockquote>\n<p>start by calculating the team's average velocity over the course of the preceding release.</p>\n</blockquote>\n<h3>Run an iteration</h3>\n<blockquote>\n<p>An ideal way to forecast velocity is to run an iteration( or two or three) an then estimate velocity from the <em>observed velocity</em> during one to three iterations.</p>\n</blockquote>\n<blockquote>\n<p>Some organizations will resist starting a project without having a more specific idea how long it will take. In such cases, stress that the need to run a few iterations first stems not from a desire to avoid making an estimate, but to avoid giving an estimate without adequate foundation.</p>\n</blockquote>\n<h3>Make a Forecast</h3>\n<blockquote>\n<ol>\n<li>Estimate the number of hours that each person will be available to work on the project each day</li>\n<li>Determine the total number of hours that will be spent on the project during the iteration</li>\n<li>Arbitrarily and somewhat randomly select stories, and expand them into their constituent tasks. Repeat until you have identified enough tasks to fill the number of hours in the iteration.</li>\n<li>Convert the velocity determined in the preceding step into a range.</li>\n</ol>\n</blockquote>\n<h3>My Summary</h3>\n<p>Always use velocity as a range.</p>\n<ul>\n<li>\n<p>Historical velocity: team's historical velocity (other projects). </p>\n<ul>\n<li>Best and worst velocities</li>\n<li>Subtract and add to the average velocity</li>\n<li>Multiply by the uncertainty cone</li>\n</ul>\n</li>\n<li>Run an iteration: Should be the default. <em>Observed velocity.</em></li>\n<li>\n<p>Make a Forecast: Last option.</p>\n<ul>\n<li>Define the project scope using user stories</li>\n<li>Create a \"plan an iteration\" with stories from the scope</li>\n<li>Determine based on the velocity of this fake iteration the total number of iteration to delivery all the scope.</li>\n</ul>\n</li>\n</ul>\n<h2>Chapter 17 - Buffering Plans for Uncertainty</h2>\n<blockquote>\n<p>A buffer is a margin for error around an estimate. In cases where there is significant uncertainty or the cost of being wrong is significant ,including a buffer is wise. The buffer help protect the project against the impact of the uncertainty. In this way, buffering a project schedule becomes an appropriate risk management strategy. </p>\n</blockquote>\n<h3>Feature Buffers</h3>\n<blockquote>\n<p>First the customer selects all of the absolutely mandatory work. The estimates for that work are summed. This represents the minimum that can be released. The customer then selects another 25% to 40% more work , selecting toward the higher end of the range for projects with more uncertainty or less tolerance for schedule risk. The estimates fort this work are added to the original estimate, resulting in a total estimate for the project. The project is then planed as normal for delivery of the entire set of functionality; however some amount of the work is optional and will be included only if time permits.</p>\n</blockquote>\n<blockquote>\n<p>On DSDM (Dynamic Systems Development Method) projects, requirements are sorted int four categories: Must Have, Should have, Could Have and Won't have. DSDM refers to this sorting as the MoSCoW rules. No more than 70% of the planned effort for a prospect can be target at must have requirements. </p>\n</blockquote>\n<h3>Schedule Buffers</h3>\n<blockquote>\n<p>the departure time is fixed, just like deadlines are on many software development projects. In this case, a schedule buffer protects against uncertainty that can affect the on-time completion of the project.</p>\n</blockquote>\n<h3>Reflecting Uncertainty in Estimates</h3>\n<blockquote>\n<p>The curve takes on this general shape because there normally is not much that can be done to accelerate the completion of a task, but there are an indefinite number of thins that can go wrong and delay the completion of a task.</p>\n</blockquote>\n<blockquote>\n<p>The additional time between the 50% an the 90% estimate is called <em>local safety</em>.</p>\n</blockquote>\n<blockquote>\n<p>We often add local safety to an estimate we want to be more confident of meeting.</p>\n</blockquote>\n<h3>Combining Buffers</h3>\n<blockquote>\n<p>When creating a release plan, our goal is to use buffers so that the team can make these types of commitments.</p>\n</blockquote>\n<h3>A Schedule Buffer Is Not Padding</h3>\n<blockquote>\n<p>I pad an estimate when I think it will take three days but I decide to tell you five, just in case.</p>\n</blockquote>\n<blockquote>\n<p>A schedule buffer is a necessary margin of safety added to the sum of estimates from which local safety has been removed.</p>\n</blockquote>\n<h3>Some Caveats</h3>\n<blockquote>\n<p>You should not hide their existence or how they are used.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Feature: Shopping list where you don't buy all the items</p>\n<p>Schedule: To arrive before your flight departs you plan yours steps until your arrival, and then adds a buffer to ensure if anything goes wrong you can still make it.</p>\n<p>Budget: Suppose you have thirty developers assigned to a project, but you have budget to allow up to thirty three developers.</p>\n<p>The buffer should not be added at each task, it should be added to the overall project deadline. Adding the buffer to the overall project deadline, we will avoid the Parkinson's law ( work that expands to its maximum length).</p>\n<p>Add buffers whenever you have a significant uncertainty or if the cost of being wrong is high. The buffers need to be communicated, and should not be hidden. If you are hiding buffers, most likely it's not a buffer and rather a padding.</p>\n<h2>Chapter 18 - Planning the Multiple-Team Project</h2>\n<h3>Establishing a Common Basis for Estimates</h3>\n<blockquote>\n<p>At the start of a project, the teams should meet and choose between story points and ideal days. They should then establish a common baseline for their estimates so that an estimate by one team will be similar to that of another team if the other team had estimated the work instead.</p>\n</blockquote>\n<blockquote>\n<p>The only time separate teams should consider estimating in different unit without a common baseline is when the products being built are truly separate and there is absolutely no opportunity for developer from one team to move onto another.</p>\n</blockquote>\n<h3>Adding Detail to User Stories Sooner</h3>\n<blockquote>\n<p>Ideally, an agile team begins an iteration with vaguely defined requirements and turns those vague requirements into functioning, tested software by the end of the iteration.</p>\n</blockquote>\n<blockquote>\n<p>On a multiple team project, is often appropriate and necessary to put more thought into the user stories before the start of the iteration. The additional detail allows multiple teams to coordinate work.</p>\n</blockquote>\n<blockquote>\n<p>What I've found to be the most useful outcome of work done in advance of the iteration is the identification of the product owner's conditions of satisfaction for the user stories that are likely to be developed during the iteration.</p>\n</blockquote>\n<h3>Lookahead Planning</h3>\n<blockquote>\n<p>When multiple teams need to coordinate work, the release plan should be updated to show and coordinate the work of the next two or three iterations.</p>\n</blockquote>\n<blockquote>\n<p>The release plan then becomes a rolling lookahead plan that always outlines expectations about the new few iterations.</p>\n</blockquote>\n<blockquote>\n<p>This is safer than planning on a handoff occurring during an iteration.</p>\n</blockquote>\n<blockquote>\n<p>However, to the extent possible, they should limit commitments to work completed before the start of the iteration.</p>\n</blockquote>\n<h3>Incorporating Feeding Buffers into the Plan</h3>\n<blockquote>\n<p>consider including a <em>feeding buffer</em>, like the schedule buffer of the previous chapter.</p>\n</blockquote>\n<blockquote>\n<p>This is, add a feeding buffer only if a team will be unable to do planned, high-priority work without the deliverables of another team.</p>\n</blockquote>\n<blockquote>\n<p>A feeding buffer that is longer than an iteration is usually the result of planning to pass a large chunk of functionality on to another team.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>These might be requirements for bigger projects</p>\n<ul>\n<li>\n<p>Establishing a common basis for estimates</p>\n<ul>\n<li>(Worked together on a past project) Select user stories to represent the baseline</li>\n<li>Collaboratively estimate user stories </li>\n</ul>\n</li>\n<li>\n<p>Adding detail to their user stories sooner</p>\n<ul>\n<li>Find at least the satisfaction condition for most user stories</li>\n</ul>\n</li>\n<li>\n<p>Performing lookahead planning</p>\n<ul>\n<li>Avoid handoff during iterations. Before the planning, try to identify dependencies between teams, and create a plan that accommodates all the user stories without handoffs during sprints. So if Team A is developing User Story 1 and Team B has the User Story 2 which depends on User Story 1. Create a plan where, Team B starts developing the User Story 2 the iteration after the Team A concludes User Story 1.</li>\n</ul>\n</li>\n<li>\n<p>Incorporating feeding buffers into the plan</p>\n<ul>\n<li>When you cannot avoid handoffs, you can add a feeding buffer to make sure the commitments are going to be fulfilled. Make sure you only use as last option, since it adds duration for the project schedule.</li>\n</ul>\n</li>\n</ul>\n<h1>Part V - Tracking and Communicating</h1>\n<h2>Chapter 19 - Monitoring the Release Plan</h2>\n<h3>Tracking the Release</h3>\n<blockquote>\n<p>First, and ideally most significant, is the amount of progress made by the team.</p>\n</blockquote>\n<blockquote>\n<p>Second, however, is any change in the scope of the project.</p>\n</blockquote>\n<blockquote>\n<p>... the developers may have learned things during the iteration that make them want to revise the story-point estimates assigned to some of the work coming later in the release plan.</p>\n</blockquote>\n<h3>Velocity</h3>\n<blockquote>\n<p>Velocity is expressed as the number of story points (or ideal days) completed per iteration.</p>\n</blockquote>\n<blockquote>\n<p>The most important rule is that a team counts points towards velocity only for stories or features that are complete at the end of the iteration.</p>\n</blockquote>\n<blockquote>\n<p>Complete means code that is well written, well factored, checked-in, and clean; complies with coding standards; and passes all tests.</p>\n</blockquote>\n<blockquote>\n<p>(when US will not be completed during iteration) Usually, this means the story will be moved out of the iteration or split and parts of it moved out.</p>\n</blockquote>\n<blockquote>\n<p>They wouldn't go so far as to accept a buggy untested version of the story, but they may reduce performance requirements, handling of special cases and so on.</p>\n</blockquote>\n<blockquote>\n<p>The more work in process a team allows to build up, the longer it will take new features to be transformed from raw ideas into functioning software.</p>\n</blockquote>\n<h3>Release Burndown Charts</h3>\n<blockquote>\n<p>This becomes a powerful visual indicator of how quickly a team is moving toward its goal.</p>\n</blockquote>\n<h3>A Release Burndown Bar Chart</h3>\n<blockquote>\n<p>it's useful to draw the release burndown chart so that you can easily see the team's velocity and the scope changes separately.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Any time work is completed, the top is lowered.</li>\n<li>When work is re-estimated, the top moves up or down.</li>\n<li>When new work is added, the bottom is lowered.</li>\n<li>When work is removed, the bottom is raised.</li>\n</ul>\n</blockquote>\n<h3>A Parking-Lot Chart</h3>\n<blockquote>\n<p>A <em>parking-lot chart</em> contains a large rectangular box for each theme (or grouping of user stories) in a release. Each box is annotated with the name of them, the number of stories in that theme, the number of story points or ideal days for those stories, and the percentage of the story points that are complete.</p>\n</blockquote>\n<blockquote>\n<p>A parking-lot chart is a powerful method for compressing a great deal of information into a small space.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Tracking the release of a project is important, because the project will change during its execution. Either because you had changes in scope, because you want to know where you are or because you learned and had to re-estimate your user stories.</p>\n<p>Velocity is the rate of progress for agile teams. It measures the completed user stories from each iteration.</p>\n<p>Why not consider unfinished work:</p>\n<ol>\n<li>It's hard to measure unfinished or incomplete work.</li>\n<li>Incomplete stories break down the trust between the developer team and the customer team on the project.</li>\n<li>Unfinished work leads to a buildup of work in process in the development process. (Too much W.I.P).</li>\n</ol>\n<p>Three types of charts:</p>\n<ol>\n<li>Burndown chart: Simple. It does not differentiate changes in scope from velocity.</li>\n<li>Burndown bar chart: Harder to understand. Differentiates velocity from changes in scope.</li>\n<li>Parking-lot chart: Compresses a great deal of information.</li>\n</ol>\n<h2>Chapter 20 - Monitoring the Iteration Plan</h2>\n<h3>The Task Board</h3>\n<blockquote>\n<p>A task board serves the dual purpose of giving a team a convenient mechanism for organizing their work and a way of seeing at a glance how much work is left.</p>\n</blockquote>\n<blockquote>\n<p>(Working on one task) This helps maintain a consistent flow of work through the process and reduces the cost of context switching among multiple tasks.</p>\n</blockquote>\n<h3>Tracking Effort Expended</h3>\n<blockquote>\n<p>On a project, it is far more useful to know how much remains to be done rather than how much has been done.</p>\n</blockquote>\n<blockquote>\n<p>tracking effort expended and comparing it with estimated effort can lead to \"evaluation apprehension\" (Sanders 1984).</p>\n</blockquote>\n<h3>Individual Velocity</h3>\n<blockquote>\n<p>Do not track individual velocity.</p>\n</blockquote>\n<blockquote>\n<p>Individuals should be given every incentive possible to work as a team.</p>\n</blockquote>\n<h3>My Summary</h3>\n<ul>\n<li>\n<p>Task Board: </p>\n<ul>\n<li>Organizes work</li>\n<li>Visual</li>\n</ul>\n</li>\n<li>\n<p>Iteration Burndown Charts: </p>\n<ul>\n<li>Same as release burndown chart</li>\n<li>For iteration longer than one week.</li>\n</ul>\n</li>\n</ul>\n<p>Do not track expended effort, focus on how much remains to be done.</p>\n<p>Do not track individuals velocity. It goes against a team spirit.</p>\n<h2>Chapter 21 - Communicating about Plans</h2>\n<blockquote>\n<p>We want all communication, but especially communication about estimates and plans, to be frequent, honest and two-way.</p>\n</blockquote>\n<blockquote>\n<p>Plans are updated throughout the project, and these updates need to be comunicated.</p>\n</blockquote>\n<blockquote>\n<p>If a developer knows that a given task will take much longer than currently expected, she needs to feel safe sharing that knowledge with the rest of the team, including her manager.</p>\n</blockquote>\n<blockquote>\n<p>It is important that communication about estimating and planning be two-way, because we want to encourage dialogue and discussion about the plan so that we make sure we always have the best possible plan (given the current knowledge) for delivering value to the organization.</p>\n</blockquote>\n<h3>Communicating the Plan</h3>\n<blockquote>\n<p>When possible, include with your communication of a target date either your degree of confidence in the estimate, a range of possible dates, or both.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Communication must be:</p>\n<ul>\n<li>Frequent</li>\n<li>Honest</li>\n<li>Two-way</li>\n</ul>\n<p>For communicating dates use either a degree of confidence, a range of possible dates or both.</p>\n<p>For communicating progress, because velocity fluctuates, consider:</p>\n<ul>\n<li>Last iteration's velocity</li>\n<li>Average velocity</li>\n<li>Average worst velocity ( slowest iterations)</li>\n</ul>\n<p>You may use a end-of-iteration summary to sum up the communication. But, don't spend too much time on it.</p>\n<h1>Part VI - Why Agile Planning Works</h1>\n<h2>Chapter 22 - Why Agile Planning Works</h2>\n<blockquote>\n<p>Planning is an attempt to find an optimal solution to the overall product development question: features, resources, and schedule.</p>\n</blockquote>\n<blockquote>\n<p>While planning, we are exploring the entire spectrum of possible solutions of how to mix these three parameters such that we create the best product possible.</p>\n</blockquote>\n<h3>Re-planning Occurs Frequently</h3>\n<blockquote>\n<p>The release plan is updated either after each iteration or, at worst, after every few iterations. Acknowledging the impossibility of creating a perfect plan goes a long way toward reducing the anxiety that accompanies such a goal</p>\n</blockquote>\n<blockquote>\n<p>Knowing that a plan can be revised at the start of the next iteration shifts a team’s focus from creating a perfect plan (an impossible goal) to creating a plan that is useful right now. </p>\n</blockquote>\n<blockquote>\n<p>For a plan to be useful it must be accurate, but we accept that early plans will be imprecise</p>\n</blockquote>\n<blockquote>\n<p>An agile estimating and planning process recognizes that our knowledge is always incomplete and requires that plans be revised as we learn.</p>\n</blockquote>\n<h3>Estimates Of Size and Duration Are Separated</h3>\n<blockquote>\n<p>A common planning flaw (on traditional as well as many agile teams) is confusing estimates of size and duration. </p>\n</blockquote>\n<blockquote>\n<p>To see the difference between estimates of size and duration, suppose I show you a book and ask you how long it will take you to read it. You can tell from its title that it’s a novel but I won’t let you look inside the book to see how many pages are in it, how wide the margins are, or how small the type is. To answer my question of how long it will take you to read this book, you first estimate the number of pages. Let’s assume you say 600. Then you estimate your rate of progress at one page per minute. You tell me it will take you 600 minutes or ten hours. In arriving at this estimate of duration (10 hours) you first estimated the size of the job (600 pages). </p>\n</blockquote>\n<h3>Plans Are Made At Different Levels</h3>\n<blockquote>\n<p>A team that works iteration to iteration without awareness of a more distant goal runs the risk of continually pursuing short-term goals while missing out on targeting a truly lucrative longer-term goal.</p>\n</blockquote>\n<h3>Plans Are Based On Features, Not Tasks</h3>\n<blockquote>\n<p>A traditional plan in the form of a Gantt chart, PERT chart, or work breakdown structure focuses on the tasks needed to create a product. An agile plan focuses instead on the features that will be needed in the product</p>\n</blockquote>\n<blockquote>\n<p>When planning by feature, the team has a much better understanding of the product</p>\n</blockquote>\n<h3>Small Stories Keep Work Flowing</h3>\n<blockquote>\n<p>On a software project, cycle time is the time from when the team begins work on a feature until it delivers value to users. The shorter the cycle time the better. </p>\n</blockquote>\n<blockquote>\n<p>One of the best ways to reduce variability is to work with reasonably small and similarly-sized units of work</p>\n</blockquote>\n<h3>Work In Process Is Eliminated Every Iteration</h3>\n<blockquote>\n<p>One of the reasons why agile planning succeeds is that all work in process is eliminated at the end of each iteration. Because work is not automatically rolled forward from one iteration to the next, each iteration is planned afresh.</p>\n</blockquote>\n<blockquote>\n<p>This means a shorter feedback loop from users to the project team, which leads to faster learning a well as more timely risk mitigation and control.</p>\n</blockquote>\n<h3>Uncertainty Is Acknowledged And Planned For</h3>\n<blockquote>\n<p>When we create a plan early in a project and do not update the plan as we acquire new knowledge, we lose the opportunity to synchronize the plan with reality</p>\n</blockquote>\n<h3>A Dozen Guidelines for Agile Estimating and Planning</h3>\n<blockquote>\n<p>the whole team needs to be involved and committed to the pursuit of the highest-value project possible.</p>\n</blockquote>\n<blockquote>\n<p> The best way to maintain a clear distinction between an estimate of size and one of duration is to use separate units that cannot be confused.</p>\n</blockquote>\n<blockquote>\n<p>Be sure to include an expression of uncertainty in any release plan you produce.</p>\n</blockquote>\n<blockquote>\n<p>If the amount of new functionality is fixed, state your uncertainty as a date range (“We’ll finish in the third quarter” or “we’ll finish in between 7 and 10 iterations”). If the date if fixed instead, express uncertainty about the exact functionality to be delivered (“We’ll be done on December 31 and the product will include at least these new features but probably no more than those other new features.”).</p>\n</blockquote>\n<blockquote>\n<p>Use re-planning opportunities to ensure that the project is always targeted at delivering the greatest value to the organization.</p>\n</blockquote>\n<blockquote>\n<p>Keep them informed by regularly publishing simple, very understandable indicators of the team’s progress.</p>\n</blockquote>\n<blockquote>\n<p>As we learn more about our customers’ needs, new features are added to the project. As we learn more about the technologies we are using or about how well we are working as a team, we adjust expectations about our rate of progress and our desired approach</p>\n</blockquote>\n<blockquote>\n<p>In addition to the value and cost of features when prioritizing consider also the learning that will occur and the risk that will be reduced by developing the feature.</p>\n</blockquote>\n<h3>My Summary</h3>\n<ul>\n<li>Re-planning occurs frequently</li>\n<li>Estimate of size and duration are separated</li>\n<li>\n<p>Plans are made at different levels</p>\n<ul>\n<li>Conveys the reality needed for each level. Daily is precise, Iteration less precise, Release is the least precise.</li>\n<li>Focus on short and long term</li>\n</ul>\n</li>\n<li>Plans Are Based On Features, Not Tasks</li>\n<li>Small Stories Keep Work Flowing</li>\n<li>Work In Process Is Eliminated Every Iteration</li>\n<li>\n<p>Tracking Is At The Team Level</p>\n<ul>\n<li>Do not track Individual performance</li>\n</ul>\n</li>\n<li>Uncertainty Is Acknowledged And Planned For</li>\n<li>\n<p>A Dozen Guidelines for Agile Estimating and Planning</p>\n<ul>\n<li>Involve the whole team</li>\n<li>Plan at different levels</li>\n<li>Keep estimates of size and duration separate by using different units</li>\n<li>Express uncertainty in either the functionality or the date</li>\n<li>Replan often</li>\n<li>Track and communicate progress</li>\n<li>Acknowledge the importance of learning</li>\n<li>Plan features of the right size</li>\n<li>Prioritize features</li>\n<li>Base estimates and plans on facts</li>\n<li>Leave some slack</li>\n<li>Coordinate teams through lookahead planning</li>\n</ul>\n</li>\n</ul>\n<h1>Part V - A Case Study</h1>\n<h2>Chapter 23 - A case Study: Bomb Shelter Study</h2>\n<blockquote>\n<p>A big part of the benefit we’re after from trying an agile process requires everyone to participate</p>\n</blockquote>\n<blockquote>\n<p>“The first form of the question is called the functional form, the second is the dysfunctional form. Asking it these two ways gives us a better answer than just asking ‘how much you want this feature.’ It tells us how the user will feel if the feature is there and how he’ll feel if it isn’t.</p>\n</blockquote>\n<blockquote>\n<p>“I want to separate our features into three categories. First are features we must have. These are cost-of-entry features like saving and restoring games. The second category is features that the more we have of them, the better. I suspect things like playing levels will be in this category. The more playing levels, like strong, medium, and weak, the better. The third category are the exciters. These are features that most users don’t expect but once they see them, they want them. The right mix of features from the latter two categories plus all the necessary must-have features can add up to a compelling product\"</p>\n</blockquote>\n<blockquote>\n<p>Everyone agreed that the level of effort was just right—not too hard, not too soft, just right. Everyone committed that as a team they would deliver those three stories in two weeks.</p>\n</blockquote>\n<blockquote>\n<p>“I’d prefer to give the estimate as a range. If we’re looking at enough work to go into an eighth iteration, we should probably say the project will take 6–10 iterations</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Shows in practice how the flow would work.</p>","frontmatter":{"title":"Agile estimating and planning","language":"en-US","coverPath":"agile-estimating-and-planning","status":"Read","date":"2020-03-01"}}},{"node":{"html":"<h1>Chapter 1</h1>\n<h2>Tuesday , September 2</h2>\n<blockquote>\n<p>We have to do more with less, to simultaneously maintain competitiveness and reduce costs.</p>\n</blockquote>\n<blockquote>\n<p>It should be like using the toilet. I use the toilet and, hell, I don't ever worry about it not working. What I don't want is to have the toilets back up and flood the entire building.</p>\n</blockquote>\n<h1>Chapter 2</h1>\n<h2>Tuesday, September 2</h2>\n<blockquote>\n<p>Show me a developer who isn't crashing production systems, and I'll show you one who can't fog a mirror. Or more likely, is on vacation.</p>\n</blockquote>\n<blockquote>\n<p>They're now logical instances inside of one big-iron server or maybe even residing somewhere in the cloud.</p>\n</blockquote>\n<blockquote>\n<p>We need to establish an accurate timeline of relevant events.</p>\n</blockquote>\n<h2>Summary</h2>\n<p>When faced by critical events establish a timeline of relevant events.</p>\n<h1>Chapter 3</h1>\n<h2>Tuesday, September 2</h2>\n<blockquote>\n<p>They're often carelessly breaking things and then disappearing, leaving Operations to clean up the mess.</p>\n</blockquote>\n<blockquote>\n<p>The only thing more dangerous than a developer is a developer conspiring with Security.</p>\n</blockquote>\n<blockquote>\n<p>\"We need more process around here and better support from the top, including IT process tooling and training. Everyone thinks that the real way to get work done is to just do it. That makes my job nearly impossible.\"</p>\n</blockquote>","frontmatter":{"title":"Phoenix Project","language":"en-US","coverPath":"phoenix-project","status":"Read","date":"2020-02-01"}}},{"node":{"html":"<h1>Prefacio</h1>\n<blockquote>\n<p>Até aquele ponto — e até 2005 —, a maior parte do desenvolvimento de software era feita usando o método em cascata</p>\n</blockquote>\n<blockquote>\n<p>O Scrum é semelhante aos sistemas autocorretivos, evolucionários e adaptativos.</p>\n</blockquote>\n<blockquote>\n<p>Scrum incorporou os conceitos de aprimoramento contínuo e produtos minimamente viáveis para obter feedback imediato dos consumidores, em vez de esperar até que o projeto tenha sido concluído.</p>\n</blockquote>\n<h1>Capitulo 1 - A maneira como o mundo funciona está quebrada</h1>\n<blockquote>\n<h3>Era por causa da maneira como as pessoas estavam trabalhando. A maneira como a maioria das pessoas trabalha. A maneira como nós achamos que o trabalho precisa ser feito, porque foi assim que aprendemos a fazê-lo.</h3>\n</blockquote>\n<blockquote>\n<p>O problema é que quando aquele plano elegante se depara com a realidade, ele cai por terra.</p>\n</blockquote>\n<blockquote>\n<p>planejar o combate é importante, mas assim que o primeiro tiro fosse disparado, o plano viraria fumaça.</p>\n</blockquote>\n<h2>Uma nova maneira de pensar</h2>\n<blockquote>\n<p>Esta nova abordagem se chama Scrum. Eu a criei vinte anos atrás. Agora essa é a <em>única</em> maneira comprovada de ajudar projetos deste tipo.</p>\n</blockquote>\n<p>Será mesmo ??</p>\n<blockquote>\n<p>eu chamei de “Scrum” essa estrutura de desempenho de equipe. O termo vem do jogo de rúgbi e se refere à maneira como um time trabalha junto para avançar com a bola no campo.</p>\n</blockquote>\n<blockquote>\n<p>Todo projeto envolve a descoberta de problemas e surtos de inspiração.</p>\n</blockquote>\n<blockquote>\n<p>O Scrum pergunta por que leva tanto tempo e tanto esforço para as coisas serem feitas, e por que somos tão deficientes para perceber quanto tempo e esforço algo vai exigir.</p>\n</blockquote>\n<blockquote>\n<p>O Scrum acolhe a incerteza e a criatividade. Coloca uma estrutura em volta do processo de aprendizagem, permitindo que as equipes avaliem o que já criaram e, o mais importante, de que forma o criaram.</p>\n</blockquote>\n<blockquote>\n<p>ao começar um projeto, por que não fazer paradas regulares para verificar se o que está sendo feito está seguindo na direção certa, e se, na verdade, os resultados são os que as pessoas desejam?</p>\n</blockquote>\n<blockquote>\n<p>De tempos em tempos, pare de fazer o que está fazendo, revise o que já fez e verifique se ainda deveria estar fazendo aquilo e como você pode fazê-lo melhor.</p>\n</blockquote>\n<h2>Consertando o FBI</h2>\n<blockquote>\n<p>Ninguém deveria passar a vida fazendo um trabalho sem significado algum.</p>\n</blockquote>\n<blockquote>\n<p>que agregará mais valor para o projeto? Faça essas coisas primeiro.</p>\n</blockquote>\n<blockquote>\n<p>que afirma que 80% do valor de qualquer parte dele está em 20% de suas funcionalidades.</p>\n</blockquote>\n<blockquote>\n<p>Fazer as pessoas priorizarem de acordo com o valor as obriga a produzir aqueles 20% primeiro. Em geral, depois que eles foram concluídos, elas se dão conta de que não precisam dos outros 80%, ou que o que parecia importante no início do projeto, na verdade, não era.</p>\n</blockquote>\n<blockquote>\n<p>Valores do manifesto ágil:\npessoas em vez de processos;\nprodutos que realmente funcionem em vez de documentação dizendo como o produto deveria funcionar;\ntrabalhar com os clientes em vez de negociar com eles;\ne responder às mudanças em vez de seguir um plano.</p>\n</blockquote>\n<blockquote>\n<p>Scrum é a estrutura que eu construí para colocar esses valores em prática. Não existe uma metodologia.</p>\n</blockquote>\n<blockquote>\n<p>\"Eu só poderei dizer a data à medida que eu vir o aprimoramento da equipe. Ou seja, o quanto vão ficar mais rápidos. O quanto eles vão conseguir <em>acelerar</em>.</p>\n</blockquote>\n<blockquote>\n<p>É claro que também é essencial que os membros da equipe descubram o que poderia <em>impedi-los</em> de acelerar. Nas palavras de Jeff Johnson: \"Eu lidei com a remoção do obstáculo\". Um \"obstáculo\" é uma ideia que vem da empresa que concebeu várias das ideias nas quais o Scrum se baseia: a Toyota.</p>\n</blockquote>\n<blockquote>\n<p>produção deveria fluir de forma calma e rápida por todo o processo, e ele dizia que uma das principais tarefas da gerência era identificar e remover os obstáculos para tal fluxo.</p>\n</blockquote>\n<blockquote>\n<p>Não é exagero dizer que, em um período de crescimento lento, tal desperdício seja mais um crime contra a sociedade do que uma perda nos negócios. A eliminação do desperdício deve ser o principal objetivo de uma empresa.</p>\n</blockquote>\n<blockquote>\n<p>O Scrum funciona com a definição de objetivos sequenciais que devem ser concluídos em um período definido.</p>\n</blockquote>\n<blockquote>\n<p>ao final de cada ciclo, deveria haver uma parte concluída do produto. Isso significava que eles precisavam ter alguma coisa funcionando, algo que poderia ser mostrado para qualquer pessoa que quisesse ver,</p>\n</blockquote>\n<blockquote>\n<p>Essa metodologia permite que as equipes tenham um feedback quase que imediato do trabalho realizado.</p>\n</blockquote>\n<blockquote>\n<p>A equipe decide a quantidade de trabalho que acredita ser capaz de realizar nas duas semanas seguintes. Eles escolhem as tarefas na lista de prioridades, as anotam em post-its e os colam na parede.</p>\n</blockquote>\n<blockquote>\n<p>No final do Sprint, a equipe se reúne e mostra o que conseguiu realizar naquele tempo.</p>\n</blockquote>\n<blockquote>\n<p>O importante era que começassem a estabelecer uma base para sentir o ritmo do trabalho - a velocidade que conseguiam atingir.</p>\n</blockquote>\n<blockquote>\n<p>“Como podemos trabalhar melhor no próximo Sprint? Quais foram os obstáculos que tivemos de remover durante esse período? Quais são os obstáculos que estão diminuindo o nosso ritmo?”.</p>\n</blockquote>\n<blockquote>\n<p>Além de descobrir a velocidade das equipes, ele também queria saber quais eram os obstáculos que as atrasava. O que ele realmente queria fazer era <em>acelerar</em> as equipes para que produzissem mais rápido.</p>\n</blockquote>\n<blockquote>\n<p> qual era a parte mais eficaz do Scrum? “As demonstrações. O trabalho voltado para um produto demonstrável com frequência”.</p>\n</blockquote>\n<blockquote>\n<p>“Scrum não é sobre os desenvolvedores. Mas sim sobre os clientes e os stakeholders ... Mostrar o produto real era a parte mais eficaz.”</p>\n</blockquote>\n<blockquote>\n<p>O crédito pertence ao homem que está realmente na arena, com o rosto desfigurado pela poeira e suor e sangue; que se esforça corajosamente; que erra, que tenta de novo e de novo, porque não existe esforço sem erros e lacunas;</p>\n</blockquote>\n<blockquote>\n<p>Uma galinha e um porco estão caminhando pela estrada, e a galinha diz:</p>\n<p>“Ei, porco, eu estava pensando que a gente devia abrir um restaurante.”\n“E qual vai ser o nome do restaurante?”, pergunta o porco.\n“Que tal ‘Presunto e Ovos’?”\n“Não, obrigado”, responde o porco.\n“Eu teria que me comprometer, mas você só teria de se envolver.”</p>\n</blockquote>\n<blockquote>\n<p>O que o Scrum faz é promover a união das equipes para criar grandes projetos, e isso exige que todos não apenas vejam o objetivo final, mas que também façam entregas incrementais para atingi-lo.</p>\n</blockquote>\n<h2>Resumo</h2>\n<p>Scrum vem para resolver o problema do waterfall, que tentava planejar tudo de antemão e seguir o plano conforme planejado. Isso não funciona.</p>\n<p>O nome Scrum vem do rugby.</p>\n<p>O Scrum assume que:</p>\n<ul>\n<li>\n<p>Planos mudam</p>\n<ul>\n<li>Descobertas de problemas</li>\n<li>Novas oportunidades</li>\n</ul>\n</li>\n<li>\n<p>Deve-se acolher incerteza e criatividade</p>\n<ul>\n<li>Aprender com as entregas</li>\n<li>Colher feedback mais rapido</li>\n</ul>\n</li>\n<li>\n<p>É muito difícil estimar projetos</p>\n<ul>\n<li>Entregas menores com estimativas váriaveis</li>\n<li>Conforme o projeto é executado a incerteza diminui e a estimativa melhora</li>\n<li>Aprendizado durante o projeto melhoram as estimativas</li>\n</ul>\n</li>\n<li>\n<p>É preciso fazer paradas para avaliar os resultados durante o processo</p>\n<ul>\n<li>Entregas dentro de cada ciclo</li>\n<li>Revisão do que foi feito</li>\n</ul>\n</li>\n<li>\n<p>É sempre possível melhorar os processos</p>\n<ul>\n<li>Melhoria continua</li>\n</ul>\n</li>\n</ul>\n<p>Começar desenvolvendo o que entrega mais valor. O 80% do valor está em 20% da funcionalidade (regra de pareto).</p>\n<p>Scrum é a aplicação do manifesto ágil.</p>\n<p>Data final de um projeto é baseada na velocidade que o time executa o projeto, que só é descoberta durante o projeto. A remoção dos bloqueios aumenta a velocidade do time.</p>\n<p>Produção deve fluir de forma clara e rápida por todo processo.</p>\n<blockquote>\n<p>O Scrum funciona com a definição de objetivos sequenciais que devem ser concluídos em um período definido.</p>\n</blockquote>\n<p>Ao fim do ciclo é preciso apresentar o que foi concluído. Isso permite feedback rápido e curto, e velocidade para mudanças, e desperdícios menores.</p>\n<p>A equipe é responsável por puxar a quantidade de trabalho que consegue entregar em um ciclo. No fim do ciclo, se analisa o quanto foi entregue. Baseado em alguns ciclos é possível definir uma velocidade.</p>\n<p>Uma vez que um ritmo foi identificado é importante buscar obstaculos para remover e aumentar a velocidade.</p>\n<h1>Capítulo 2 - As origens do Scrum</h1>\n<blockquote>\n<p>observar, avaliar, decidir e agir.</p>\n</blockquote>\n<blockquote>\n<p>“Nós temos que parar de fazer as coisas que estão nos matando”.</p>\n</blockquote>\n<h2>Aprendendo a pensar como um robô</h2>\n<blockquote>\n<p>Hesitante no começo, ele tropeçava pela sala como um filhote de gamo se erguendo sobre as pernas pela primeira vez. No entanto, a cada passo, ia se tornando mais seguro. As pernas aprendiam rapidamente a colaborar entre si e trabalhar juntas.</p>\n</blockquote>\n<blockquote>\n<p>O título era “The New New Product Development Game”</p>\n</blockquote>\n<blockquote>\n<p>As equipes eram multifuncionais. Tinham autonomia e um objetivo transcendente: estavam em busca de algo maior do que elas mesmas.</p>\n</blockquote>\n<blockquote>\n<p>A direção não impunha ordens - ao contrário, os executivos eram líderes facilitadores focados em retirar os obstáculos do caminho, não em determinar o que deviam fazer e como deveriam desenvolver o produto.</p>\n</blockquote>\n<blockquote>\n<p>Em 1995, apresentei um artigo com Ken Schwaber, intitulado “SCRUM Development Process”</p>\n</blockquote>\n<p><a href=\"http://jeffsutherland.com/oopsla/schwapub.pdf\">TODO LER</a></p>\n<h2>Inspeção e adaptação</h2>\n<blockquote>\n<p>Não basta melhorar uma vez, é preciso fazê-lo constantemente. Sempre procure algo que possa ser aprimorado.</p>\n</blockquote>\n<blockquote>\n<p>Nunca, jamais, conforme-se com o lugar onde está.</p>\n</blockquote>\n<blockquote>\n<p>[...] Não importa o quanto seus técnicos sejam excelentes, vocês, que são líderes, devem buscar sempre avançar no aprimoramento da qualidade e na uniformidade do produto, para que os seus ténicos consigam fazer melhorias. Portanto, o primeiro passo pertence à gerência. Primeiro, os técnicos da sua empresa e suas fábricas precisam saber que vocês se dedicam com fervor ao avanço da uniformidade e qualidade dos produtos e com um senso de responsabilidade em relação à qualidade do produto. Nada acontecerá com isso se você só falar sobre o assunto. As ações são importantes. - Edwards Deming</p>\n</blockquote>\n<blockquote>\n<p>E o método é agir e aplicar o conceito que tornou Deming tão famoso, ou seja, o ciclo PDCA (<strong>P</strong> lan [planeje],<strong>D</strong> o [faça],<strong>C</strong> heck [verifique],<strong>A</strong> ct [aja])</p>\n</blockquote>\n<blockquote>\n<p>E é dessa forma que qualquer tipo de produção \"enxuta\" ( o termo americano para o uso dos conceitos do Sistema Toyota de Produção ), ou desenvolvimento Scrum de produção, é feito.</p>\n</blockquote>\n<h2>Mude ou morra</h2>\n<blockquote>\n<p>é algo que você só consegue aprender fazendo, e no qual seu corpo, sua mente e seu espírito ficam alinhados por meio da prática e do aprimoramento constantes.</p>\n</blockquote>\n<h2>Resumo</h2>\n<h3>Errando e aprendendo</h3>\n<p>Temos que abdicar das coisas negativas e começar a aplicar PDCA. A maneira antiga não funciona mais.</p>\n<p>No começo vai ser mais difícil e mais erros serão cometidos, a equipe pode encontrar uma velocidade menor. Mas com o tempo as coisas vão melhorar, até que enfim alcança resultados melhores que os antigos. Exemplo de um robô de IA.</p>\n<blockquote>\n<p>As equipes eram multifuncionais. Tinham autonomia e um objetivo transcendente: estavam em busca de algo maior do que elas mesmas.</p>\n</blockquote>\n<p>O Scrum só se aprende fazendo. Se aprende através da prática e do aprimoramento constante. </p>\n<h3>Melhorias continua e constantes</h3>\n<p>Papel da direção é remover os obstáculos do caminho. A equipe determina o que deve fazer e como deveriam desenvolver o produto.</p>\n<p>As melhorias precisam ser constantes, sempre a procura de algo a melhorar.</p>\n<p>As ações são mais importantes do que somente falar sobre o assunto.</p>\n<p>Lean ou enxuto é a aplicação do Toyotismo nos estados unidos.</p>\n<h1>Capítulo 3 - Equipes</h1>\n<p>Quais são os elementos comuns que as equipes realmente notáveis possuem?</p>\n<blockquote>\n<p><strong>Transcendência</strong>: elas têm um senso de propósito além do comum. O objetivo percebido por todos permite que eles transformem o ordinário em extraordinário.a decisão de não estar na média, mas ser grandioso</p>\n</blockquote>\n<blockquote>\n<p><strong>Autonomia</strong>: as equipes se auto-organizam e se autogerenciam; têm o poder de tomar as próprias decisões sobre como fazer o próprio trabalho e têm o poder de fazer com que tais decisões sejam acatadas.</p>\n</blockquote>\n<blockquote>\n<p><strong>Interfuncionalidades</strong>: as equipes possuem todas as habilidades necessárias para concluir o projeto: E tais habilidades alimentam e reforçam umas as outras.“Quando todos os membros da equipe se encontram em uma sala grande, a informação de alguém se torna sua, mesmo sem você perceber\".</p>\n</blockquote>\n<h2>A longa fileira cinza</h2>\n<p>Fala sobre transcendencia</p>\n<h2>Scrum em período de rebelião</h2>\n<blockquote>\n<p>Em todas as grandes equipes, são os seus membros que decidem de que forma vão atingir os objetivos definidos pelos líderes da organização.</p>\n</blockquote>\n<blockquote>\n<p>Um dos conceitos-chave do Scrum é que os membros da equipe decicem, <em>eles mesmos</em>, como irão executar o trabalho. A responsabilidade da direção da empresa é estabelecer os objetivos estratégicos, e o trabalho da equipe é decidir como atingi-los.</p>\n</blockquote>\n<blockquote>\n<p>O que você fez desde a última vez que conversamos?\nO que você vai fazer antes de voltarmos a conversar?\nO que está atrapalhando o seu trabalho?</p>\n</blockquote>\n<blockquote>\n<p>sendo um verdadeiro Mestre Scrum, era certificar-se de que tudo que estivesse atrapalhando a equipe em uma reunião estivesse resolvido na seguinte.</p>\n</blockquote>\n<h2>Uma equipe para concluir o trabalho</h2>\n<blockquote>\n<p>A terceira perna do banco para grandes equipes é que todas elas tenham as habilidades necessárias para concluir o trabalho.</p>\n</blockquote>\n<blockquote>\n<p>Cada equipe funciona com todas as pessoas participando de todos os processos, do início ao fim.</p>\n</blockquote>\n<blockquote>\n<p>Equipes que consigam fazer com que o projeto inteiro seja concluído.</p>\n</blockquote>\n<h2>Scrum na guerra</h2>\n<blockquote>\n<p>sempre que há transferências entre equipes, existe a possibilidade de desastre.</p>\n</blockquote>\n<h2>O tamanho importa, sim, mas não do modo como você imagina</h2>\n<blockquote>\n<p>A dinâmica da equipe só funciona bem em grupos pequenos .sete pessoas, podendo haver variação de duas a mais ou a menos,</p>\n</blockquote>\n<blockquote>\n<p>\"acrescentar mais gente em um projeto de software atrasado resulta em um atraso ainda maior”</p>\n</blockquote>\n<blockquote>\n<p>trazer uma nova pessoa e fazê-la se tornar rápida faz com que todos os outros fiquem mais lentos.</p>\n</blockquote>\n<blockquote>\n<p>Nós não sabemos o que cada um está fazendo. E ficamos mais lentos quando tentamos descobrir.</p>\n</blockquote>\n<blockquote>\n<p>Todo o trabalho que está em andamento, os desafios enfrentados, os progressos feitos, tudo tem de ser transparente para todos.</p>\n</blockquote>\n<h2>O Mestre Scrum</h2>\n<blockquote>\n<p>o foco intenso no objetivo,\na colaboração radical\no anseio de aniquilar\nexcitação universal quando qualquer membro do time conseguia passar com a bola.</p>\n</blockquote>\n<blockquote>\n<p>O Mestre deveria ser o facilitador de todas as reuniões, avaliar se havia transparência e, o mais importante, ajudar a equipe a descobrir quais eram os obstáculos no caminho.</p>\n</blockquote>\n<blockquote>\n<p>O trabalho do Mestre Scrum é guiar a equipe em direção ao aprimoramento contínuo — sempre perguntar “Como podemos fazer melhor o que fazemos?”.</p>\n</blockquote>\n<blockquote>\n<p>“O que podemos mudar no modo como trabalhamos?” e “Qual é o nosso maior problema?”</p>\n</blockquote>\n<h2>Não odeie o jogador, odeie o jogo</h2>\n<blockquote>\n<p>Em vez de buscar culpados e falhas, ele recompensa o comportamento positivo porque se concentra nas pessoas trabalhando em conjunto e conseguindo cumprir as tarefas.</p>\n</blockquote>\n<blockquote>\n<p>Pessoas comuns, apenas fazendo seu trabalho, e sem qualquer hostilidade particular de sua parte, podem se tornar agentes em um processo destrutivo terrível. Além disso, mesmo quando os efeitos destrutivos de seu trabalho se tornam evidentemente claros, e eles recebem instruções para continuar fazendo ações incompatíveis com padrões fundamentais de moralidade, relativamente poucas pessoas têm os recursos necessários para resistir a autoridade.</p>\n</blockquote>\n<h2>Em busca da \"grandiosidade\"</h2>\n<blockquote>\n<p>O Scrum é sobre estabelecer o sistema certo com os devidos incentivos e dar às pessoas liberdade, respeito e autoridade para fazerem as coisas por si mesmas.A grandiosidade não pode ser imposta, precisa vir de dentro.</p>\n</blockquote>\n<h2>Resumo</h2>\n<p>Equipes grandiosas tem três caracteristicas em comum. Tem um objetivo transcendente, são autônomas e sao compostas de pessoas que tem todas as capacidades para concluir o projeto. O capitulo é basicamente sobre isso.</p>\n<h3>Transcendentes</h3>\n<p>Um objetivo maior que o time.</p>\n<h3>Autonomia</h3>\n<p>A direção estabelece a estratégia, o time estabelece como vai atingi-la.</p>\n<p>O ScM tem um papel essencial nas reuniões diárias, identificar o que bloqueia e remover até o próximo dia. Essas perguntas ajudam a identificar os obstáculos.</p>\n<p>O que você fez desde a última vez que conversamos?\nO que você vai fazer antes de voltarmos a conversar?\nO que está atrapalhando o seu trabalho?</p>\n<h3>Interfuncionalidade</h3>\n<p> A equipe precisa ter todas as capacidades de fazer o projeto de fim a fim. Todas as pessoas participam de todos os projetos de fim a fim.</p>\n<p>Ter somente uma equipe, reduz a necessidade de passagem de conhecimento, que é um problema comum que acontece problemas.</p>\n<p>Tamanho da equipe de 5 a 9 pessoas. </p>\n<p>Lei de brooks, adicionar mais pessoas leva mais tempo para terminar o projeto.</p>\n<p>Primeiro, por que dar velocidade pra uma pessoa nova, remove a velocidade dos outros.</p>\n<p>Segundo, por que quanto mais pessoas, mais é preciso esforço pra alinhar a comunicação, e maior a chance de falhas.</p>\n<h3>Aprimorando a equipe</h3>\n<p>O papel do ScM é facilitar as reuniões e remover obstáculos. Focar na melhoria continua.</p>\n<p>Não procurar culpados, o Scrum foca no comportamento positivo das pessoas trabalhando em conjunto para resolver os problemas.</p>\n<p>Estudo do choque, que mostra que as pessoas são sucetiveis ao ambiente em que estão. Não procure culpados, procure sistemas ruins.</p>\n<p>A grandiosidade não pode ser imposta, ela precisa vir de dentro. Autonomia, respeito e liberdade ajudam a alcançar esse sentimento.</p>\n<h1>Capítulo 4 - Tempo</h1>\n<blockquote>\n<p>O fluxo impiedoso do tempo define fundamentalmente o modo como enxergamos o mundo e a nós mesmos.</p>\n</blockquote>\n<blockquote>\n<p>Não somos bons em manter o foco, passamos muito mais horas do que o necessário no escritório, e somos péssimos em fazer estimativas de quanto tempo algo levará para ser feito.</p>\n</blockquote>\n<h2>O sprint</h2>\n<blockquote>\n<p>Aposto que raramente você recebe um feedback até que eles estejam concluídos - e isso pode levar meses, até anos. Você pode estar seguindo na direção <em>completamente</em> errada durante muito tempo, sem suspeitar de nada, e isso vai acabar subtraindo uma grande parte da sua vida.</p>\n</blockquote>\n<blockquote>\n<p>Quanto mais cedo você entregar o produto para os clientes, mais rápido eles serão capazes de dizer se você está fazendo algo de que precisam.</p>\n</blockquote>\n<blockquote>\n<p>Todos vamos trabalhar por períodos curtos e, então, paramos para ver aonde chegamos.</p>\n</blockquote>\n<blockquote>\n<p>O quadro está dividido em algumas colunas: “Pendências”, “Fazendo”, “Feito”.</p>\n</blockquote>\n<blockquote>\n<p>Todos na equipe conseguem visualizar o que cada um está fazendo o tempo todo.</p>\n</blockquote>\n<blockquote>\n<p>Você não pode fazer um Sprint de uma semana e, depois, um de três semanas. Você precisa ser consistente: quer estabelecer o ritmo de trabalho no qual as pessoas saibam o que pode ser feito em determinado período.</p>\n</blockquote>\n<blockquote>\n<p>Nada mais pode ser acrescentado por ninguém fora da equipe.</p>\n</blockquote>\n<blockquote>\n<p>interferir e distrair a equipe reduz drasticamente sua velocidade.</p>\n</blockquote>\n<h2>Reunião diária</h2>\n<blockquote>\n<p>O que você fez ontem para ajudar a equipe a concluir o Sprint?\nO que você vai fazer hoje para ajudar a equipe a concluir o Sprint? Que obstáculos a equipe está enfrentando?</p>\n</blockquote>\n<blockquote>\n<p>Se ela levar mais que 15 minutos, você está fazendo algo errado.O objetivo dessa reunião é ajudar todos na equipe a saberem exatamente em que ponto do Sprint eles estão.</p>\n</blockquote>\n<blockquote>\n<p>Quanto maior a saturação de comunicação — quanto mais todos sabem de tudo —, mais rápida é a equipe.</p>\n</blockquote>\n<blockquote>\n<p>Se as pessoas têm um cargo específico, elas tendem a fazer apenas as coisas que parecem combinar com esse cargo.</p>\n</blockquote>\n<blockquote>\n<p>Ali, onde o trabalho era feito, só havia membros de uma equipe.</p>\n</blockquote>\n<blockquote>\n<p>Se a equipe inteira não estivesse presente, a comunicação simplesmente não acontecia.</p>\n</blockquote>\n<blockquote>\n<p>Se algo precisasse ser discutido mais a fundo, anotávamos e nos reuníamos depois.</p>\n</blockquote>\n<blockquote>\n<p>A terceira regra era que todo mundo tinha de participar de forma efetiva. Para isso, eu disse que todos deviam ficar de pé.</p>\n</blockquote>\n<blockquote>\n<p>O importante é que ela deve ocorrer no mesmo horário todos os dias, com as mesmas três perguntas, que todos estejam de pé e que ela não dure mais do que 15 minutos.</p>\n</blockquote>\n<blockquote>\n<p>A ideia é que a equipe converse de forma rápida de como poderão seguir em direção à vitória</p>\n</blockquote>\n<blockquote>\n<p>A passividade não é apenas algo indolente, mas também algo que afeta o desempenho do restante da equipe.</p>\n</blockquote>\n<blockquote>\n<p>A equipe precisa querer ser grandiosa.</p>\n</blockquote>\n<h2>Repetição</h2>\n<blockquote>\n<p>Cada Sprint é uma oportunidade de fazer algo totalmente nove; a cada dia, uma chance de se aprimorar. O Scrum encoraja uma visão holística do mundo.</p>\n</blockquote>\n<h2>Resumo</h2>\n<p>O tempo define como trabalhamos e como vemos o mundo.</p>\n<p>Não somos bom em manter foco e em estimativas.</p>\n<h3>Modo de trabalho - Sprint</h3>\n<p>O sprint ajuda a ter feedback continuo, o que evita o desperdício. Quanto antes entregar mais rápido vem o feedback, e menor o desperdício.</p>\n<p>Dentro do Sprint todos sabem as tarefas, e se reuniem uma vez por dia para alinhar. As tarefas ficam visiveis para todos acompanharem. Todos sabem o que todos estão fazendo.</p>\n<p>Uma vez que o sprint foi fechado, não se adciona nada por pessoas de fora da equipe. Interferências reduzem a velocidade do time.</p>\n<h1>Capítulo 5 - O desperdício é um crime</h1>\n<blockquote>\n<p>O coração do Scrum é o ritmo.</p>\n</blockquote>\n<blockquote>\n<p>ciclo virtuoso que sempre se reforce e que promova o que temos de melhor e diminua o que temos de pior?</p>\n</blockquote>\n<blockquote>\n<p>Muri: desperdício causado pela irracionalidade;\nMura: desperdício causado pela inconsistência;\nMuda: desperdício causado pelos resultados.\nPDCA\n<em>Plan</em> significa evitar o Muri\n<em>Do</em> significa evitar o Mura\n<em>Check</em> significa evitar o Muda</p>\n</blockquote>\n<h2>Fazer uma coisa de cada vez</h2>\n<blockquote>\n<p>A tarefa consiste em escrever os números de 1 a 10 em algarismos arábicos e romanos (I, II, III, IV etc.), e as letras do alfabeto, do A ao L.</p>\n</blockquote>\n<blockquote>\n<p>Há um diagrama excelente que aparece em um trabalhos clássicos sobre como desenvolver software para computadores, “Software com qualidade”, por Gerald Weinberg:</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Número de projetos simultâneos</th>\n<th>Porcentagem do tempo disponível por projeto</th>\n<th>Perda com a troca de contexto</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>1</td>\n<td>100%</td>\n<td>0%</td>\n</tr>\n<tr>\n<td>2</td>\n<td>40%</td>\n<td>20%</td>\n</tr>\n<tr>\n<td>3</td>\n<td>20%</td>\n<td>40%</td>\n</tr>\n<tr>\n<td>4</td>\n<td>10%</td>\n<td>60%</td>\n</tr>\n<tr>\n<td>5</td>\n<td>5%</td>\n<td>75%</td>\n</tr>\n</tbody>\n</table>\n<p><img src=\"../assets/img/priorizacao-projetos.png\" alt=\"troca de contexto\"></p>\n<blockquote>\n<p>A priorização e foco minimizarão os custos da troca de contexto</p>\n</blockquote>\n<blockquote>\n<p>Eles não mudam o tamanho do projeto, nem os elementos envolvidos na sua criação, mas apenas fazendo uma coisa exclusivamente antes de avançar faz com que a tarefa leve um pouco mais do que a metade do tempo.</p>\n</blockquote>\n<blockquote>\n<p>O que eu quero, porém, é que você se torne consciente do custo da troca de contexto. Ela é real e você deve tentar minimizá-la.</p>\n</blockquote>\n<blockquote>\n<p>minimize esse desperdício tentando fazer de uma vez só todas as tarefas que exigem um tipo específico de concentração.</p>\n</blockquote>\n<h2>Fazer pela metade não é o mesmo que fazer</h2>\n<blockquote>\n<p>O Scrum pega muito dos seus conceitos no modelo de manufatura dos japoneses que foram apresentados no livro clássico O sistema Toyota de produção: além da produção em larga escala</p>\n</blockquote>\n<blockquote>\n<p>A ideia é que é um desperdício ter um monte de coisas à sua volta se elas não forem usadas para construir algo.</p>\n</blockquote>\n<blockquote>\n<p>Se, por exemplo, tudo o que uma fábrica de automóveis tem é um monte de carros pela metade, ela gastou muito dinheiro e esforço, mas não criou nada de valor real.</p>\n</blockquote>\n<blockquote>\n<p>Mas o maior erro que você pode cometer é tentar fazer cinco coisas de uma vez. Isso é ser multitarefa, e você provavelmente não conseguirá fazer tudo, o que vai deixá-lo com um “trabalho em processo”.</p>\n</blockquote>\n<blockquote>\n<p>Você pintou uma das paredes do banheiro, a ração do cachorro ainda está no porta-malas do carro, e o cheque da hipoteca já está preenchido, mas ainda não foi enviado, e você já juntou as folhas do jardim, mas falta ensacá-las.</p>\n</blockquote>\n<blockquote>\n<p>Fazer as coisas pela metade é o mesmo que não fazer nada.</p>\n</blockquote>\n<blockquote>\n<p>Mas “concluir” significa um produto completo que pode ser entregue e usado por um cliente.</p>\n</blockquote>\n<blockquote>\n<p>Você gastou recursos, esforço e tempo e não conseguiu nada que se encontre em um estado que pode ser entregue.</p>\n</blockquote>\n<blockquote>\n<p>Talvez tivesse sido melhor criar algo menor — algo que realmente funcione.</p>\n</blockquote>\n<blockquote>\n<p>Se você investe muito valor em coisas que não estão entregando valor, não terá recursos para fazer outras coisas</p>\n</blockquote>\n<blockquote>\n<p>Você tem que ter algum estoque; a chave é minimizá-lo.</p>\n</blockquote>\n<h2>Faça certo da primeira vez</h2>\n<blockquote>\n<p>cada trabalhador tem a capacidade de parar toda a linha. Quando isso acontece, todos se reúnem na linha que parou — não para brigar com o cara que fez isso, mas para corrigir o problema detectado.</p>\n</blockquote>\n<blockquote>\n<p>querem é corrigir o problema de uma vez, porque assim estará resolvido para sempre.</p>\n</blockquote>\n<blockquote>\n<p>A maneira como você lida com esses erros pode ter um impacto extraordinário na velocidade e no nível de qualidade com que você faz as coisas.</p>\n</blockquote>\n<blockquote>\n<p>o momento certo de corrigir um problema seja quando ele é detectado, e não depois.</p>\n</blockquote>\n<blockquote>\n<p>Levou 24 vezes mais tempo . Se um bug fosse corrigido no dia em que foi criado, levava uma hora para corrigir; três semanas depois, levava 24 horas.</p>\n</blockquote>\n<blockquote>\n<p>Você tem que se tornar a pessoa que era no passado de novo, colocar-se de volta dentro de uma mente que não existe mais. Fazer isso demora, e demora muito</p>\n</blockquote>\n<blockquote>\n<p>faça as coisas certas da primeira vez . O único elemento novo que os dados acrescentam é que, se você cometer um erro — e todos nós cometemos —, corrija-o assim que o notar.</p>\n</blockquote>\n<h2>Trabalhar de mais resulta em mais trabalho</h2>\n<blockquote>\n<p>Trabalhar até mais tarde não era um sinal de comprometimento, era um sinal de fracasso.</p>\n</blockquote>\n<blockquote>\n<p>Se você não pode realmente tirar um tempo sem precisar se certificar de que tudo está indo bem no escritório, parece que você não está gerenciando bem a sua equipe.</p>\n</blockquote>\n<blockquote>\n<p>Você consegue uma produtividade maior. Você é mais feliz. E você tem mais qualidade”. É óbvio. Trabalhar menos aumenta a produtividade e a qualidade.</p>\n</blockquote>\n<blockquote>\n<p>pessoas que trabalham horas demais começam a cometer erros,</p>\n</blockquote>\n<blockquote>\n<p>Funcionários que trabalham além da conta tendem a se distrair mais e a começar a distrair os outros. Logo eles começam a tomar decisões ruins.</p>\n</blockquote>\n<blockquote>\n<p>“esgotamento do ego”. A ideia é que fazer qualquer escolha envolve um gasto de energia.</p>\n</blockquote>\n<blockquote>\n<p>Então, existe um número limitado de decisões sensatas que você pode tomar em qualquer dia, e, à medida que vai tomando decisões, diminui a capacidade de controlar seu próprio comportamento. Você começa a cometer erros — e pode acabar cometendo erros sérios.</p>\n</blockquote>\n<blockquote>\n<p>Ao não trabalhar demais, você vai conseguir produzir mais e com mais qualidade.</p>\n</blockquote>\n<blockquote>\n<p>Tudo que importa é a rapidez e a qualidade com que ele é entregue.</p>\n</blockquote>\n<h2>Seja razoável</h2>\n<blockquote>\n<p>Desperdício irracional:</p>\n<p>Você quer dar à sua equipe objetivos desafiadores - fazer com que queiram mais. Mas você não vai querer que eles lutem por objetivos absurdos ou impossíveis.</p>\n</blockquote>\n<blockquote>\n<p>Desperdício expectativas exageradas:\nUma equipe que depende de ações heroicas constantes para cumprir os prazos não está trabalhando da maneira que deveria.</p>\n</blockquote>\n<blockquote>\n<p>Desperdício de sobrecarga:\ndesperdiçam tempo e não produzem qualquer valor.</p>\n</blockquote>\n<blockquote>\n<p>Não seja escroto — e não permita, estimule ou aceite esse tipo de comportamento nos outros.</p>\n</blockquote>\n<h2>Fluxo</h2>\n<blockquote>\n<p>Qualquer “processo” que as pessoas usam é desperdício, e isso inclui o Scrum.</p>\n</blockquote>\n<blockquote>\n<p>e processos ruins estão tão enraizados em nossa forma de pensar</p>\n</blockquote>\n<blockquote>\n<p>o Scrum faz é se concentrar na tentativa de eliminar os desperdícios</p>\n</blockquote>\n<h2>Resumo</h2>\n<h3>Desperdicios</h3>\n<p>Existem 3 formas de desperdicio:</p>\n<ul>\n<li><em>Muri</em>: desperdício causado pela irracionalidade</li>\n<li><em>Mura</em>: desperdício causado pela inconsistência</li>\n<li><em>Muda</em>: desperdício causado pelos resultados</li>\n</ul>\n<p>O PDCA evita os desperdicios:</p>\n<ul>\n<li><em>Plan</em> significa evitar o Muri</li>\n<li><em>Do</em> significa evitar o Mura</li>\n<li><em>Check</em> significa evitar o Muda</li>\n</ul>\n<h3>Irracionalidade</h3>\n<p>Desperdício por irracionalidade está relacionado com multitarefas, e a ideia de que fazer mais coisas ao mesmo entrega mais rapido. Na pratica isso é pessimo.</p>\n<p>Fazer várias coisas ao mesmo tempo causa troca de contexto, e a troca de contexto gera um custo alto de desperdicio.</p>\n<p>Exercicio que demonstra escrever do 1 ao 15 e do A ao O. Primeiro os numeros e depois as letras. Fazendo junto, numero e letras.</p>\n<p>Priorizacão e foco evitam a troca de contexto ( desperdício ) e garantem entregas mais rapidas e com mais qualidade.</p>\n<h3>Inconsistência</h3>\n<p>O segundo desperdicio, de inconsistência está relacionado com fazer as coisas pela metade. Repriorizacões no meio do sprint acabam fazendo isso. O valor nunca é entregue. Multitarefa + não entregar é a pior coisa.</p>\n<p>Além de parar com coisas pela metade, outro problema é entregar várias coisas que não entregam valor. Quanto mais tempo investido em algo que não entrega valor, menos tempo teremos para investir em valor.</p>\n<h3>Sobrecarga</h3>\n<p>A terceira forma de desperdicio é a sobrecarga. </p>\n<p>Fazer sem qualidade por causa da sobrecarga gera erros. Quanto mais tarde um erro é encontrado mais dificil e maior o esforço para corrigir.</p>\n<p>Corrigir um bug 3 semanas depois de entregue leva 24 vezes a mais que corrigir um bug no mesmo dia.</p>\n<p>Quando problemas aparecem, e eles vão, devem ser corrigidos imediatamente. Todos tem o poder de parar a linha de produção.</p>\n<p>Trabalhar de mais é sinal de fracasso. E diminui a produtividade.</p>\n<p>\"Esgotamento de ego\" é a ideia que toda decisão tem um custo. Mesmo a mais simples. Quem trabalha por muito tempo, começa a tomar decisões ruins e atrapalhar os demais em sua volta.</p>\n<p>Ao não cometer os erros, será mais rapido. Logo, trabalhar menos pode gerar mais velocidade e melhor qualidade.</p>\n<p>qualquer processo é um desperdicio, inclusive o Scrum. O Scrum é focado em eliminar desperdicios.</p>\n<h1>Capítulo 6 - Planeje a realidade, não a fantasia</h1>\n<blockquote>\n<p>pensaram que poderiam planejar tudo com antecedência.</p>\n</blockquote>\n<blockquote>\n<p>Não quanto tempo, mas sim quanto trabalho .</p>\n</blockquote>\n<blockquote>\n<p>“Definição de Feito”.Todo mundo sabe quando algo está concluído ou não; existem padrões claros aos quais cada parte do trabalho deve atender.</p>\n</blockquote>\n<blockquote>\n<p>as pessoas simplesmente dizem que tudo é importante. Mas o que ele estava perguntando era “O que vai trazer mais valor ao projeto? Vamos fazer essas coisas primeiro”.</p>\n</blockquote>\n<h2>Planejando um casamento</h2>\n<blockquote>\n<p>O cone da incerteza</p>\n</blockquote>\n<blockquote>\n<p>estimativas iniciais de trabalho podem variar de 400% do tempo que o trabalho realmente levou a 25% do tempo gasto. Trata-se de uma magnitude de erro de dezesseis vezes.Conforme um projeto avança e mais coisas são resolvidas, as estimativas vão se aproximando mais da linha da realidade, até que não haja mais estimativas, apenas a realidade.</p>\n</blockquote>\n<blockquote>\n<p>Mas a chave é refinar o plano no decorrer do projeto, em vez de tentar planejar tudo com antecedência. Planeje apenas a quantidade de detalhes suficiente para entregar o próximo incremento de valor, e estime o restante do projeto em partes maiores.</p>\n</blockquote>\n<blockquote>\n<p>todos esses elementos e classificá-los por prioridade. E isso será diferente para pessoas diferentes.</p>\n</blockquote>\n<blockquote>\n<p>É necessário ter esses dados, porque se você se deparar com restrições de data ou de custos, saberá onde começar a fazer os cortes:</p>\n</blockquote>\n<blockquote>\n<p>organizar por valor, independente de qual seja ele.</p>\n</blockquote>\n<h2>O tamanho importa, mas apenas de forma relativa</h2>\n<blockquote>\n<p>Usar estimativas relativas.</p>\n</blockquote>\n<blockquote>\n<p>Fazer estimativas em grupo dessa forma é uma maneira de obter uma previsão bem mais precisa do que se tivéssemos de fazê-las sozinhos.</p>\n</blockquote>\n<h2>O oráculo de Delfos</h2>\n<blockquote>\n<p>“efeito manada”. Você já deve ter participado de uma reunião assim: alguém aparece com uma ideia, e todo mundo começa a falar sobre ela. E mesmo que você discorde a princípio, segue adiante porque o grupo resolveu seguir.</p>\n</blockquote>\n<blockquote>\n<p>“Uma cascata informativa ocorre quando é ótimo para um indivíduo, ao observar as ações da pessoa à sua frente, seguir o comportamento dessa pessoa sem considerar as suas próprias informações”</p>\n</blockquote>\n<blockquote>\n<p>é essencial que você use seu próprio julgamento, e use outras estimativas para melhorar a sua, não para substituí-la.</p>\n</blockquote>\n<blockquote>\n<p>O que os dois pesquisadores fizeram foi conduzir uma série de estudos anônimos. Nenhum dos peritos sabia quem eram os demais; eles só recebiam as estimativas feitas por eles. Depois de cada questionário, a dupla de pesquisadores pegava as respostas — e os dados usados para chegar a elas — e as fornecia de volta para o grupo, retirando qualquer característica identificadora. Limpar e repetir.</p>\n</blockquote>\n<h2>Pôquer do planejamento</h2>\n<blockquote>\n<p>Então, a vantagem do método Delphi é que ele pega uma faixa ampla de opções, tenta remover ao máximo possível os preconceitos e, com declarações bem informadas, mas anônimas, estreita as opiniões até chegar a uma estimativa largamente aceitável.</p>\n</blockquote>\n<blockquote>\n<p>Cada item que precisa ser estimado é levado à mesa. Então, todos puxam a carta que acreditam representar o esforço necessário para concluí-lo e a colocam em cima da mesa com o número voltado para baixo. Todos viram suas cartas ao mesmo tempo. Se todo mundo está a uma carta de diferença do outro (digamos, um 5, dois 8 e um 13), a equipe apenas soma os resultados e tira a média (naquele caso, 6,6) e segue para o outro item.</p>\n</blockquote>\n<blockquote>\n<p>Se os números das cartas mostrarem uma diferença superior a três , as pessoas com as cartas mais altas e as mais baixas falam sobre o motivo por que acreditam que o seu número é o adequado.Então, fazemos uma nova rodada para a mesma tarefa.</p>\n</blockquote>\n<blockquote>\n<p>Esse método incrivelmente simples é um modo de evitar qualquer tipo de comportamento de ancoragem, tais como os efeitos manada ou halo, e permite que toda a equipe compartilhe conhecimento sobre uma tarefa específica.</p>\n</blockquote>\n<blockquote>\n<p>Contudo, é crucial que você esteja com a equipe que realmente vai executar o trabalho para fazer as estimativas, e não alguns especialistas “ideais” para isso.</p>\n</blockquote>\n<blockquote>\n<p>as pessoas que estão fazendo o trabalho sabem quanto tempo e esforço cada tarefa exige.</p>\n</blockquote>\n<blockquote>\n<p>As equipes, conforme já disse antes, são individuais e únicas — cada uma tem seu próprio ritmo. Forçá-las a se encaixar em um processo inflexível é uma receita para o desastre.</p>\n</blockquote>\n<h2>Não existem tarefas; existem apenas histórias</h2>\n<blockquote>\n<p>Então, a primeira coisa em que você deve pensar ao considerar uma tarefa é em um personagem ou em um papel</p>\n</blockquote>\n<blockquote>\n<p>Para quem</p>\n</blockquote>\n<blockquote>\n<p>pensar no quê </p>\n</blockquote>\n<blockquote>\n<p>Por que</p>\n</blockquote>\n<blockquote>\n<p>Então, antes de priorizar o que precisa ser feito em sua empresa, você precisa definir o personagem, o usuário, o cliente — a pessoa que vai usar o que você vai produzir. Você precisa saber do que gostam, do que não gostam, suas paixões, entusiasmos, frustrações e alegrias. Logo, precisa compreender as suas motivações.</p>\n</blockquote>\n<h2>Escreva histórias curtas</h2>\n<blockquote>\n<p>não prescrevem como serão feitas</p>\n</blockquote>\n<blockquote>\n<p>a equipe decide de que forma o trabalho será realizado, mas o que será realizado é decidido pelo valor que traz aos negócios.</p>\n</blockquote>\n<blockquote>\n<p>A coleção completa de histórias que podem construir a ideia de uma livraria virtual costuma ser chamada de “Épico” — uma história grande demais para se fazer, mas que inclui um número de histórias menores que se somam até formar uma ideia única.</p>\n</blockquote>\n<h2>Esteja preparado e conclua a história</h2>\n<blockquote>\n<p>INVEST</p>\n</blockquote>\n<blockquote>\n<p><strong>I</strong>ndependente: A história precisa ser prática e passível de ser “feita” por si só. Ela não deve ser inerentemente dependente de outra história.\n<strong>N</strong>egociável. Até que esteja sendo realizada, ela precisa poder ser reescrita. A permissão para alterações está embutida nela.\n<strong>V</strong>aliosa. Ela realmente deve acrescentar valor ao cliente, usuário ou stakeholder.\n<strong>E</strong>stimável. Você deve ser capaz de mensurá-la.\n<strong>S</strong>mall [pequena]. A história precisa ser pequena o suficiente para que você possa estimar e planejar facilmente. Se ela for grande demais, reescreva-a ou divida-a em histórias menores.\n<strong>T</strong>estável. A história precisa ter um teste no qual deve passar para ser completa. Escreva o teste antes de fazer a sua história.</p>\n</blockquote>\n<blockquote>\n<p>Para cada história que se deseja fazer lá deve haver uma “definição de Pronto” (ou seja, “ela atende aos critérios INVEST?”) e, por fim, uma “definição de Feito” (ou seja, “Quais condições precisam ser atendidas, que testes precisam ser feitos para considerá-la feita?”).</p>\n</blockquote>\n<h2>Planejamento do Sprint</h2>\n<blockquote>\n<p>\"tudo bem, o que podemos fazer neste Sprint? Essas histórias estão prontas? Elas podem ser concluídas ao final desta iteração? Poderemos demonstra-las para o cliente e mostrar-lhe valor real?</p>\n</blockquote>\n<h2>Conheça a sua velocidade</h2>\n<blockquote>\n<p>Temos todas essas histórias, essas coisas que precisam ser feitas; e já fizemos as estimativas para cada uma delas — esta é um oito, aquela é um três etc. E, então, começamos o primeiro Sprint; digamos que ele tenha a duração de uma semana. Ao final da semana, contamos as histórias que foram concluídas, totalizamos os pontos em que foram estimadas, e esse número revela a velocidade da equipe. Uma vez que você conheça a velocidade, pode olhar para quantas histórias ainda precisa realizar e quantos pontos elas representam e, então, saberá quando o trabalho será concluído.</p>\n</blockquote>\n<blockquote>\n<p>Nada está escrito em pedra. Questione tudo.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Existe alguma coisa que podemos fazer para acelerar mais as coisas?</li>\n<li>Podemos nos livrar de alguns itens pendentes? Será que existem coisas que outras equipes possam fazer?</li>\n<li>Será que podemos deixar de fazer algumas coisas? Podemos reduzir o escopo do projeto de alguma forma?</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>a cultura na qual as pessoas trabalham muda, e isso pode ser assustador para algumas pessoas.</p>\n</blockquote>\n<blockquote>\n<p>Mudar aquela cultura, porém, foi o que permitiu o surgimento da verdadeira excelência”.</p>\n</blockquote>\n<blockquote>\n<p>O mapa não é o terreno. Não se apaixone pelo seu plano. É quase certo que ele esteja errado.</p>\n</blockquote>\n<blockquote>\n<p>O trabalho é uma história. Pense primeiro sobre quem vai obter valor com algo, então, pense no que é, e, então, por que eles precisam daquilo.</p>\n</blockquote>\n<h2>Resumo</h2>\n<h3>Planejamento</h3>\n<p>Planejar tudo de antecedência não funciona, quando o projeto começa novos desafios aparecem, é preciso se adaptar ao plano. O esforço final pode variar de 400% pra cima ou 25% abaixo da estimativa inicial. Esse é chamado o cone da incerteza.</p>\n<p>Planejar o suficiente para a proxima iteração, planejar o restante em partes maiores. Uma vez que esses itens estão quebrados, é importante lista-los e priorizalos.</p>\n<p>As prioridades devem começar pelo que entrega mais valor. É preciso identificar o que é prioridade entre os desenvolvimentos, não pode existir, tudo é importante.</p>\n<p>Caso haja restrição de datas ou de custos cortar pelo que foi considerado menos importante.</p>\n<p>Durante as estimativas é preciso evitar o efeito manada, cada pessoa tem que ter o senso crítico e expor sua opinião.</p>\n<p>Pesquisadores, aplicaram questionarios anônimos e passavam para pares revisar ( sem identificação ) e foram refinando até chegar em um resultado ideal.</p>\n<p>O poker planning todos votam anônimamente, para evitar o efeito manada. Durante a discussão os votos menores e maiores descutem para tentar remover qualquer preconceito com informação. O objetivo é chegar em um valor amplamente aceitavel.</p>\n<p>Alem da estimativa mais assertiva, acontece uma troca de conhecimento entre a equipe.</p>\n<p>É crucial que as estimativas sejam feitas pelas equipes que vão executar as tarefas. As pessoas que vão executar as tarefas sabem quanto esforço cada tarefa exige.</p>\n<h3>User Stories</h3>\n<p>Todas as historias devem ser desenvolvidas para alguem, que precisa de algo, para alcançar alguma coisa. É preciso entender, quem é o usuário, cliente, ou quem for usar para saber o que a pessoa gosta, do que ela precisa e por ai vai.</p>\n<p>As user stories não prescrevem como serão feitas. A equipe decide o como e o negócio decide o, o que deve ser feito.</p>\n<p>Um conjunto de user stories é chamado de épico. Algo que é muito grande para ser feito sozinho.</p>\n<p>As user stories devem seguir o INVEST</p>\n<p>e devem ter uma definicão de pronto e uma definicão de feito.</p>\n<p>Sempre nos sprints devemos perguntar</p>\n<ul>\n<li>o que podemos fazer neste Sprint?</li>\n<li>Essas histórias estão prontas?</li>\n<li>Elas podem ser concluídas ao final desta iteração?</li>\n<li>Poderemos demonstra-las para o cliente e mostrar-lhe valor real?</li>\n</ul>\n<h3>Velocidade</h3>\n<p>Ao fim do sprint, ver quantas user stories foram concluidas e somar os pontos. Assim é possível saber a velocidade do time, e saber quanto tempo um projeto vai levar.</p>\n<p>É possível aumentar a velocidade. Sempre questionar tudo.</p>\n<ol>\n<li>Existe alguma coisa que podemos fazer para acelerar mais as coisas?</li>\n<li>Podemos nos livrar de alguns itens pendentes? Será que existem coisas que outras equipes possam fazer?</li>\n<li>Será que podemos deixar de fazer algumas coisas? Podemos reduzir o escopo do projeto de alguma forma?</li>\n</ol>\n<p>Mudar a cultura permite alcançar excelência.</p>\n<h1>Capítulo 7 - Felicidade</h1>\n<blockquote>\n<p>Mas se você perguntar a eles quando se sentiram mais felizes, então ouvirá que foi naqueles momentos de dificuldadede levar o corpo, a mente e o espírito ao limite.</p>\n</blockquote>\n<blockquote>\n<p>“nós não somos recompensados por apreciarmos a jornada em si, mas por um término bem-sucedido. A sociedade recompensa resultados, não processos; chegadas, não jornadas”.</p>\n</blockquote>\n<blockquote>\n<p>Não atingimos picos mais altos todos os dias, ou conseguimos uma pontuação alta, nem obtemos um grande bônus. A maior parte dos nossos dias é dedicada a seguirmos em direção aos nossos objetivos, independente de quais sejam.</p>\n</blockquote>\n<h2>Felicidade é sucesso</h2>\n<blockquote>\n<p>felicidade precede resultados importantes e são indicadores de prosperidade”.</p>\n</blockquote>\n<blockquote>\n<p>O Scrum tem como foco pegar essas coisas pequenas e construí-las, de forma sistemática, como degraus para o sucesso.</p>\n</blockquote>\n<h2>Quantificando a felicidade</h2>\n<blockquote>\n<p>a meta não é chegar a determinado nível de produtividade e se manter ali, mas analisar constantemente seu processo para melhorá-lo cada vez mais.</p>\n</blockquote>\n<blockquote>\n<p>[...] Kaizen, ou aperfeiçoamento. [...] no que chamamos de \"Retrospectiva do Sprint\". [...]eles se sentam e pensam sobre o que deu certo, o que poderia ter sido melhor e o que pode ser aperfeiçoado.</p>\n</blockquote>\n<blockquote>\n<p>é necessária uma dose de maturidade emocional e uma atmosfera de confiança.</p>\n</blockquote>\n<blockquote>\n<p>É essencial que as pessoas, como uma equipe , assumam a responsabilidade pelos seus processos e resultados, e procurem soluções, da mesma maneira.</p>\n</blockquote>\n<blockquote>\n<p>precisam ter coragem de levantar as questões que realmente as incomodam,</p>\n</blockquote>\n<blockquote>\n<p>Ao mesmo tempo, devem ter a maturidade de ouvir o feedback, absorvê-lo e buscar uma solução, em vez de assumirem uma postura defensiva.</p>\n</blockquote>\n<blockquote>\n<p>A Retrospectiva do Sprint corresponde à \"verificação\" (check) do ciclo PDCA de Deming. A chave é chegar à etapa \"aja\", àquele <em>kaizen</em>, que vai realmente mudar o processo e torná-lo melhor da próxima vez.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Em uma escala de 1 a 5, como você se sente em relação ao papel que desempenha na empresa? </li>\n<li>Usando essa mesma escala, como você se sente em relação à empresa como um todo?</li>\n<li>Por que você se sente assim?</li>\n<li>O que tornaria você mais feliz no próximo Sprint?</li>\n</ol>\n</blockquote>\n<blockquote>\n<p>Você precisa definir o que é o sucesso de uma forma concreta e contestável,</p>\n</blockquote>\n<h2>Torne tudo visível</h2>\n<blockquote>\n<p>autonomia, maestria e objetivo.</p>\n</blockquote>\n<blockquote>\n<p>é a habilidade de controlar o próprio destino, a sensação de se aperfeiçoar cada vez mais e saber que você está fazendo algo maior do que si mesmo.</p>\n</blockquote>\n<blockquote>\n<p>prelúdio para autonomia, maestria e objetivo é a transparência.</p>\n</blockquote>\n<blockquote>\n<p>É por isso que, no Scrum, qualquer pessoa pode participar de qualquer reunião.</p>\n</blockquote>\n<blockquote>\n<p>Se você não pode confiar nas pessoas que está contratando para subirem a bordo com você, então está contratando as pessoas erradas e estabeleceu um sistema no qual o fracasso já está incorporado.</p>\n</blockquote>\n<blockquote>\n<p>Qualquer pessoa pode entrar na sala, olhar para o quadro e saber exatamente como a equipe está se desenvolvendo.</p>\n</blockquote>\n<blockquote>\n<p>Quando tudo está claro, a equipe pode se auto-organizar para superar os problemas que se tornaram óbvios.</p>\n</blockquote>\n<h2>Proporcionando felicidade</h2>\n<blockquote>\n<p>quanto mais as pessoas se conectam com as pessoas com quem trabalham, mais felizes elas são — e, ao que parece, mais produtivas e inovadoras também.</p>\n</blockquote>\n<blockquote>\n<p>encorajam fisicamente encontros ao acaso.</p>\n</blockquote>\n<blockquote>\n<p>mudança de pensamento: de “trabalhar para uma empresa” para “trabalhar com a minha empresa”.</p>\n</blockquote>\n<blockquote>\n<p>Uma pessoa em uma equipe pode ter algum conhecimento ou habilidade específicos — conhecimentos que vão juntando como pessoas avarentas e sovinas, sem compartilhar. Eles consideram esses conhecimentos uma posse que assegura seus empregos.</p>\n</blockquote>\n<blockquote>\n<p>O Scrum dá às pessoas uma estrutura em que essa mudança aconteça. Fornece uma base para a empresa inteira seguir na direção do objetivo comum.</p>\n</blockquote>\n<blockquote>\n<p>Seus pilares são a transparência, o trabalho em equipe e a colaboração.</p>\n</blockquote>\n<h2>Estoure a bolha da felicidade</h2>\n<blockquote>\n<p>Os gestores podem encorajar a autonomia ao permitir que as pessoas tomem as próprias decisões sobre como fazer seu trabalho.</p>\n</blockquote>\n<blockquote>\n<p>Os gestores também devem ter tolerância zero em relação a grosserias e jamais permitir que um funcionário envenene a cultura corporativa por meio do abuso ou desrespeito.E, por fim, eles devem dar um feedback rápido e direto.</p>\n</blockquote>\n<blockquote>\n<p>A bolha da felicidade [ ...] \"ei, nós melhoramos tanto e não precisamos mais melhorar\"</p>\n</blockquote>\n<blockquote>\n<p>temem a mudança em si, sentindo que se o que eles têm está funcionando, por que mudar?</p>\n</blockquote>\n<blockquote>\n<p>Mestre Scrum. Ele precisa ser capaz de detectar o problema e abordá-lo com a equipe.essencial que ele faça perguntas difíceis.</p>\n</blockquote>\n<blockquote>\n<p>Um “tolo sábio” é uma pessoa que faz perguntas desconfortáveis ou levanta verdades desconfortáveis.</p>\n</blockquote>\n<p>História do imperador, tolo sábio.</p>\n<h2>Feliz hoje, feliz amanhã</h2>\n<p>Hedonistas:</p>\n<blockquote>\n<p>um monte de gente reunida em uma garagem figurativa apenas fazendo coisas porque elas são legais e divertidas, sem dar muita atenção a criar um produto sustentável.</p>\n</blockquote>\n<p>Niilistas:</p>\n<blockquote>\n<p>Descobrem que o mundo que tatno adoravam é uma drova, pois agora existem todos os tipos de regras, testes e relatórios; é uma droga hoje, e eles acham que ser[a uma droga pra sempre.</p>\n</blockquote>\n<p>Trazidos para gerenciar o lugar:</p>\n<blockquote>\n<p>São dispostos a trabalhar oitenta horas por semana ( e desejosos de obrigar os outros a fazerem o mesmo), porque acreditam que serão promovidos depois e, por isso, serão mais felizes. Então, quando isso finalmente acontece, eles precisam se contentar apenas com novas dores de cabeça , que exigirão ainda mias tempo de trabalho.</p>\n</blockquote>\n<p>Tenta identificar e incorajar o Scrum:</p>\n<blockquote>\n<p>indivíduo que está trabalhando em coisas que são legais hoje, mas mantendo um olhar em direção a um futuro melhor, e que está convencido de que o trabalho será divertido para sempre.</p>\n</blockquote>\n<blockquote>\n<p>as pessoas conquistarem crescimento pessoal e realização.</p>\n</blockquote>\n<blockquote>\n<p>A felicidade precisa servir como um equipamento para produzir resultados.</p>\n</blockquote>\n<h2>Resumo</h2>\n<p>O final não é a recompensa, a jornada que é a recompensa. Assim como quando alguem que sobe o monte Everest vai ter mais pra contar sobre os momentos de dificuldades, os momentos de superação. Focar em manter um ambiente agradável e de constante evolução.</p>\n<p>A felicidade precede o sucesso, o Scrum ajuda na criação desse ambiente agradável atráves de pequenos e consistentes passos.</p>\n<h3>Melhoria continua</h3>\n<p>A meta sempre deve ser a melhoria continua, sempre existe alguma coisa que podemos fazer melhor.</p>\n<p>Esse conceito vem do Toyotismo do Kaizen, e no Scrum é praticado nas retrospectivas.</p>\n<p>Nas retrospectivas, se conversa sobre o que foi bom, e o que pode melhorar. É preciso bastante maturidade para alcançar resultados positivos.</p>\n<p>O time precisa ter confiança e coragem para levantar os pontos negativos que realmente incomodam. O time e os individuos precisam se responsabilizar pelos pontos levantados.</p>\n<p>O time e individuos precisam tomar uma posição resoluta em relação aos problemas levantados na retrospectiva.</p>\n<p>A retrospectiva equivale ao Check do PDCA, é importante chegar no passo Act, para a melhoria realmente aconteça.</p>\n<p>É imprescindivel definir o que é sucesso para cada ação, assim é possivel identificar se houve evolução ou não.</p>\n<p>Como se sentiram no Sprint? O que fez vocês se sentirem assim? Como podemos fazer para melhorar?</p>\n<h3>O que torna o time feliz</h3>\n<p>Autonomia, maestria e objetivo.</p>\n<p>As pessoas precisam ter o sentimento que fazem parte de algo maior que elas mesmas.</p>\n<p>Para alncançar esses 3 pontos é preciso transparência.</p>\n<p>No Scrum, o board é visível para todos, para que todos possam saber no que o time está trabalhando, qual a velocidade, quem está fazendo quais tarefas.</p>\n<p>Qualquer pessoa, pode participar de qualquer reunião.</p>\n<p>É preciso confiar nas pessoas que você está contratando. </p>\n<p>Com a transparência a equipe se torna capaz de se auto-gerenciar.</p>\n<h3>Esporte coletivo</h3>\n<p>Quanto mais as pessoas se conectam, mais elas trabalham felizes e entregam melhores resultados.</p>\n<p>Queremos incentivar o pensamento de trabalhar com a empresa, ao inves de para a empresa.</p>\n<p>O trabalho em equipe, colaboração, troca de conhecimento ajudam a guiar em busca desse objetivo maior.</p>\n<h3>Autonomia e estagnação</h3>\n<p>O gestores devem incentivar autonomia para que o time tome suas proprias decisões.</p>\n<p>Jamais deve-se tolerar alguém abusando ou mal-tratando os colegas. A cultura da empresa deve ser protegida.</p>\n<p>Um possível problema, é que o time pode cair em uma bolha de felicidade. Achando que já evoluiram o suficiente e não precisam mais melhorar.</p>\n<p>É papel do ScM, identificar e abordar com o time. Fazer o papel de tolo sábio, e fazer perguntas desconfortáveis para re-ativar os processos de melhoria continua.</p>\n<p>Papel do tolo sábio, pode ser exemplificado pela historia do imperador que foi enganado e comprou um vestido que só os dignos poderiam ver. Todos da realeza concordaram que o vestido era lindo, pois não queriam parecer não dignos. Ele desfilou pelado, até uma criança apontar o fato de ele estar pelado.</p>\n<h3>Perfil ideal</h3>\n<p>Queremos o perfil de pessoas que trabalham com coisas legais, mas sempre estão olhando para o futuro, em busca de melhorias. E sabem que sempre serão felizes com o que estão fazendo.</p>\n<h1>Capítulo 8 - Prioridades</h1>\n<blockquote>\n<p>é sobre impulsionar o impacto,</p>\n</blockquote>\n<blockquote>\n<p>Eles não sabiam o que fazer, nem quando.</p>\n</blockquote>\n<p><img src=\"../assets/img/product-owner-priorities.png\" alt=\"Product owner priorities\"></p>\n<blockquote>\n<p>se concentrar apenas no que pode construir, é possível que acabe fazendo algo que ninguém, na verdade, quer,</p>\n</blockquote>\n<blockquote>\n<p>se concentrar no que pode vender, talvez prometa coisas que não conseguirá construir.</p>\n</blockquote>\n<blockquote>\n<p>só construir o que consegue vender, mas sem qualquer entusiasmo, acabará trabalhando duro para construir algo medíocre.</p>\n</blockquote>\n<h2>Pendências: o que fazer e quando fazer</h2>\n<blockquote>\n<p>A primeira coisa que você precisa fazer quando está implementando o Scrum é criar uma lista de pendências.</p>\n</blockquote>\n<blockquote>\n<p>quer uma lista de tudo que poderia ser incluído na visão daquele produto.</p>\n</blockquote>\n<blockquote>\n<p>quais os itens que terão maior impacto para a empresa, que são mais importantes para o cliente, que possam resultar em um ganho maior de dinheiro e são mais fáceis de fazer?</p>\n</blockquote>\n<blockquote>\n<p>gerem mais valor e tenham o menor risco.</p>\n</blockquote>\n<blockquote>\n<p>gerando valor para seus clientes o mais rápido possível.</p>\n</blockquote>\n<blockquote>\n<p>80% do valor está em 20% dos atributos.</p>\n</blockquote>\n<blockquote>\n<p>descobrir como você desenvolve esses 20% primeiro.</p>\n</blockquote>\n<h2>O dono do produto</h2>\n<blockquote>\n<p>Existem apenas três papéis no Scrum. Ou você faz parte da equipe e está fazendo o trabalho — é o Mestre Scrum, que ajuda a equipe a descobrir como trabalhar melhor; ou, então, você é o Dono do Produto, que decide que trabalho deve ser feito. Ele (ou ela) é o dono da Lista de Pendências, do que está ali e, o mais importante, em que ordem.</p>\n</blockquote>\n<blockquote>\n<p>escolher profissionais de todos esses grupos para criar uma equipe interfuncional capaz de fazer</p>\n</blockquote>\n<blockquote>\n<p>Mas o que eles não têm é autoridade.</p>\n</blockquote>\n<blockquote>\n<p>Eles não fazem avaliações de desempenho, não promovem funcionários nem dão aumentos. Mas decidem sobre o carro e como ele será feito — por persuasão, não coerção.</p>\n</blockquote>\n<blockquote>\n<p>Ele tem de persuadir, convencer e demonstrar que aquela maneira é a melhor.</p>\n</blockquote>\n<blockquote>\n<p>Eu queria aquilo no Scrum, mas também sabia muito bem que havia pouquíssimas pessoas com aquele nível de habilidade e experiência. Então, dividi o papel em dois, dando ao mestre Scrum o <em>como</em> Dono do Produto o <em>o quê</em>.</p>\n</blockquote>\n<blockquote>\n<p>O Dono do Produto precisava ser capaz de dar à equipe o feedback do cliente em cada um dos Sprints.</p>\n</blockquote>\n<blockquote>\n<p>O Mestre Scrum e a equipe são responsáveis pela rapidez com que estão produzindo e como podem aumentar a velocidade. O Dono do Produto é responsável por traduzir a produtividade da equipe em valor.</p>\n</blockquote>\n<blockquote>\n<ol>\n<li>Precisa ser bem-informado em relação ao setor: entender o processo que a equipe está executando o suficiente para saber o que pode ser feito e, tão importante quanto isso, o que não pode. Mas ele também precisa entender bem o o quê o suficiente para saber como traduzir o que pode ser feito em um valor verdadeiro e significativo.</li>\n<li>ele precisa ter autonomia para decidir sobre a visão de como o produto será, e o que precisa ser feito para chegar até lá.</li>\n<li>Disponível para a equipe, explicar o que precisa ser feito e o porquê.</li>\n<li>Ele precisa ser responsável pelo valor.</li>\n</ol>\n</blockquote>\n<h2>Observe, Oriente, Decida, Aja</h2>\n<blockquote>\n<p>Dono do Produto tome decisões rápidas, com base em feedback de tempo real.</p>\n</blockquote>\n<blockquote>\n<p>\"Observar\": ver a situação como um todo — e não apenas a partir de seu próprio ponto de vista.\n\"Orientar\": reflete a forma como você vê o mundo e o lugar que ocupa nele, mas o mundo que você é <em>capaz</em> de ver.\nObservação + orientação levam a \"Decisão\" e \"Ação\"</p>\n</blockquote>\n<blockquote>\n<p>Ao entregar um incremento funcional, o que o Scrum faz é dar ao Dono do Produto a capacidade de ver quanto valor tal incremento cria e como as pessoas reagem a ele. Então, com base em tal informação, ele pode mudar o que a equipe vai fazer no próximo Sprint. Isso forma um ciclo constante de feedback que acelera a inovação e a adaptação, além de fazer com que o Dono do Produto seja capaz de medir quanto valor foi entregue</p>\n</blockquote>\n<blockquote>\n<p>O importante não é ter um projeto totalmente estabelecido no início, mas construir um protótipo funcional e, depois, ver o que é possível aprimorar.</p>\n</blockquote>\n<blockquote>\n<p>A ideia é que o quanto antes você tenha um feedback real, mais rápido você será capaz de construir um produto melhor.</p>\n</blockquote>\n<blockquote>\n<p>produto minimamente viável. Qual é o mínimo absoluto que posso construir e ainda entregar como valor a um cliente?</p>\n</blockquote>\n<blockquote>\n<p>Um pequeno mundo agradável onde não existem mudanças... [criaturas que vivem nesse mundo são] dinossauros; eles vão morrer. A chave do negócio é não se tornar um dinossauro. Se você se encontra em uma condição de equilíbrio, você está morto...</p>\n</blockquote>\n<h2>Primeiro, as coisas mais importantes</h2>\n<blockquote>\n<p>atualizando e mudando constantemente a prioridade das coisas nas Pendências a cada Sprint,</p>\n</blockquote>\n<blockquote>\n<p>A chave é reconhecer a incerteza, aceitar completamente que a imagem atual da sequência e do valor só é relevante naquele instante específico.</p>\n</blockquote>\n<blockquote>\n<p>porque os gestores não sabem exatamente onde o valor reside é tornar tudo prioridade.</p>\n</blockquote>\n<h2>Liberação</h2>\n<blockquote>\n<p>Sempre que estiver fazendo algo, você quer colocar seu produto nas mãos de quem vai realmente usá-lo o mais rápido possível.</p>\n</blockquote>\n<blockquote>\n<p>Qual é o nível de eficácia que o produto deve ter? Bem, ele deve realmente funcionar,</p>\n</blockquote>\n<blockquote>\n<p>Você precisa levar o produto até o público o quanto antes possível! Isso lhe dará o feedback de que precisa para energizar seu ciclo de decisão e priorização.</p>\n</blockquote>\n<blockquote>\n<p>Chega a ser engraçado de tão incompleto que está.</p>\n</blockquote>\n<blockquote>\n<p>Cometa esses erros nos estágios iniciais, causando o menor prejuízo possível.</p>\n</blockquote>\n<h2>Pagamento por nada e alterações gratuitas</h2>\n<blockquote>\n<p>então o gerenciamento de projetos tradicional foi projetado para impedir isso. Ele foi projetado para impedir a entrega de valor rápido demais.</p>\n</blockquote>\n<blockquote>\n<p>Liste todas as funcionalidades que você espera;</p>\n</blockquote>\n<blockquote>\n<p>Ao final, existe determinado número de pontos para cada atributo.</p>\n</blockquote>\n<blockquote>\n<p>cliente, que, neste cenário está obrigado, contratualmente, a trabalhar junto com o Dono do Produto,</p>\n</blockquote>\n<blockquote>\n<p>Qualquer item ou atributo das Pendências pode ser colocado em qualquer outro lugar.</p>\n</blockquote>\n<blockquote>\n<p>basta retirar atributos de tamanho equivalente do produto.</p>\n</blockquote>\n<h2>Risco</h2>\n<blockquote>\n<p>Os três tipos mais comuns de risco são o risco de mercado, o risco técnico e o risco financeiro.</p>\n</blockquote>\n<blockquote>\n<p>pessoas querem o que estou construindo? Elas podem realmente construir o que quero? Nós realmente conseguiremos vender o que produzimos?</p>\n</blockquote>\n<blockquote>\n<p>risco de mercado. O Scrum ajuda você a minimizá-lo ao enfatizar a entrega em incrementos.</p>\n</blockquote>\n<blockquote>\n<p>verdade, as pessoas não sabem o que querem até que possam testar algo.</p>\n</blockquote>\n<blockquote>\n<p>Risco teórico. A decisão não foi tomada com base em construtos teóricos; eles tinham algo que podiam olhar e tocar.</p>\n</blockquote>\n<blockquote>\n<p>permite que diferentes ideias sejam expressas de forma rápida sem um investimento volumoso.</p>\n</blockquote>\n<blockquote>\n<p>Risco financeiro. Eles construíram algo interessante, mas não conseguem vendê-lo por um valor suficiente para ter lucro.</p>\n</blockquote>\n<blockquote>\n<p>primórdios da Internet, quando o espaço on-line permitiu pela primeira vez que as empresas mirassem em segmentos específicos de cliente, a “hiperconcentração” era considerada algo valioso.</p>\n</blockquote>\n<blockquote>\n<p>embora as pessoas amem o seu produto, elas não o amam o suficiente para parar e pensar o que custa para produzi-lo.</p>\n</blockquote>\n<h2>Aqui está o que você vai fazer amanhã</h2>\n<blockquote>\n<p>como Dono no Produto, faça um mapa de para onde você acha que as coisas estão se encaminhando.</p>\n</blockquote>\n<blockquote>\n<p>apenas uma projeção no tempo, então não planeje demais, apenas faça estimativas.</p>\n</blockquote>\n<blockquote>\n<p>A mensagem mais importante é que tudo que está sendo feito é de conhecimento de todos.</p>\n</blockquote>\n<blockquote>\n<p>Você não precisa dedicar muito tempo em planejamento, reflexões, meditações, declaração da missão, ou projeções para daqui a cinco anos.</p>\n</blockquote>\n<blockquote>\n<p>O que você pode ter acreditado ser necessário no início pode ser algo de que você, na verdade, nunca precisou.</p>\n</blockquote>\n<h2>Resumo</h2>\n<h1>Capítulo 9 - Mude o mundo</h1>\n<blockquote>\n<p>Eles vão aplicar a nova abordagem SMART — Specific [específica], Mensurável, Alcançável, Relevante, e com Tempo determinado.</p>\n</blockquote>\n<blockquote>\n<p>Eis como um projeto começa na Valve: alguém decide começá-lo. Só isso. Eles decidem o melhor uso de seu tempo e energia, o que servirá melhor para a empresa e os clientes, e fazem isso. Como conseguem outras pessoas para trabalharem no projeto? Convencem-nas.</p>\n</blockquote>\n<blockquote>\n<p>ela não espera ser forçada a mudar por uma crise, mas sim, muda constantemente</p>\n</blockquote>\n<blockquote>\n<p>revisão dos pares é constante.</p>\n</blockquote>\n<blockquote>\n<p>em vez de se dar o luxo de ter alguém para dizer a você o que fazer, você tem um grupo de pares dizendo a você o que eles acham do que você decidiu fazer.</p>\n</blockquote>\n<blockquote>\n<p>O importante é se livrar do que está no caminho, retirar os obstáculos para que elas se tornem as pessoas que são capazes de ser.</p>\n</blockquote>\n<h2>Resumo</h2>\n<h1>Apêndice - Implementando o Scrum: como começar</h1>\n<blockquote>\n<p>A equipe só deve demonstrar o que satisfaz a Definição de Feito.</p>\n</blockquote>","frontmatter":{"title":"A arte de fazer o dobro do trabalho na metade do tempo","language":"pt-BR","coverPath":"a-arte-de-fazer-o-dobro-do-trabalho-na-metade-do-tempo","status":"Read","date":"2020-01-01"}}},{"node":{"html":"","frontmatter":{"title":"The pragmatic programmer","language":"en-US","coverPath":null,"status":"Read","date":"2020-01-01"}}},{"node":{"html":"<p><a href=\"https://tools.ietf.org/html/rfc6749\">RFC6749 - The OAuth 2.0 Authorization Framework</a></p>\n<h1>Abstract</h1>\n<blockquote>\n<p>[...] enables a third-party application to obtain limited access to an HTTP service, either on behalf of a resource Owner by orchestrating an approval interaction between the resource owner and the HTTP service, or by allowing the third-party application to obtain access on its own behalf.</p>\n</blockquote>\n<h1>1. Introduction</h1>\n<blockquote>\n<p>In the traditional client-server authentication model, the client requests an access-restricted resource (protected resource) on the server by authenticating with the server using the resource owner's credentials.</p>\n</blockquote>\n<p>Some problems arise with this approach:</p>\n<ul>\n<li>Third-party apps need to store the resource owner credentials.</li>\n<li>Servers need to support password authentication.</li>\n<li>Third-party apps gain overly broad access to the resource owner's protected resources.</li>\n<li>Resource owners cannot revoke access to specific third-parties.</li>\n<li>Compromise of third-parties results in compromise of resource owner's credentials and the data protected by that credentials.</li>\n</ul>\n<p>OAuth introduces an authorization layer and separates the roles of the client from that of the resource owner. Using OAuth a different set of credentials is issued for the client.</p>\n<p>Instead of using the resource owner's credentials, the client obtains an access tokens. Access tokens are issued to third-party clients by an authorization server with the approval of the resource owner.</p>\n<p><strong>OAuth is designed for use with HTTP.</strong></p>\n<p><strong>OAuth 2.0 is not retrocompatible with OAuth 1.0.</strong></p>\n<h2>1.1 Roles</h2>\n<p>OAuth defines four roles:</p>\n<h3>Resource Owner / End-user</h3>\n<blockquote>\n<p>An entity capable of granting access to a protected resource.</p>\n</blockquote>\n<p>When the resource owner is a person, it is  referred to as an end-user.</p>\n<h3>Resource Server</h3>\n<blockquote>\n<p>The server hosting the protected resources, capable of accepting and responding to protected resource requests using access tokens.</p>\n</blockquote>\n<h3>Client</h3>\n<blockquote>\n<p>An application making protected resource requests on behalf of the resource owner and with its authorization. The term \"client\" does not imply any particular implementation characteristics (e.g., whether the application executes on a server, a desktop, or other devices).</p>\n</blockquote>\n<h3>Authorization Server</h3>\n<blockquote>\n<p>The server issuing access tokens to the client after successfully authenticating the resource owner and obtaining authorization.</p>\n</blockquote>\n<h2>1.2 Protocol flow</h2>\n<ol>\n<li>The <strong>client</strong> requests authorization from the <strong>resource owner</strong>. The authorization request can be directly to the resource owner or preferably indirectly via the authorization server as intermediary.</li>\n<li>The <strong>client</strong> receives an <strong>authorization grant,</strong> which is a credential representing the resource owner's authorization. There are four types of authorization grant defined in this spec, it might exist extensions.</li>\n<li>The <strong>client</strong> requests an <strong>access token</strong> by authenticating with the <strong>authorization server</strong> and presenting the <strong>authorization grant</strong>.</li>\n<li>The <strong>authorization server</strong> authenticates the <strong>client</strong> and validates the <strong>authorization grant,</strong> and if valid, issues an <strong>access token</strong>.</li>\n<li>The <strong>client</strong> requests the protected resource from the <strong>resource server</strong> and authenticates by presenting the <strong>access token</strong>.</li>\n<li>The <strong>resource server</strong> validates the <strong>access token</strong>, and if valid, serves the request.</li>\n</ol>\n<h2>1.3 Authorization Grant</h2>\n<blockquote>\n<p>An authorization grant is a credential representing the resource owner's authorization (to access its protected resources) used by the client to obtain the access token.</p>\n</blockquote>\n<p>The RFC 6749 defines four grant types: authorization code, implicit, resource owner password credentials and client credentials.</p>\n<h3>1.3.1 Authorization Code</h3>\n<p>The <strong>client</strong> redirects the <strong>resource owner</strong> to an a<strong>uthentication server</strong>, which acts as intermediary. The <strong>authentication server</strong> authenticates the <strong>resource owner</strong> and the <strong>resource owner</strong> can authorize the <strong>client</strong>. Once the <strong>client</strong> is authorized, the <strong>authorization server</strong> directs the resource owner back to the client with the <strong>authorization code</strong>.</p>\n<blockquote>\n<p>Because the resource owner only authenticates with the authorization server, the resource owner's credentials are never shared with the client.</p>\n</blockquote>\n<p>Pros:</p>\n<ul>\n<li>Authenticates the client</li>\n<li>Transmission of the token directly to the client</li>\n</ul>\n<p>Cons:</p>\n<ul>\n<li>Two step authentication, therefore, more round trips.</li>\n</ul>\n<h3>1.3.2 Implicit</h3>\n<p><strong>NOT IN THE RFC: Currently this flow is considered legacy due the lack of client authentication.</strong></p>\n<p>The implicit grant is a simplified version of the <strong>authorization code</strong>.</p>\n<blockquote>\n<p>[...] (it's) optimized for clients implemented in a browser using a scripting language such as Javascript.</p>\n</blockquote>\n<p>Instead of the <strong>authentication server</strong> issuing an <strong>authorization code</strong>, in the Implicit Grant flow, the <strong>authentication server</strong> issues directly the <strong>access token</strong>.</p>\n<p>It's called implicit because there is no need of intermediary credentials such as authorization code.</p>\n<p>Cons:</p>\n<ul>\n<li>\n<p>The <strong>authentication server</strong> doesn't authenticates the <strong>client</strong></p>\n<ul>\n<li>It might be possible to identify based on the redirect uri</li>\n</ul>\n</li>\n<li>The <strong>access token</strong> may be exposed to the resource owner or other apps running in the user-agent.</li>\n</ul>\n<p>Pro:</p>\n<ul>\n<li>One step authentication, therefore, less round trips. Improve responsiveness and efficiency.</li>\n</ul>\n<h3>1.3.3 Resource Owner Password Credentials</h3>\n<blockquote>\n<p>The resource owner password credentials(i.e., username and password) can be used directly as an authorization grant to obtain an access token.</p>\n</blockquote>\n<p><strong>Resource owner</strong> password credentials should only be used when there is a high degree of trust between the <strong>resource owner</strong> and the <strong>client</strong> or if you don't have the option for other grant types.</p>\n<p>The <strong>resource owner</strong>'s credentials are used only once to exchange for the access token.</p>\n<blockquote>\n<p>This grant type can eliminate the need for the client to store the resource owner credentials for future use,  by exchanging the credentials with a long-lived access token or refresh token.</p>\n</blockquote>\n<h3>1.3.4 Client Credentials</h3>\n<blockquote>\n<p>The client is acting on its own behalf (the client is also the resource owner)[...]</p>\n</blockquote>\n<p>or </p>\n<blockquote>\n<p>The authorization scope is limited to the protected resources under the control of the client, or to protected resources previously arranged with the authorization server.</p>\n</blockquote>\n<p>Either the <strong>client</strong> is acting as <strong>resource owner</strong>, or the <strong>client</strong>'s authorization scope is restricted to access protected resources which the <strong>client</strong> manages.</p>\n<p>Pro: </p>\n<ul>\n<li>One step authentication, therefore, less round trips. Improve responsiveness and efficiency.</li>\n</ul>\n<h2>1.4 Access Token</h2>\n<blockquote>\n<p>Access tokens are credentials used to access protected resources.</p>\n</blockquote>\n<p>It represents an authorization issue to the client with a specific scope and durations of access. The token is granted by the resource owner and enforced by the resource server and authorization server.</p>\n<p>The access token provides an abstraction layer, replacing different authorization constructs, like username and password with a single token understood by the resource server. It enables to issue token with different authorizations than the authorization grant used to obtain the token.</p>\n<p>Access tokens can have different formats, structures, and methods of utilization.</p>\n<h2>1.5 Refresh Token</h2>\n<p>Refresh token are credentials used to obtain access tokens when the current access token becomes invalid or expires. Or to require tokens with different authorization scope.</p>\n<p>The authorization server issues the refresh token together with the access token. They are intended for us only with authorization servers and are never sent to the resource server.</p>\n<p>A refresh token acts as an authorization granted to the client by the resource owner. </p>\n<ol>\n<li>The client requests an access token by authenticating with the authorization server and presenting an authorization grant.</li>\n<li>The authorization server authenticates the client and validates the authorization grant, and if valid, issues an access token and a refresh token.</li>\n<li>The client can requests a protected resource (sending the access token) and the resource server validates the access token, and if valid, provides the protected resource the client requested.</li>\n<li>(Once the access token expires) The client requests a protected resource and the resource server reject to serve because the access token expired.</li>\n<li>The client requests a new access token by authenticating with the authentication server and presenting the refresh token.</li>\n<li>The authentication server validates the client and its refresh token, and if valid, issues a new access token (and, optionally, a new refresh token).</li>\n</ol>\n<h2>1.6 TLS Version</h2>\n<p>The TLS version varies. At the time of the writing the TLS 1.2 was the most recent, but with a more restricted base of deployments. While the TLS 1.0 provided the broader interoperability.</p>\n<h2>1.7 HTTP Redirections</h2>\n<p>OAuth 2.0 makes heavy usage of HTTP redirection. HTTP redirection code are considered implementation detail.</p>\n<h2>1.8 Interoperability</h2>\n<p>OAuth 2.0 provides a rich authorization framework. The framework is hightly extensible and flexible and is expected to have a wide range of non-interoperable implementations.</p>\n<p>This specification also lets few require components partially or fully undefined. Therefore, it was designed with the clear expectation that future work will come to fulfill this gaps.</p>\n<h2>1.9 Notational conventions</h2>\n<p>Uses keywords as decribed in RFC 2119.</p>\n<h1>2. Client Registration</h1>\n<blockquote>\n<p>Before initiating the protocol, the client registers with the authorization server.</p>\n</blockquote>\n<p>The client registration is beyond the scope of this specification.</p>\n<p>When registering a client, the client must:</p>\n<ul>\n<li>specify the client type</li>\n<li>provide its client redirection URI </li>\n<li>include any other information required by the authorization server</li>\n</ul>\n<h2>2.1 Client Types</h2>\n<p>OAuth 2.0 defines two client types based on their ability to authenticate securely with the authorization server.</p>\n<ul>\n<li>confidential: Clients capable of maintaining the confidentiality of their credentials, or capable of secure client authentication by other means. E.g. secure server.</li>\n<li>public: Clients incapable of maintaining the confidentiality of their credentials, and incapable of secure client authentication via any other means. E.g. native apps or web-broser based.</li>\n</ul>\n<p>A client may be implemented as a distributed set of components, each with a different client type. Either the authorization server should provide support for such clients or the clients should be registered as separeta clients.</p>\n<p>These are the clients profiles considered:</p>\n<ul>\n<li>web application: It's considered a confidential client running on a web server. The client credentials as well as any access token issued to the client are stored on the web server and are not exposed nor accessible by the resource owner. This refers to server side web apps.</li>\n<li>user-agent-based application: It's considered a public client in which the client code is downloaded from a web server and executes within a user-agent on the device used by the resource owner. Protocol data and credentials are easily accessible and often visible to the resource owner. This refers to static site apps.</li>\n<li>native application: It's considered a public client installed and executed on the device used by the resource owner. Protocol data and credentials are easily accessible and often visible to the resource owner.</li>\n</ul>\n<h2>2.2 Client identifier</h2>\n<p>Each client will have its own client identifier issued by the authentication server. The client identifier is not a secret. It's exposed to the resource owner and must not be used to authenticate without additional information.</p>\n<h2>2.3 Client Authentication</h2>\n<p>For confidential clients, the client authenticates with the authentication server. The means of that are provided by the authorization server.</p>\n<p>Authentication servers will issue a set of client credentials for confidential clients (password, public and private key pairs) to use during the client authentication with the authorization server.</p>\n<p>For public clients, it's up to the authentication server to determine whether they will require the client authentication or not. Either way, the authentication server must not rely on this authentication.</p>\n<h3>2.3.1 Client Password</h3>\n<p>Clients with a client password can use the Basic authentication as authentication method with authentication server.</p>\n<p>The request's content type is <code>application/x-www-form-urlencoded</code>.</p>\n<p>The client identifier encoded value used as username.</p>\n<p>The client password encoded value used as password.</p>\n<p>Accordingly to RFC2617:</p>\n<blockquote>\n<p>To receive authorization, the client sends the userid and password, separated by a single colon (\":\") character, within base64 encoded string in the credentials.</p>\n</blockquote>\n<p>Although not recommend, but for cases where basic authentication is not possible, as an alternative the authentication server can support in the request-body the parameters:</p>\n<ul>\n<li>client_id: The client identifier</li>\n<li>client_secret: The client secret</li>\n</ul>\n<p>The endpoints used for client authentication are protected by the authentication server from brute force attacks.</p>\n<h3>2.3.2 Other Authentication Methods</h3>\n<p>The authorization server can support other authentication methods. But, when using other client authentication methods, it's required to have a mapping of the client identifier to the authentication scheme.</p>\n<h2>2.4 Unregistered Clients</h2>\n<p>It's beyond the scope of this RFC.</p>\n<h1>3. Protocol Endpoints</h1>\n<p>There are 3 endpoints to consider:</p>\n<ul>\n<li>\n<p>Authorization server endpoints</p>\n<ul>\n<li>Authorization endpoint: used by the client to obtain the authorization grant.</li>\n<li>Token endpoint: used by the client to exchange the authorization grant for an access token.</li>\n</ul>\n</li>\n<li>\n<p>Client endpoint</p>\n<ul>\n<li>Redirection endpoint: The authorization server redirects the resource owner to it with the authorization grant.</li>\n</ul>\n</li>\n</ul>\n<h2>3.1 Authorization Endpoint</h2>\n<blockquote>\n<p>The authorization endpoint is used to interact with the resource owner and obtain an authorization grant. </p>\n</blockquote>\n<h3>3.1.1 Response Type</h3>\n<blockquote>\n<p>The authorization endpoint is used by the authorization code grant type and implicit grant type flows.</p>\n</blockquote>\n<p>The client must inform the <code>response_type</code> as a HTTP parameter specifying either <code>code</code> for the authorization code grant type or <code>token</code> for an implicit grant type.</p>\n<h3>3.1.2 Redirection Endpoint</h3>\n<p>After the Resource owner authentication, the authentication servers redirects the client to the client's endpoint. The client's endpoint was previously established with the authorization server, usually during the client registration in the authorization server.</p>\n<h4>3.1.2.1 Endpoint Request Confidentiality</h4>\n<p>Redirection endpoint should use TLS (HTTPS) if the authentication server will send sensitive credentials, tokens or code.</p>\n<h4>3.1.2.2 Registration Requirements</h4>\n<p>Both public clients and any client using implicit grant type flow are required to register redirection endpoints.</p>\n<h4>3.1.2.3 Dynamic Configuration</h4>\n<p>If the client didn't configured, or misconfigured the redirection endpoint. It can still send it through the <code>redirect_url</code> request parameter.</p>\n<h4>3.1.2.4 Invalid Endpoint</h4>\n<p>In case the redirect fails, the resource owner is notified and the authorization server does not redirects.</p>\n<h4>3.1.2.5 Endpoint Content</h4>\n<blockquote>\n<p>The redirection request to the client’s endpoint typically results in an HTML document response, processed by the user-agent. If the HTML response is served directly as the result of the redirection request, any script included in the HTML document will execute with full access to the redirection URI and the credentials it contains.</p>\n</blockquote>\n<p>Therefore, the client should extract the credentials and redirect again to another endpoint without exposing the credentials. Also this initial page should not have any third party scripts.</p>\n<h2>3.2 Token Endpoint</h2>\n<blockquote>\n<p>The token endpoint is used by the client to obtain an access token by presenting its authorization grant or refresh token.</p>\n</blockquote>\n<p>The endpoint uses the HTTP \"Post\" method.</p>\n<h3>3.2.1 Client Authentication</h3>\n<p>Confidential clients or clients with client credentials must authenticate in the authentication server.</p>\n<p>That way even if the authorization code was issued through an insecure channel, only with the client authentication the authorization code will generate the access token.</p>\n<h2>3.3 Access Token Scope</h2>\n<blockquote>\n<p>The authorization and token endpoints allow the client to specify the scope of the access request using the \"scope\" request parameter. In turn, the authorization server uses the \"scope\" response parameter to inform the client of the scope of the access token issued.</p>\n</blockquote>\n<h1>4. Obtaining Authorization</h1>\n<h2>4.1 Authorization Code Grant</h2>\n<blockquote>\n<p>The authorization code grant type is used to obtain both access tokens and refresh tokens and is optimized for confidential clients.</p>\n</blockquote>\n<blockquote>\n<p>this is a redirection-based flow, the client must be capable of interacting with the resource owner’s user-agent (typically a web browser) and capable of receiving incoming requests (via redirection) from the authorization server.</p>\n</blockquote>\n<h3>4.1.1 Authorization Request</h3>\n<p>Request parameters must be encoded by <code>\"application/x-www-form-urlencoded\"</code>.</p>\n<ul>\n<li><code>response_type</code> - required: Defines which is the authorization grant flow to be used. This RFC defines both <code>code</code> and <code>token</code> representing respectively authorization code and implicit flow.</li>\n<li><code>client_id</code> - required: The id of the Client. Every client is issued an id when they do the registration in the authorization server.</li>\n<li><code>redirect_uri</code>- optional: The URI where the authorization server will redirect the client after the authorization is granted.</li>\n<li><code>scope</code> - optional: Client's scope request which might not be fully attended by the authorization server.</li>\n<li><code>state</code> - recommended: A value sent in order to ensure the code received in the redirect is from the same state you sent in your <code>/authorize</code> request.</li>\n</ul>\n<blockquote>\n<p>The authorization server validates the request to ensure that all required parameters are present and valid. </p>\n</blockquote>\n<p>See section 2.3 for more information about client authentication.</p>\n<h3>4.1.2 Authorization Response</h3>\n<blockquote>\n<p>The authorization server issues an authorization code and delivers it to the client by adding the following parameters to the query component of the redirection URI using the \"application/x-www-form-urlencoded\"</p>\n</blockquote>\n<ul>\n<li><code>code</code> - required: The authorization code generated by the authorization server bound to the client id and redirect uri.</li>\n<li><code>state</code> - required if Client sent this data: Same value sent by the client.</li>\n</ul>\n<h4>4.1.2.1 Error Response</h4>\n<blockquote>\n<p>If the request fails due to a missing, invalid, or mismatching redirection URI, or if the client identifier is missing or invalid, the authorization server SHOULD inform the resource owner of the error and MUST NOT automatically redirect the user-agent to the invalid redirection URI.</p>\n</blockquote>\n<ul>\n<li><code>error</code> - required: One of the following: invalid<em>request, unauthorized</em>client, access<em>denied, unsupported</em>response<em>type, invalid</em>scope, server<em>error or temporarily</em>unavailable.</li>\n<li><code>error_description</code> - optional: Human readable information.</li>\n<li><code>error_uri</code> - optional: An URI with more information about the error.</li>\n<li><code>state</code> - required if Client sent this data: Same value sent by the client.</li>\n</ul>\n<h3>4.1.3 Access Token Request</h3>\n<blockquote>\n<p>The token endpoint is used by the client to obtain an access token by presenting its authorization grant or refresh token.</p>\n</blockquote>\n<ul>\n<li><code>grant_type</code> - required: Which type of authorization grant you are using. <code>authorization_code</code></li>\n<li><code>code</code> - required: The authorization code received from the authorization server</li>\n<li><code>redirect_uri</code> required if the \"redirect_uri\" parameter was included in the authorization request: Same value as the value sent in the authorization request.</li>\n<li><code>client_id</code> required: The client identifier.</li>\n</ul>\n<h3>4.1.4 Access Token Response</h3>\n<blockquote>\n<p>If the access token request is valid and authorized, the authorization server issues an access token and optional refresh token</p>\n</blockquote>\n<h2>4.2 Implicit Grant</h2>\n<p>Deprecated</p>\n<h2>4.3 Resource Owner Password Credentials Grant</h2>\n<p>Deprecated</p>\n<h2>4.4 Client Credentials Grant</h2>\n<blockquote>\n<p>The client can request an access token using only its client credentials </p>\n</blockquote>\n<blockquote>\n<p>The client credentials grant type MUST only be used by confidential clients.</p>\n</blockquote>\n<h3>4.4.2 Access Token Request</h3>\n<ul>\n<li><code>grant_type</code> - required: Which type of authorization grant you are using. <code>client_credentials</code></li>\n<li><code>scope</code> - optional: Client's scope request which might not be fully attended by the authorization server.</li>\n</ul>\n<blockquote>\n<p> The authorization server MUST authenticate the client.</p>\n</blockquote>\n<p>See section 2.3 for more information about client authentication.</p>\n<h2>4.5. Extension Grants</h2>\n<blockquote>\n<p>The client uses an extension grant type by specifying the grant type using an absolute URI (defined by the authorization server) as the value of the \"grant_type\" parameter of the token endpoint, and by adding any additional parameters necessary</p>\n</blockquote>\n<h1>5. Issuing an Access Token</h1>\n<blockquote>\n<p>If the access token request is valid and authorized, the authorization server issues an access token and optional refresh token</p>\n</blockquote>\n<h2>5.1 Successful Response</h2>\n<ul>\n<li><code>access_token</code> - required: The access token issued by the authorization server.</li>\n<li><code>token_type</code> - required: The type of the token issued</li>\n<li><code>expires_in</code> - recommended: The lifetime in seconds of the access token</li>\n<li><code>refresh_token</code> - optional: It can be used to obtain new access tokens using the same authorization grant. Must not be availabe in the client credentials grant flow.</li>\n<li><code>scope</code> - optional, if identical to the scope requested by the client; otherwise, required. The scope of the access token.</li>\n</ul>\n<blockquote>\n<p>The parameters are included in the entity-body of the HTTP response using the \"application/json\" media type</p>\n</blockquote>","frontmatter":{"title":"RFC6749 - The OAuth 2.0 Authorization Framework","language":"en-US","coverPath":null,"status":"Read","date":"2018-06-01"}}},{"node":{"html":"<p><a href=\"https://tools.ietf.org/html/rfc7521\">RFC7521 - Assertion Framework for OAuth 2.0 Client Authentication and Authorization Grants</a></p>\n<h1>Abstract</h1>\n<blockquote>\n<p>[...] provides a framework for the use of assertions with OAuth 2.0 in the form of a new client authentication mechanism and a new authorization grant type.</p>\n</blockquote>\n<blockquote>\n<p>[...] to provide a common framework for OAuth 2.0 to interwork with other identity systems using assertions and to provide alternative client authentication mechanism.</p>\n</blockquote>\n<h1>1. Introduction</h1>\n<blockquote>\n<p>An assertion is a package of information that facilitates the sharing of identity and security information across security domains.</p>\n</blockquote>\n<p>This specification provides two functionalities:</p>\n<ul>\n<li>a framework that uses packages of informations (assertions) as a credential representing the resource owner's authorization (authorization grant) with OAuth 2.0.</li>\n<li>a framework for assertions to be used for client authentication.</li>\n</ul>\n<p>You'll also find:</p>\n<blockquote>\n<p>[...] generic mechanisms for transporting assertions during interactions with an authorization server's token endpoint</p>\n</blockquote>\n<blockquote>\n<p>[...] general rules for the content and processing of those assertions.</p>\n</blockquote>\n<p>The intent behind adding this two functionalities are two:</p>\n<ul>\n<li>Alternative client authentication mechanism (one that doesn't send client secrets).</li>\n<li>Facilitate the use of OAuth 2.0 in client-server integration scenarios, where the end user may not be present.</li>\n</ul>\n<p>Using the assertions as authorization grant is separable from using the assertion from client authentication. Each functionality can be used on its own.</p>\n<blockquote>\n<p>Client assertion authentication is nothing more than an alternative way for a client to authenticate to the token endpoint.</p>\n</blockquote>\n<h1>2. Notational Conventions</h1>\n<p>Uses keywords as decribed in RFC 2119.</p>\n<h1>3. Framework</h1>\n<p>An assertion typically contains:</p>\n<ul>\n<li>information about a subject or principal</li>\n<li>information about the party that issued it</li>\n<li>when it was issued </li>\n<li>under which conditions the assertion is to be considered valid</li>\n</ul>\n<p>The entity that creates and signs or integrity-protects it is know as the \"Issuer\".</p>\n<p>The entity that consumes the assertion is known the \"Relying party\". In the context of this document, the authorization server acts as a Relying party.</p>\n<p>The issuer must protect the integrity of an assertion by applying a digital signature or a Message Authentication Code (MAC). Its intent is to identify the issuer and ensure the assertion's content integrity.</p>\n<p>In addition, an assertion might be encrypted.</p>\n<p>The specification doesn't define how the Client obtains the assertion, but it suggests two main ways:</p>\n<ol>\n<li>Assertion created by a third party</li>\n</ol>\n<p>The Client obtains an assertion from a third-party entity capable of issuing, renewing, transforming, and validating security tokens. This entity is known as \"security token service\" (STS) or just \"token service\". </p>\n<p>A trust relationship exists between the STS and the relying party (authorization service).</p>\n<ol start=\"2\">\n<li>Self issued assertion</li>\n</ol>\n<p>The Client creates assertions locally.</p>\n<p>Self-issued assertions can be used to demonstrate knowledge of some secret, such as a Client secret, without actually communicating the secret directly in the transaction.</p>\n<p>In the point of view of the entity presenting the assertion, there are two types of assertion:</p>\n<ol>\n<li>Bearer Assertion: Any entity in possession of a bearer assertion (the bearer) can use it to get access to the associated resources (without demonstrating possession of a cryptographic key)</li>\n<li>Holder-of-key Assertion: To access the associated resources, the entity presenting the assertion must demonstrate possession of additional cryptographic material. The STS thereby binds a key identifier to the assertion, and the Client has to demonstrate to the Relying Party knowledge about it.</li>\n</ol>\n<p>The scope of this specification only covers a Client presenting a bearer assertion to an authorization server.</p>\n<h1>4. Transporting Assertions</h1>\n<p>Is mandatory to use HTTPS.</p>\n<h2>4.1. Using assertions as Authorization Grants</h2>\n<p>When using assertions as authorization grants, the client includes the assertion and related information using the following HTTP request parameters:</p>\n<ul>\n<li><code>grant_type</code>: REQUIRED. The format of the assertion.</li>\n<li><code>assertion</code>: REQUIRED. The assertion being used as an authorization grant.</li>\n<li><code>scope</code>: OPTIONAL. The request scope must be equal or less than the scope originally granted to the authorized accessor.</li>\n</ul>\n<p>The authentication of the client is optional. Therefore, <code>client_id</code> is only required when the client authentication relies on this parameter.</p>\n<p>Example:</p>\n<pre><code>POST /token HTTP/1.1\nHost: server.example.com\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=urn:ietf:params:oauth:grant-type:saml2-bearer&#x26;\nassertion={assertion-goes-here}\n</code></pre>\n<p>An assertion used in this context is a short lived representation of the authorization grant. The access token issued by authorization server should not have a lifespan way bigger than its assertion validity.</p>\n<h3>4.1.1. Error Response</h3>\n<p>If an assertion is not valid or has expired the value of the <code>error</code> parameter must be the <code>invalid_grant</code>.</p>\n<h2>4.2 Using Assertions for Client Authentication</h2>\n<p>When using assertions as client credentials, the client includes the assertion and related information using the following HTTP request parameters:</p>\n<ul>\n<li><code>client_assertion_type</code>: REQUIRED. The format of the assertion.</li>\n<li><code>client_assertion</code>: REQUIRED. The assertion being used to authenticate the client.</li>\n<li><code>client_id</code>: OPTIONAL. It's unnecessary, the client is already identified by the subject of the assertion. If specified, it must be the same as identified by the client assertion.</li>\n</ul>\n<p>Example:</p>\n<pre><code>POST /token HTTP/1.1\nHost: server.example.com\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=authorization_code&#x26;\ncode={{authorization_code_goes_here}}&#x26;\nclient_assertion_type=urn:ietf:params:oauth:client-assertion-type:saml2-bearer&#x26;\nclient_assertion={client_assertion-goes-here}\n</code></pre>\n<h3>4.2.1. Error Response</h3>\n<p>If an assertion is invalid for any reason or if more than one client authentication mechanism is used the value of the <code>error</code> parameter must be the <code>invalid_client</code>.</p>\n<h1>5. Assertion Content and Processing</h1>\n<p>Provides general content and processing model for the use of assertions with OAuth 2.0.</p>\n<h2>5.1. Assertion Metamodel</h2>\n<p>The following are general terms abstract from any particular assertion format.</p>\n<h3>Issuer</h3>\n<p>A unique identifier for the entity that issued the assertion. It holds the key material used to sign or integrity-protect the assertion. OAuth Clients (self-issued assertions) and STS are examples.</p>\n<h3>Subject</h3>\n<p>A unique identifier for the principal that is the subject of the assertion. Two possible interpretations:</p>\n<ul>\n<li>Assertions during client authentication: Identifies the client to the authorization server using the value in the <code>client_id</code>.</li>\n<li>Assertions during authorization grant: Identifies an authorized accessor for which the access token is being requested.</li>\n</ul>\n<h3>Audience</h3>\n<p>Identifies the party or parties intended to process the assertion. The URL of the token endpoint can be used to indicate the authorization server as a valid intended audience.</p>\n<h3>Issued At</h3>\n<p>Time at which the assertion was issued. UTC format.</p>\n<h3>Expires At</h3>\n<p> Time at which the assertion expires. UTC format.</p>\n<h3>Assertion ID</h3>\n<p>An nonce or unique identifier for the assertion.</p>\n<h2>5.2. General Assertion Format and Processing Rules</h2>\n<p>The assertion must contain:</p>\n<ul>\n<li>Issuer</li>\n<li>Subject</li>\n<li>Audience: It must identify the authorization server as the intended audience.</li>\n<li>Expires At: Authorization server must reject assertions that are expired.</li>\n<li>Issued At</li>\n</ul>\n<h1>6. Common Scenarios</h1>\n<h2>6.1. Client Authentication</h2>\n<p>A client uses an assertion to authenticate to the authorization server's token endpoint (as described in 4.2).</p>\n<p>The assertion's Subject is the client.</p>\n<p>If it's a self-issued assertion the assertion's Issuer is also the client.</p>\n<h2>6.2. Client Acting on Behalf of Itself</h2>\n<p>A client is accessing resources on behalf of itself. Analogous to the Client Credentials Grant. It combines both the authentication an authorization grant usage patterns.</p>\n<p>The interaction with the authorization server is treated as using an assertion for Client authentication (as described in 4.2), while using the <code>client_credentials</code> as <code>grant_type</code> to indicate that the client is requesting an access token for itself.</p>\n<p>Example:</p>\n<pre><code>POST /token HTTP/1.1\nHost: server.example.com\nContent-Type: application/x-www-form-urlencoded\n\ngrant_type=client_credentials&#x26;\nclient_assertion_type=urn:ietf:params:oauth:client-assertion-type:saml2-bearer&#x26;\nclient_assertion={client_assertion-goes-here}\n</code></pre>\n<h2>6.3. Client Acting on Behalf of a User</h2>\n<p>A client is accessing a resource on behalf of a user. It has to inform the <code>assertion</code> as the <code>grant_type</code> parameter (as described in 4.1). The Subject identifies an authorized accessor, typically the resource owner or an authorized delegate.</p>\n<h3>6.3.1. Client Acting on Behalf of an Anonymous User</h3>\n<p>A client is accessing resources on behalf of an anonymous user. A mutually agree-upon Subject identifier indicating anonymity is used.</p>\n<p>Authorization may be based upon additional criteria, such as additional attributes or claims provided in the assertion.</p>\n<h1>7. Interoperability Considerations</h1>\n<blockquote>\n<p>On its own, this specification is not sufficient to produce interoperable implementations.</p>\n</blockquote>\n<p>The RFC 7522 and RFC 7523 specify additional details about assertions encoding and processing rules.</p>\n<p>Specific items that require agreement are:</p>\n<ul>\n<li>Values for the Issuer and Audience identifier</li>\n<li>Supported assertion and client authentication types</li>\n<li>The location of the token endpoint</li>\n<li>The key used to apply and verify the diginal signature or MAC over assertions</li>\n<li>Maximum assertion lifetime allowed</li>\n<li>Subject and attribute requirements </li>\n</ul>\n<h1>8. Security Considerations</h1>\n<h2>8.1. Forged Assertion</h2>\n<p>Threat: An adversary could forge or alter an assertion in order to obtain an acces token (as described in 4.1) or to impersonate a client (as described in 4.2).</p>\n<p>Countermeasure: Apply mechanisms for protecting the integrity of the assertion (digital signature or MAC).</p>\n<h2>8.2. Stolen Assertion</h2>\n<p>Threat: An adversary might obtain an assertion and then reuse it later in time.</p>\n<p>Countermeasure: Use secure communication channels with server authentication for all network exchanges.</p>\n<h2>8.3. Unauthorized Disclosure of Personal Information</h2>\n<p>Threat: Other entities to obtain information about an individual (role in an organization, authentication information, ...)</p>\n<p>Countermeasure: Use of TLS and encrypt assertion.</p>\n<h2>8.4. Privacy Considerations</h2>\n<p>An assertion may contain privacy-sensitive information, therefore, use TLS and encrypt this information.</p>\n<h1>9. IANA Considerations</h1>\n<p>Register of <code>assertion</code>, <code>client_assertion</code>, <code>client_assertion_type</code>.</p>","frontmatter":{"title":"RFC7521 - Assertion Framework for OAuth 2.0 Client Authentication and Authorization Grants","language":"en-US","coverPath":null,"status":"Read","date":"2018-06-01"}}},{"node":{"html":"<p><a href=\"https://tools.ietf.org/html/rfc7522\">Security Assertion Markup Language (SAML) 2.0 Profile for OAuth 2.0 Client Authentication and Authorization Grants</a></p>\n<h1>1. Introduction</h1>\n<p>The Security Assertion Markup Language 2.0 is an XML-based framework that allows identity and security information to be shared across security domains.</p>\n<p>SAML is defined by another specification. <code>OASIS.saml-core-2.0-os</code>.</p>\n<p>SAML is primarily targeted at providing cross domain Web browser single sign-on.</p>\n<p>The assertion, an XML security token, defined by the SAML specification is often used by other protocols and specs.</p>\n<p>An Identity Provider issues Assertions.</p>\n<p>A Service Provider relies on its content to identify the Assertion's subject for security-related purposes.</p>\n<p>After that, the specification gives a small overview about OAuth 2.0 (RFC 6749) and the use of assertions as authentication grant/ client credentials (RFC 7521).</p>\n<p>This specification defines two main things:</p>\n<ol>\n<li>How a SAML assertion can be used to request an access token when a client wishes to utilize an existing trust relationship (SAML Assertion), without a direct user approval step at the authorization server.</li>\n<li>How a SAML can be used as a client authentication mechanism.</li>\n</ol>\n<h2>1.1. Notational Conventions</h2>\n<p>Uses keywords as described in RFC 2119.</p>\n<h2>1.2. Terminology</h2>\n<p>As described in the RFC6749, RFC 7521 and OASIS.saml-core-2.0-os.</p>\n<h1>2. HTTP Parameter Bindings for Transporting Assertions</h1>\n<p>Same parameters as defined in the RFC7521.</p>\n<h2>2.1. Using SAML Assertions as Authorization Grants</h2>\n<p>As described in the section 4.1 of the RFC7521.</p>\n<p>The value of <code>grant_type</code> parameter is <code>urn:ietf:params:oauth:grant-type:saml2-bearer</code>.</p>\n<p>The value of <code>assertion</code> parameter is contains a single SAML 2.0 Assertion encoded by <code>base64url</code>.</p>\n<p>The <code>scope</code> parameter may be used to indicate the request scope.</p>\n<p>Authentication of the client is optional.</p>\n<p>Same example as RFC7521 in the section 4.1.</p>\n<h2>2.2. Using SAML Assertions for Client Authentication</h2>\n<p>As described in the section 4.2 of the RFC7522.</p>\n<p>The value of <code>client_assertion_type</code> parameter is <code>urn:ietf:params:oauth:client-assertion-type:saml2-bearer</code></p>\n<p>The value of <code>client_assertion</code> parameter contains a single SAML 2.0 Assertion encoded by <code>base64url</code>.</p>\n<p>Same example as RFC7521 in the section 4.2.</p>\n<h1>3. Assertion format and Processing Requirements</h1>\n<p>Authorization server must validate the Assertion according to the criteria below.</p>\n<ol>\n<li>Assertion's Issuer has an unique identifier.</li>\n<li>It must have an <code>&#x3C;Conditions></code> element with an <code>&#x3C;AudiencRestriction></code> element with an <code>&#x3C;Audience></code> element that identifies the authorization server as audience.</li>\n<li>\n<p>The Assertion must have an <code>&#x3C;Subject></code> element that identifies the principal that is the subject of the Assertion.</p>\n<ol>\n<li>Authorization grant: Resource Owner or authorized delegate.</li>\n<li>Client authentication: the <code>client_id</code>.</li>\n</ol>\n</li>\n<li>The assertion must have expiry. Either as the <code>&#x3C;Conditions NotOnOrAfter=\"\"></code> or <code>&#x3C;SubjectConfirmationData NotOnOrAfter=\"\"></code>.</li>\n<li>The <code>&#x3C;Subject></code>must have at least one <code>&#x3C;SubjectConfirmation></code>.</li>\n<li>The authorization server rejects the Assertion if the expiry instant has passed.</li>\n<li>Should contain a single <code>&#x3C;AuthnStatement></code> if the assertion issuer directly authenticated the subject.</li>\n<li>May have <code>AttributeStatement</code>.</li>\n<li>The Assertion must be digitally signed or have a MAC applied by the issuer.</li>\n<li>The assertion may have encrypted elements.</li>\n<li>The authorization server must reject an Assertion that is not valid in all other respects per SAML specification.</li>\n</ol>\n<h2>3.1. Authorization Grant Processing</h2>\n<p>Assertion authorization grant may be used with or without client authentication or identification. </p>\n<p>If client credentials are presented in the request, the authorization server must validate them.</p>\n<p>If the assertion is not valid, the server creates a error response as defined by the OAuth 2.0.</p>\n<p>The <code>error</code> parameter must be <code>invalid_grant</code>.</p>\n<h2>3.2. Client Authorization Processing</h2>\n<p>If the client authorization is not valid the authorization server response follows the OAuth 2.0 structure.</p>\n<p>The <code>error</code> parameter must be <code>ìnvalid_client</code>.</p>\n<h1>4. Authorization Grant Example</h1>\n<p>The Assertion is issued and signed by the SAML Identity Provider (\"<a href=\"https://saml-idp.example.com%22\">https://saml-idp.example.com\"</a>).</p>\n<p>The subject of the assertion is identified by email address (\"brian@example.com\"). He authenticated to the Identity Provider through digital signature.</p>\n<p>The intended audience is \"<a href=\"https://saml-sp.example.net%22\">https://saml-sp.example.net\"</a>, which is an SAML Service Provider. The authorization service identifies itself to the SAML Service Provider.</p>\n<p>The assertion is sent as part of an access token request to the authorization server.</p>\n<p>The specification contains an assertion as example.</p>\n<h1>5. Interoperability Considerations</h1>\n<p>Very similar to the RFC 7521.</p>\n<h1>6. Security Considerations</h1>\n<p>Same security considerations as the applicable to the RFC 6749, RFC 7521 and SAML 2.0.</p>\n<h1>7. Privacy Considerations</h1>\n<p>An assertion may contain privacy-sensitive information, therefore, use TLS and encrypt this information.</p>\n<h1>8. IANA</h1>\n<p>Registers the values <code>grant-type:saml2-bearer</code> and <code>client-assertion-type:saml2-bearer</code> .</p>","frontmatter":{"title":"RFC7522 - Security Assertion Markup Language (SAML) 2.0 Profile for OAuth 2.0 Client Authentication and Authorization Grants","language":"en-US","coverPath":null,"status":"Read","date":"2018-06-01"}}},{"node":{"html":"<h1>Foreword by Mike Cohn</h1>\n<blockquote>\n<p> Build quality into the product rather than trying to test it in later.</p>\n</blockquote>\n<h1>Preface</h1>\n<h2>Why We Wrote This Book</h2>\n<blockquote>\n<p>delivering business value using tests that the business can understand. </p>\n</blockquote>\n<blockquote>\n<p>Agile development isn’t the only way to successfully deliver software.</p>\n</blockquote>\n<blockquote>\n<p>The programmers write and automate unit and integration tests that provide good code coverage. They are disciplined in the use of source code control and code integration. Skilled testers are involved from the start of the development cycle and are given time and resources to do an adequate job of all necessary forms of testing. An automated regression suite that covers the system functionality at a higher level is run and checked regularly. The development team understands the customers’ jobs and their needs, and works closely together with the business experts.</p>\n</blockquote>\n<h2>How to Use This Book</h2>\n<blockquote>\n<p>Test automation is a central focus of successful agile teams, and it’s a scary topic for lots of people</p>\n</blockquote>\n<h2>Just Start Doing It - Today!</h2>\n<blockquote>\n<p>... what he thought was the number one success factor for agile testing, he answered: “Start doing it—today!\"</p>\n</blockquote>\n<h1>Part I Introduction</h1>\n<h2>Chapter 1 What Is Agile Testing, Anyway?</h2>\n<blockquote>\n<p> The “whole team” approach promoted by agile development is central to our attitude toward quality and testing,</p>\n</blockquote>\n<h3>What Do We Mean by \"Agile Testing\"?</h3>\n<blockquote>\n<p>With TDD, the programmer writes a test for a tiny bit of functionality, sees it fail, writes the code that makes it pass, and then moves on to the next tiny bit of functionality.</p>\n</blockquote>\n<blockquote>\n<p>Programmers also write code integration tests to make sure the small units of code work together as intended.</p>\n</blockquote>\n<blockquote>\n<p>(About unit or component testing) ...helping the programmers know what code to write next.</p>\n</blockquote>\n<blockquote>\n<p>Agile development encourages us to solve our problems as a team.</p>\n</blockquote>\n<blockquote>\n<p>we’re working together with a team of people who all feel responsible for delivering the best possible quality, and who are all focused on testing.</p>\n</blockquote>\n<blockquote>\n<p>When we say \"agile testing\" in this book, we're usually talking about business-facing tests, tests that define the business experts' desired features and functionalities. \"Testing\" in this book (...) includes just about everything beyond unit and component level testing: functional, system, load, performance, security, stress, usability, exploratory, end-to-end, and user-acceptance.</p>\n</blockquote>\n<blockquote>\n<p>Testing an application with a plan to learn about it as you go, and letting that information guide your testing,</p>\n</blockquote>\n<h3>A Little Context for Roles and Activities on an Agile Team</h3>\n<h4>Customer Team</h4>\n<blockquote>\n<p>The customer team writes the stories or feature sets that the developer team delivers.</p>\n</blockquote>\n<blockquote>\n<p>They provide examples that will drive coding in the form of business-facing tests.</p>\n</blockquote>\n<blockquote>\n<p>helping elicit requirements and examples and helping the customers express their requirements as tests.</p>\n</blockquote>\n<h4>Developer Team</h4>\n<blockquote>\n<p>Everyone involved with delivering code is a developer</p>\n</blockquote>\n<blockquote>\n<p>Many agile practitioners discourage specialized roles on teams and encourage all team members to transfer their skills to others as much as possible.</p>\n</blockquote>\n<blockquote>\n<p>Testers advocate for quality on behalf of the customer and assist the development team in delivering the maximum business value.</p>\n</blockquote>\n<h4>Interaction between Customer and Developer Teams</h4>\n<blockquote>\n<p>The customer and developer teams work closely together at all times.</p>\n</blockquote>\n<blockquote>\n<p>Ideally, they’re just one team with a common goal.</p>\n</blockquote>\n<blockquote>\n<p>They’ll work together to define requirements with tests and examples, and write the code that makes the tests pass.</p>\n</blockquote>\n<h3>Traditional vs Agile Testing</h3>\n<blockquote>\n<p>someone will need to write tests that illustrate the requirements for each story days or hours before coding begins.</p>\n</blockquote>\n<blockquote>\n<p>The real difference is that we like to do these tests as early in the development process as we can so that they can also drive design and coding.</p>\n</blockquote>\n<blockquote>\n<p>The difference here is that all the testing is not left until the end.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This chapter explains what is agile testing and who are the person responsible for it and their roles.</p>\n<p>Writing automated tests and applying TDD is not necessarily an agile testing practice. Even, teams that don't consider themselves agile could apply these techniques simply because they improve software design and prevent defects. This technology facing tests are part of the programmer's domain.</p>\n<p>Agile development and testing is about working closely together and being responsible for delivering the best quality product to customers.</p>\n<p>The author defines \"Agile testing\" as business facing tests. Tests that define the business expert domain.</p>\n<p>They also define \"Testing\" is something more broad, from functional, system, load, performance, security, stress, usability, exploratory, e2e and user acceptance. And, all other types of tests.</p>\n<p>Agile teams, composed of customer and developer teams, should collaborate as they are a whole team, in order to achieve business goals. The main difference from agile and phased (traditional ) development is that agile brings tests as early as possible in the development process, while in phased development tests are one of the last phases.</p>\n<h2>Chapter 2 Ten Principles for Agile Testers</h2>\n<h3>What's an Agile Tester?</h3>\n<blockquote>\n<p>a professional tester who embraces change, collaborates well with both technical and business people, and understands the concept of using tests to document requirements and drive development. Agile testers tend to have good technical skills, know how to collaborate with others to automate tests, and are also experienced exploratory testers.</p>\n</blockquote>\n<h3>The Agile Testing Mind-Set</h3>\n<blockquote>\n<p>To us, an agile team is one that continually focuses on doing its best work and delivering the best possible product.</p>\n</blockquote>\n<blockquote>\n<p>She's ready to gather and share information, to work with the customer or product owner in order to help them express their requirements adequately so that they can get the features they need, and to provide feedback on project progress to everyone.</p>\n</blockquote>\n<blockquote>\n<p>Creativity, openness to ideas, willingness to take on any task or role, focus on the customer, and a constant view of the big picture are just some components of the agile testing mind-set.</p>\n</blockquote>\n<h3>Applying Agile Principles and Values</h3>\n<blockquote>\n<ul>\n<li>Provide Continuous Feedback</li>\n<li>Deliver Value to the Customer</li>\n<li>Enable Face-to-Face Communication</li>\n<li>Have Courage</li>\n<li>Keep It Simple</li>\n<li>Practice Continuous Improvement</li>\n<li>Respond to Change</li>\n<li>Self-Organize</li>\n<li>Focus on People</li>\n<li>Enjoy</li>\n</ul>\n</blockquote>\n<h4>Provide Continuous Feedback</h4>\n<blockquote>\n<p>One of the agile tester’s most important contributions is helping the product owner or customer articulate requirements for each story in the form of examples and tests.</p>\n</blockquote>\n<blockquote>\n<p>run these tests early and often so they’re continually guided by meaningful feedback.</p>\n</blockquote>\n<blockquote>\n<p>Is management worried about how work is progressing? Display a big visible chart of tests written, run, and passing every day.</p>\n</blockquote>\n<h4>Deliver Value to the Customer</h4>\n<blockquote>\n<p>If we let new features creep in, we risk delivering nothing on time.</p>\n</blockquote>\n<blockquote>\n<p>If we get too caught up with edge cases and miss core functionality on the happy path, we won’t provide the value the business needs.</p>\n</blockquote>\n<blockquote>\n<p>The worst-case scenario is that only the core functionality gets released. That’s better than delivering nothing or something that works only halfway. </p>\n</blockquote>\n<blockquote>\n<p>we still need to start by making sure the happy path works.</p>\n</blockquote>\n<blockquote>\n<p>and add negative and boundary tests later.</p>\n</blockquote>\n<h4>Enable Face-to-Face Communication</h4>\n<blockquote>\n<p>The agile tester should look for unique ways to facilitate communication.</p>\n</blockquote>\n<blockquote>\n<p>We found great success with the “Power of Three.” This meant that all discussions about a feature needed a programmer, a tester, and the product owner.</p>\n</blockquote>\n<h4>Have Courage</h4>\n<blockquote>\n<p>We need courage to allow others to make mistakes, because that’s the only way to learn the lesson.</p>\n</blockquote>\n<h4>Keep It Simple</h4>\n<blockquote>\n<p>take a simple approach to ensuring that software meets the customer requirements.</p>\n</blockquote>\n<blockquote>\n<p>because it’s up to the customer team to decide what level of quality they want to pay for.</p>\n</blockquote>\n<blockquote>\n<p>The ultimate decisions are up to the customer. The team can help the customer make good decisions by its taking a simple, step-by-step approach to its work.</p>\n</blockquote>\n<blockquote>\n<p>Agile testing means doing the simplest tests possible to verify that a piece of functionality exists or that the customer’s quality standard (eg., performance) has been met.</p>\n</blockquote>\n<blockquote>\n<p>Simple doesn’t mean easy. For testers, it means testing “just enough” with the lightest-weight tools and techniques we can find that will do the job.</p>\n</blockquote>\n<blockquote>\n<p>We need to automate regression tests, but we should push them down to the lowest level possible in order to encourage fast feedback. Even simple smoke tests might be enough for business-facing test automation.</p>\n</blockquote>\n<blockquote>\n<p>but start with the basics, time-boxing side trips and evaluating how far to go with edge cases.</p>\n</blockquote>\n<h4>Practice Continuous Improvement</h4>\n<blockquote>\n<p>Testers bring testing issues up for the whole team to address. </p>\n</blockquote>\n<blockquote>\n<p>Agile testers and their teams are always on the lookout for tools, skills, or practices that might help them add more value or get a better return on the customer’s investment. </p>\n</blockquote>\n<blockquote>\n<p>“AADD,” Agile Attention Deficit Disorder. Anything not learned quickly might be deemed useless. </p>\n</blockquote>\n<blockquote>\n<p>Agile testers use this opportunity to raise testing-related issues and ask the team to brainstorm ways to address them. </p>\n</blockquote>\n<blockquote>\n<p>I suggested keeping an “impediment backlog” of items that were keeping us from being as productive as we’d like to be. </p>\n</blockquote>\n<h4>Respond to Change</h4>\n<blockquote>\n<p>Stability is what testers crave so that they can say, “I’ve tested that; it’s done.” Continuously changing requirements are a tester’s nightmare. However, as agile testers, we have to welcome change. </p>\n</blockquote>\n<blockquote>\n<p>Some teams have analysts who can spend more time with the business experts to do some advance planning. </p>\n</blockquote>\n<blockquote>\n<p>Automated testing is one key to the solution. One thing we know for sure: No agile team will succeed doing only manual testing. We need robust automation in order to deliver business value in a time frame that makes it valuable. </p>\n</blockquote>\n<h4>Self-Organize</h4>\n<blockquote>\n<p>Automating tests is hard, but it is much easier when you have the whole team working together. Any testing issue is easier to address when you have people with multiple skill sets and multiple perspectives attacking it. </p>\n</blockquote>\n<blockquote>\n<p>The programmers would start implementing new stories in a new, testable architecture, using test-driven development. The testers would write manual regression test scripts, and the entire team—programmers, testers, the system administrator, and the DBA—would execute them on the last two days of every iteration. </p>\n</blockquote>\n<blockquote>\n<p>When an agile team faces a big problem, perhaps a production show stopper or a broken build, it’s everyone’s problem. </p>\n</blockquote>\n<blockquote>\n<p>When the team creates its own approach and commits to it, its members adopt a new attitude toward testing. </p>\n</blockquote>\n<h4>Focus on People</h4>\n<blockquote>\n<p>In the history of software development, testers haven’t always enjoyed parity with other roles on the development team. Some people saw testers as failed programmers or second-class citizens in the world of software development. </p>\n</blockquote>\n<blockquote>\n<p>Testing knowledge is one component of any team’s ability to deliver value. </p>\n</blockquote>\n<h4>Enjoy</h4>\n<h4>Adding Value</h4>\n<blockquote>\n<p>In agile development, the whole team takes responsibility for delivering high-quality software that delights customers and makes the business more profitable.</p>\n</blockquote>\n<blockquote>\n<p>Even with short iterations and frequent releases, it’s easy to develop a gap between what the customer team expects and what the team delivers. Using tests to drive development helps to prevent this, but you still need the right tests.</p>\n</blockquote>\n<blockquote>\n<p>Programmers focus on making things work. If they’re coding to the right requirements, customers will be happy. </p>\n</blockquote>\n<blockquote>\n<p>Agile testers ask questions of both customers and developers early and often, and help shape the answers into the right tests. </p>\n</blockquote>\n<blockquote>\n<p>By the end of the iteration, testers verify that the minimum testing was completed. </p>\n</blockquote>\n<h3>My Summary</h3>\n<p>An Agile tester is defined as a professional tester who embraces changes ( which goes against the common sense looking for stability ), collaborates with technical and business people to foster the quality mindset within the team.</p>\n<p>An agile team is looking to continually achieve the best they can do.</p>\n<p>Then the chapter talks about each of the 10 values of agile testers. See the bullet list above.</p>\n<h1>Part II Organizational Challenges</h1>\n<h2>Chapter 3 Cultural Challenges</h2>\n<h3>Organizational Culture</h3>\n<blockquote>\n<p>If a company tried agile and had poor results, people will be suspicious of trying it again, citing examples of why it didn’t work. They might even actively campaign against it. </p>\n</blockquote>\n<h4>Quality Philosophy</h4>\n<blockquote>\n<p>Does it tolerate poor quality? Does it take customers’ quality requirements into account, or is it just concerned with getting the product into the customers’ hands as fast as it can? </p>\n</blockquote>\n<blockquote>\n<p>A team that tries to use agile development in such an environment faces an uphill battle. </p>\n</blockquote>\n<blockquote>\n<p>what about testers who are used to building test scripts according to a requirements document? Can they learn to ask the questions as the codeis being built? Testers who don’t change their approach to testing have a hard time working closely with the rest of the development team. </p>\n</blockquote>\n<blockquote>\n<p>Most quality assurance professionals are eager to take what they’ve learned and make it better. These people are adaptable enough to not only survive, but to thrive in an agile project. </p>\n</blockquote>\n<h4>Sustainable pace</h4>\n<blockquote>\n<p>Teams might need to work for short bursts of unsustainable pace now and then, but it should be the exception, not the norm. </p>\n</blockquote>\n<blockquote>\n<p>If overtime is required for short periods, the whole team should be working extra hours. </p>\n</blockquote>\n<h4>Customer relationships</h4>\n<blockquote>\n<p>An open relationship is critical to the success of an agile project, where the relationship between the customer team and the development team is more like a partnership than a vendor-supplier relationship. </p>\n</blockquote>\n<blockquote>\n<p>Customers are critical to the success of your agile project. They prioritize what will be built and have the final say in the quality of the product. </p>\n</blockquote>\n<blockquote>\n<p>define acceptance tests that will prove that conditions of satisfaction are met. </p>\n</blockquote>\n<h4>Organization Size</h4>\n<blockquote>\n<p>Act Now, Apologize Later</p>\n</blockquote>\n<blockquote>\n<p>large organization, the bureaucratic wheels turn so slowly that your team might have to figure out and implement its own solutions. </p>\n</blockquote>\n<h3>Barriers to Successful Agile Adoption by Test/QA Teams</h3>\n<h4>Additional Roles</h4>\n<blockquote>\n<p>It’s critical that you take the time to analyze what roles your product needs to be successful, and if you need to fill them from outside the team, do it. </p>\n</blockquote>\n<h4>Not Understanding Agile Concepts</h4>\n<blockquote>\n<p>Plenty of teams simply adopt practices that work for them regardless of the original source, or they invent their own. </p>\n</blockquote>\n<blockquote>\n<p>For example, if you’re a tester who is pushing for the team to implement continuous integration, but the programmers simply refuse to try,you’re in a bad spot. If you’re a programmer who is unsuccessful at getting involved in some practices, such as driving development withbusiness-facing tests, you’re also in for conflict.</p>\n</blockquote>\n<blockquote>\n<p>Many of the agile development practices are synergistic, so if they are used in isolation, they might not provide the benefits that teams are looking for. </p>\n</blockquote>\n<blockquote>\n<p>Diverse viewpoints are good for a team, but everyone needs to be headed in the same direction. </p>\n</blockquote>\n<blockquote>\n<p>“mini-waterfall” phenomenon that often occurs when a traditional software development organization implements an agile development process. </p>\n</blockquote>\n<h4>Past Experience/Attitude</h4>\n<blockquote>\n<p>Ask everyone to be part of the solution, and work together to find out what processes and practices work best for their particular situations. </p>\n</blockquote>\n<h4>Cultural Differences among Roles</h4>\n<blockquote>\n<p>Customers need some way to know how development is progressing and whether their conditions of satisfaction are being met. </p>\n</blockquote>\n<h3>Introducing Change</h3>\n<h4>Talk about Fears</h4>\n<blockquote>\n<p>Fear is a common response to change. Forcing people to do something they don’t want is detrimental to positive change. </p>\n</blockquote>\n<blockquote>\n<ul>\n<li>You have the right to the tools you need to perform testing tasks in a timely manner. </li>\n<li>You have the right to expect your entire team, not just yourself, to be responsible for quality and testing. </li>\n</ul>\n</blockquote>\n<h4>Give Team Ownership</h4>\n<blockquote>\n<p>He made sure that the business understood that quality was more important than quantity or speed. </p>\n</blockquote>\n<h4>Celebrate Success</h4>\n<blockquote>\n<p>My better understanding of the application’s workings made me understand that the risk and cost of fixing it was potentially much more risky than the benefit. I believe that thinking like this isn’t a bad thing as long as we are always mindful of the end customer impact, not just the internal cost. </p>\n</blockquote>\n<h3>Management Expectations</h3>\n<h4>Cultural Changes for Managers</h4>\n<blockquote>\n<p>It is the team (which includes the customer) that defines the level of quality necessary to deliver a successful application. </p>\n</blockquote>\n<blockquote>\n<p>He said that in a traditional waterfall type project, the reports all showed that everything was going according to plan until the very end,and then everything was in a panic state and “nothing worked.” </p>\n</blockquote>\n<blockquote>\n<p>In the agile project, there were problems every day that needed to be addressed. Agile projects were more work on a consistent basis, but at least he was getting realistic reports. There were no surprises at the end of the project. </p>\n</blockquote>\n<blockquote>\n<p>testing is no longer a separate activity that occurs after development but that testing and coding are integrated activities. </p>\n</blockquote>\n<h4>Speaking the Manager's Language</h4>\n<blockquote>\n<p>If you need time and funds to learn and implement an automated test tool, explain to management that over time, automated regression tests will let your team go faster and deliver more functionality in each iteration. </p>\n</blockquote>\n<h3>Change Doesn't Come Easy</h3>\n<h4>Let Them Feel Pain</h4>\n<blockquote>\n<p>Sometimes you just have to watch the train wreck. If your suggestions for improvement were rebuffed, and the team fails, bring your suggestion up again and ask the team to consider trying it for a few iterations. People are most willing to change in the areas where they feel the most pain. </p>\n</blockquote>\n<h4>Beware the Quality Police Mentality</h4>\n<blockquote>\n<p>Be a collaborator, not an enforcer.</p>\n</blockquote>\n<h4>Vote with Your Feet</h4>\n<blockquote>\n<p>Nobody cares about quality, and you feel invisible despite your best efforts. It might be time to look for a better team. Some teams are happy the way they are and simply don’t feel enough pain to want to change. </p>\n</blockquote>\n<h3>My Summary</h3>\n<p>It's essential that testers evolve their skills and adapt to a new testing philosophy - even though it's hard on the testers and it will required courage, without it they are faded to fail in an agile environment.</p>\n<h2>Chapter 4 Team Logistics</h2>\n<h3>Team Structure</h3>\n<h4>Independent QA Teams</h4>\n<blockquote>\n<p>Rather than keeping the testers separate as an independent team to test the application after coding, think about the team as a community of testers. Provide a learning organization to help your testers with career development and a place to share ideas and help each other.</p>\n</blockquote>\n<h4>Integration of Testers into an Agile Project</h4>\n<blockquote>\n<p>Developers get training on pair programming, test-driven development, and other agile practices, while testers often seem to get no training at all. </p>\n</blockquote>\n<blockquote>\n<p>The pairing of programmers and testers can only improve communication about the quality of the product. </p>\n</blockquote>\n<blockquote>\n<p>One major advantage of an integrated project team is that there’s only one budget and one schedule. There is no “testing” time to cut if all of the functionality is not finished. If there is no time to test a new feature, then there is no time to develop it in the first place. </p>\n</blockquote>\n<h3>Physical Logistics</h3>\n<blockquote>\n<p>To support agile values and principles, teams work better when they have ready access to all team members, easy visibility of all project progress charts, and an environment that fosters communication. </p>\n</blockquote>\n<blockquote>\n<p>The number of testers needed will vary and depends upon the complexity of the application, the skill set of the testers, and the tools used.</p>\n</blockquote>\n<h3>Resources</h3>\n<h4>Tester-Developer Ratio</h4>\n<blockquote>\n<p>One or two programmers wore a “tester hat” for each iteration, writing customer-facing tests ahead of coding and performing manual tests.</p>\n</blockquote>\n<blockquote>\n<p>The web-based financial application we produce has highly complex business logic, is high risk, and test intensive. Testing tasks often add up to the same amount of time as programming tasks. Even with a relatively high tester–programmer ratio, programmers do much of the functional test automation and sometimes pick up manual testing tasks. </p>\n</blockquote>\n<h3>Building a Team</h3>\n<h4>Self-Organizing Team</h4>\n<blockquote>\n<p>and there are times a coach needs to provide strong encouragement and lead the team when it needs leadership. </p>\n</blockquote>\n<h4>Every Team Member Has Equal Value</h4>\n<blockquote>\n<p>Every team member has equal value to the team.</p>\n</blockquote>\n<blockquote>\n<p>If you’re a tester stuck on an automation problem, have the courage to ask a team member for help.</p>\n</blockquote>\n<h4>What Can You Do?</h4>\n<blockquote>\n<p>Courage is especially important. Get up and go talk to people; ask how you can help. Reach out to team members and other teams with direct communication. Notice impediments and ask the team to help remove them. </p>\n</blockquote>\n<h3>My Summary</h3>\n<p>QA teams work better when they are part of the project's team instead of a independent QA team. Focus on the \"whole-team approach\".</p>\n<p>When working physically together you should avoid testes and the rest of the project must work closely to achieve the best result.</p>\n<p>When building a team, most of the times self-organizing teams will achieve the best results. However, eventually a leader will have to step into to lead the team.</p>\n<p>Remember the in the whole team everyone has equal value. There is no space for the me vs you, is all us. Also, there is no unique tester developer ratio, each project, each team, each system will have to determine their own ratio.</p>\n<p>QA members will have to receive training similarly to the developers.</p>\n<h2>Chapter 5 Transitioning Typical Processes</h2>\n<h3>My Summary</h3>\n<p>​                                         </p>\n<p>on an agile project, but we still need ways to measure progress, track defects, and plan testing.\na fundamental lean measurement is the time it takes to go “from concept to cash,” from a customer’s feature request to delivered software. They call this measurement “cycle time.”\nMeasurements such as cycle time that involve the whole team are more likely to drive you toward success than are measures confined to isolated roles or groups.\nMetrics that measure milestones along a journey to achieve team goals are useful.\nwe don’t achieve the desired improvement, it’s more important to figure out why than to lament whatever amount our bonus was reduced as aresult.\nwe’re using a burndown chart, and we’re burning up instead of down, that’s a red flag to stop, take a look at what’s happening, and make surewe understand and address the problems.\nHowever, it is important to recognize that the number itself means nothing. For example, the tests might be poorly written, or to have a well tested product, maybe we need 10,000 tests. Numbers don’t work in isolation.\nWhen you’re trying to figure out what to measure, first understand what problem you are trying to solve.\nFocus on the goal, not the metrics.<br>\nAn increased number of defect reports might mean the team is doing a better job of testing, not that they are writing more buggy code.\nThere’s not much value in knowing the rate of bugs found and fixed during development, because finding and fixing them is an integral part of development.\nWe could write a test to show the failure, fix the code, and keep the test in our regression suite.\nAccording to lean principles, this inventory of defects is a waste. As a team, we should be thinking of ways to reduce this waste.\nAs with all tools used by your agile development team, you should consider the whole team’s opinion.\nWhether or not you use a DTS, you want to make defects as visible as possible.\nIf you choose the wrong tool, cut your losses and start researching alternatives.\nConcentrate on improving communication and building collaboration. If you encounter a lot of defects, investigate the source of the problem.\nWe like to think of a test strategy as a static document that seldom changes, while a test plan is created new and is specific to each new project.\nIf your organization wants documentation about your overall test approach to projects, consider taking this information and putting it in astatic document that doesn’t change much over time.\nA test strategy document can be used to give new employees a high-level understanding of how your test processes work.\nThe power of planning is to identify possible issues and dependencies, to bring risks to the surface to be talked about and to be addressed,and to think about the big picture.\nIf a requirement changed, we needed to know that we had changed the appropriate test cases.\nWe build functionality in tiny, well-defined steps. We work with the team closely and know when something changes.\nWe can then collaborate with the customer to define acceptance tests. We test each story as the programmer works on it, so we know that nothing goes untested.\nDocuments such as traceability matrices might be needed to fulfill requirements imposed by the organization’s audit standards or qualitymodels.\nQuality assurance teams in traditional development organizations are often tasked with providing information for auditors and ensuringcompliance with audit requirements.\nMaintain the team’s focus on delivering high-quality software that provides real business value, and see how you can work within the model.\nTools are easier to use when used with code that’s designed for testability.\nagile development starts with customer tests, which tell the team what to code.\nThis concept of testing to help the programmers is new to many testers and is the biggest difference between testing on a traditional projectand testing on an agile project.\nThe lower left quadrant represents test-driven development, which is a core agile development practice.\nUnit tests verify functionality of a small subset of the system, such as an object or method. Component tests verify the behavior of a larger part of the system, such as a group of classes that provide some service\nBoth types of tests are usually automated<br>\nThey enable the programmers to measure what Kent Beck has called the internal quality of their code [Beck, 2000, 2004].\nThe process of writing tests first helps programmers design their code well. These tests let the programmers confidently write code to delivera story’s features without worrying about making unintended changes to the system. They can verify that their design and architecture decisionsare appropriate. Unit and component tests are automated and written in the same programming language as the application.\nProgrammer tests are normally part of an automated process that runs with every code check-in, giving the team instant, continual feedbackabout their internal quality.\nThese business-facing tests, also called customer-facing tests and customer tests, define external quality and the features that the customers want.\nWith agile development, these tests are derived from examples provided by the customer team. They describe the details of each story.Business-facing tests run at a functional level, each one verifying a business satisfaction condition. They’re written in a way businessexperts can easily understand using the business domain language.\nIt’s possible this quadrant could duplicate some of the tests that were done at the unit level; however, the Quadrant 2 tests are orientedtoward illustrating and confirming desired system behavior at a higher level.\nMost of the business-facing tests that support the development team also need to be automated. One of the most important purposes of tests in these two quadrants is to provide information quickly and enable fast troubleshooting. They must be run frequently in order to give the team early feedback in case any behavior changes unexpectedly.\nStill, some automated tests must verify the user interfaces and any APIs that client applications might use.\nAll of these tests should be run as part of an automated continuous integration, build, and test process.\nUser interaction experts use mock-ups and wireframes to help validate proposed GUI (graphical user interface) designs with customers and to communicate those designs to the developers before they start to code them.\nThe tests in this group are tests that help support the team to get the product built right but are not automated.\nAcceptance tests verify that all aspects of the system, including qualities such as usability and performance, meet customer requirements.\nThe quick feedback provided by Quadrants 1 and 2 automated tests, which run with every code change or addition, form the foundation of an agile team.\nThese tests first guide development of functionality, and when automated, then provide a safety net to prevent refactoring and the introduction of new code from causing unexpected results.\nLisa’s Story<br>\nperfect example of continous integration and agile testing<br>\nThey verify that the business logic and the user interfaces behave according to the examples provided by the customers.\ncritique can include both praise and suggestions for improvement.\nWe review the software in a constructive manner, with the goal of learning how we can improve it.\nAs we learn, we can feed new requirements and tests or examples back to the process that supports the team and guide development.\nThe business experts might overlook functionality, or not get it quite right if it isn’t their field of expertise. The team might simply misunderstand some examples. Even when the programmers write code that makes the business-facing tests pass, they might not be delivering what the customer really wants.\nWhen we do business-facing tests to critique the product, we try to emulate the way a real user would work the application. This is manual testing that only a human can do. We might use some automated scripts to help us set up the data we need, but we have to use our senses, our brains, and our intuition to check whether the development team has delivered the business value required by the customers.\nUser Acceptance Testing (UAT) gives customers a chance to give new features a good workout and see what changes they may want in the future,and it’s a good way to gather new story ideas.\nFocus groups might be brought in, studied as they use the application, and interviewed in order to gather their reactions. Usability testingcan also include navigation from page to page or even something as simple as the tabbing order. Knowledge of how people use systems is anadvantage when testing usability.\nExploratory testing is central to this quadrant. During exploratory testing sessions, the tester simultaneously designs and performs tests,using critical thinking to analyze the results.\nThis offers a much better opportunity to learn about the application than scripted tests.\nExploratory testing is a more thoughtful and sophisticated approach than ad hoc testing. It is guided by a strategy and operates within defined constraints.\nExploratory testing works the system in the same ways that the end users will. Testers use their creativity and intuition. As a result, it is through this type of testing that many of the most serious bugs are usually found.\nNontechnical customer team members often assume that the developers will take care of concerns such as speed and security, and that the programmers are intent on producing only the functionality prioritized by the customers.\nIf we know the requirements for performance, security, interaction with other systems, and other nonfunctional attributes before we startcoding, it’s easier to design and code with that in mind.\nTechnology-facing tests that critique the product should be considered at every step of the development cycle and not left until the very end.\nIn many cases, such testing should even be done before functional testing.\nunit tests didn’t test any nonfunctional requirements such as capacity, performance, scalability, and usability.\nIf the iteration demo reveals that the team misunderstood the customer’s requirements, maybe you’re not doing a good enough job of writing customer tests to guide development.\nFor most products, we need all four categories of testing to feel confident we’re delivering the right value. Not every story requires security testing, but you don’t want to omit it because you didn’t think of it.\nThe technology-facing and business-facing tests that drive development are central to agile development, whether or not you actually write task cards for them.\nA combination of tests from all four quadrants will let the team know when each feature has met the customer’s criteria for functionality and quality.\nNo matter what resources have to be brought in from outside the development team, the team is still responsible for getting all four quadrantsof testing done.\nWe believe that a successful team is one where everybody participates in the crafting of the product and that everyone shares the team’sinternal pain when things go wrong.\nImplementing the practices and tools that enable us to address all four quadrants of testing can be painful at times, but the joy ofimplementing a successful product is worth the effort.\nTechnical debt builds up when the development team takes shortcuts, hacks in quick fixes, or skips writing or automating tests because it’sunder the gun.\nThe code base gets harder and harder to maintain. Like financial debt, “interest” compounds in the form of higher maintenance costs and lower team velocity. Programmers are afraid to make any changes, much less attempt refactoring to improve the code, for fear of breaking it.Sometimes this fear exists because they can’t understand the coding to start with, and sometimes it is because there are no tests to catchmistakes.\nHowever, we need to bear in mind that each organization, product, and team has its own unique situation, and each needs to do what works for it in its individual situation.\nA single product or project’s needs might evolve drastically over time.\nwww.context-driven-testing.com<br>\ncheck it out<br>\nWithout a foundation of test-driven design, automated unit and component tests, and a continuous integration process to run the tests, it’shard to deliver value in a timely manner.\nUnit tests and component tests ensure quality by helping the programmer understand exactly what the code needs to do, and by providing guidance in the right design.\nThey help the team to focus on the story that’s being delivered and to take the simplest approach that will work.\nUnit tests verify the behavior of parts as small as a single object or method [Meszaros, 2007]. Component tests help solidify the overall design of a deployable part of the system by testing the interaction between classes or methods.\nShe can pair with a tester to help make sure all aspects of that piece of code, and its communication with other units, are tested.\nThe term test-driven development misleads practitioners who don’t understand that it’s more about design than testing.\nCode developed test-first is naturally designed for testability.\nWhen post-development testing time is occupied with finding and fixing bugs that could have been detected by programmer tests, there’s no timeto find the serious issues that might adversely affect the business.\nThe more bugs that leak out of our coding process, the slower our delivery will be, and in the end, it is the quality that will suffer.\nSolid source code control, configuration management, and continuous integration are essential to getting value from programmer tests that guide development.\nContinuous integration saves time and motivates each programmer to run the tests before checking in the new code.\nThe development cycles are shorter, but code is still being thrown “over the wall” to testers who run out of time to test because the code isof poor quality.\nSpeed should never be the end goal of an agile development team. Trying to do things fast and meet tight deadlines without thinking about the quality causes us to cut corners and revert to old, bad habits.\nHappily, though, speed is a long-term side effect of producing code with the highest possible internal quality.\nA safety net of automated unit and code integration tests enables the programmers to refactor frequently.\nYou might find so many bugs while testing the “happy path” that you never have time to test more complex scenarios and edge cases.\nDriving coding practices with tests means that the programmers probably understood the story’s requirements reasonably well.\nIf we find a defect, we show it to the programmer, who writes a unit test to reproduce the bug and then fixes it quickly.\nAfter a development team has mastered TDD, the focus for improvement shifts from bug prevention to figuring out better ways to elicit andcapture requirements before coding.\ntest-first development merely says that the tests are written before the production code; it does not imply that the production code is made to work one test at a time\nWriting tests and writing code with those tests in mind means programmers are always consciously making code testable.\nTest-driven development means that programmers will write each test before they write the code to make it pass.\nWriting “testable code” is a simple concept, but it’s not an easy task, especially if you’re working on old code that has no automated testsand isn’t designed for testability.\ncommon approach in designing a testable architecture is to separate the different layers that perform different functions in the application.\n“create your application to work without either a UI or a database so you can run automated regression tests against the application, work when the database becomes unavailable, and link applications together without any user involvement.”\nIt’s hard to write unit tests for code that isn’t designed for testability, and it’s hard to change code that isn’t safeguarded with unittests.\nNew stories were coded test-first in a new architecture while the old system was still maintained. Over time, much of the system has been converted to the new architecture, with the goal of eventually doing away with the old system.\nTeams should take time to consider how they can create an architecture that will make automated tests easy to create, inexpensive to maintain,and long-lived.\nDon’t be afraid to revisit the architecture if automated tests don’t return enough value for the investment in them.\nLisa’s<br>\nexample of automation and ci<br>\nRegression bugs will be caught early, when they’re cheapest to fix.\nbusiness-facing tests might cover a bit of the same ground as unit or code integration tests, but they have such different purposes that waste isn’t a worry.\nEach unit test is independent and tests one dimension at a time. This means that when a unit test fails, the programmer can identify theproblem quickly and solve the issue just as quickly.\nPush tests to lower levels whenever possible; if you identify a test case that can be automated at the unit level, that’s almost always abetter return on investment.\nif we aren’t programmers ourselves, we don’t necessarily have much credibility when we urge the programmers to adopt practices such as TDD.\nTDD is really more of a design activity, it’s essential that the person writing the code also write the tests, before writing the code.\nFearless Change<br>\nlook for it<br>\nWhen I wanted my team to start using FitNesse, I identified the programmer who was most sympathetic to my cause and asked him to pair with meto write FitNesse tests for the story he was working on.\nWork with the product owner to make quality your goal, and communicate the quality criteria to the team.\nEncourage the programmers to take time to do their best work instead of worrying about meeting a deadline.\nYour job is to explain to the business managers how making quality a priority will ensure that they get optimum business value.\nBudget time for major refactoring, for brainstorming about the best approach to writing unit and code integration tests, and for evaluating,installing, and upgrading tools.\nAt the retrospective, raise issues that are hampering successful delivery. For example, “We aren’t finishing testing tasks before the end ofthe iteration” is a problem for the whole team to address. If one reason for not finishing is the high number of unit-level bugs, suggest experimenting with TDD, but allow programmers to propose their own ways to address the problem.\nIf the team isn’t doing an adequate job with the tests in this quadrant, the other types of testing will be much more difficult. This doesn’t mean you can’t get value from the other quadrants on their own—it just means it will be harder to do so because the team’s code will lack internal quality and everything will take longer.\nWhen you label or tag a build, make sure you label or tag the test code too, even if it doesn’t get released to production.\nintegrate code changes from different programmers, run the unit tests to verify no regression bugs have occurred, and provide the code in a deployable format.\nmanage the build but also provide easy ways to report and document build results, and they integrate easily with build automation and testtools.\nWithout an automated build process you’ll have a hard time deploying code for testing as well as releasing.\nGUI code can and should be developed test-first as well.<br>\nThat’s not much information, and it’s not meant to be. Stories are a brief description of desired functionality and an aid to planning and prioritizing work.\nthe customer team and development team strike up a conversation based on the story. The team needs requirements of some kind, and they needthem at a level that will let them start writing working code almost immediately. To do this, we need examples to turn into tests that willconfirm what the customer really wants.\nThese tests help provide the big picture and enough details to guide coding. Business-facing tests express requirements based on examples anduse a language and format that both the customer and development teams can understand.\nThe business-facing tests in Quadrant 2 are written for each story before coding is started, because they help the team understand what code to write.\nQuadrant 2 tests define and verify external quality, and help us know when we’re done.\nThe customer tests to drive coding are generally written in an executable format, and automated, so that team members can run the tests asoften as they like in order to see if the functionality works as desired. These tests, or some subset of them, will become part of an automated regression suite so that future development doesn’t unintentionally change system behavior.\nAs we discuss the stories and examples of desired behavior, we must also define nonfunctional requirements such as performance, security, and usability.\nIt’s helpful if some members of the technical team can participate in story-writing sessions so that they can have input into the functionality stories and help ensure that technical stories are included as part of the backlog.\nStories by themselves don’t give much detail about the desired functionality. They’re usually just a sentence that expresses who wants the feature, what the feature is, and why they want it.\nStories are only intended as a starting point for an ongoing dialogue between business experts and the development team.\nIf team members understand what problem the customer is trying to solve, they can suggest alternatives that might be simpler to use and implement.\nagile teams expand on stories until they have enough information to write appropriate code.\nThese tests guide programmers as they write the code and help the team know when it has met the customers’ conditions of satisfaction.\nIn agile development, we accept that we’ll never understand all of the requirements for a story ahead of time.\nRequirements changes are pretty much inevitable.<br>\nWe can also use our tests to provide a common language that’s understood by both the development team and the business experts.\nlife examples of desired and undesired behavior can be expressed so that they’re understood by both the business and technical sides.\nThe tests need to be written in a way that’s comprehensible to a business user reading them yet still executable by the technical team.\nBusiness-facing tests also help define scope, so that everyone knows what is part of the story and what isn’t.\nWorst-case scenarios tend to generate ideas. They also help us consider risk and focus our tests on critical areas. Another good question is,“What’s the best thing that could happen?” This question usually generates our happy path test, but it might also uncover some hidden assumptions.\nMost importantly, ask the customer to give you examples of how the feature should work.\nOur challenge is to capture examples, which might be expressed in the business domain language, as tests that can actually be executed.\nEach example or test has one point of view. Different people will write different tests or examples from their unique perspectives. We’d liketo capture as many different viewpoints as we can, so think about your users.\nClose, constant collaboration between the customer team and the developer team is key to obtaining examples on which to base customer teststhat drive coding.\nIf we get several different versions of how a piece of functionality should work, we won’t know what to code. Let’s consider ways to get customers to agree on the conditions of satisfaction for each story.\nA Product Owner is a role in Scrum. He’s responsible not only for achieving advance clarity but also for acting as the “customerrepresentative” in prioritizing stories.\nYour development team can’t successfully deliver what the business wants unless conditions of satisfaction for a story are agreed to up front.\nIt’s easy to lose track of the big picture when we’re focusing on a small number of stories in each iteration.\nmake a list of all of the parts of the system that might be affected by a story.\nTake time to identify the central value each story provides and figure out an incremental approach to developing it. Plan small increments of writing tests, writing code, and testing the code some more. This way, your Quadrant 2 tests ensure you’ll deliver the minimum value as planned.\nHave the product owner and other interested stakeholders explain the stories. You might find that some stories need to be subdivided or that additional stories need to be written to fill in gaps.\nA smart incremental approach to writing customer tests that guide development is to start with the “thin slice” that follows a happy path from one end to the other.\nThe sooner you can build the end-to-end path, the sooner you can do meaningful testing, get feedback, start automating tests, and start exploratory testing.\nFinishing stories a small step at a time helps spread out the testing effort so that it doesn’t get pushed to the end of the iteration.\nThey start with the happy path and show that the story meets the intended need. They cover various user scenarios and ensure that other partsof the system aren’t adversely affected.\nWhen the tests all pass and any missed requirements have been identified, we are done for the purpose of supporting the programmers in their quest for code that does the “right thing.”\nDriving development with tests doesn’t mean we’ll identify every single requirement up front or be able to predict perfectly when we’re done.\nCoding to predefined tests doesn’t work well if the tests are for improbable edge cases. While we don’t want to test only the happy path, it’sa good place to start. After the happy path is known, we can define the highest risk scenarios—cases that not only have a bad outcome but alsohave a good possibility of happening.\nWe might need to test the entire life cycle of the account. We don’t have time to test more than necessary, so decisions about what to test are critical. The right tests help us mitigate the risk brought by the change.\nAlways consider how each individual story impacts other parts of the system. Use realistic test data, use concrete examples as the basis ofyour tests, and have a lot of whiteboard discussions (or their virtual equivalent) in order to make sure everyone understands the story.\nFor example, many of the most serious issues are usually uncovered during manual exploratory testing. Performance, security, stability, and usability are also sources of risk.\nWhen programmers on an agile team get ready to do test-driven development, they use the business-facing tests for the story in order to knowwhat to code.\nThey need to be clearly understood, easy to run, and provide quick feedback; otherwise, they won’t get used.\nInexperienced agile teams might accept the need to drive coding with automated tests at the developer test level more easily than at thecustomer test level. However, without the customer tests, the programmers have a much harder time knowing what unit tests to write.\nTeams that automate only technology-facing tests find that they can have bug-free code that doesn’t do what the customer wants. Teams thatdon’t automate any tests will anchor themselves with technical debt.\n“role, function, business value” pattern for user stories that Mike Cohn describes in User Stories Applied, as in: As a (role), I want(function) so that (business value).\nLean development teaches us to avoid waste while we develop software.\nThe big difference in agile development is that we create and discuss the mock-ups just as we’re about to start writing the code, rather than weeks or months beforehand.\nIn agile development, we create these diagrams as we’re about to start writing tests and code.\nbecause they use a more natural language for specifying the tests.\nwith a focus on the domain rather than on the technology,<br>\nInstead of the word “test” or “assert,” BDD uses the word “should.” By thinking in terms of behavior, it’s natural to write specificationsahead of code. Test specifications use a domain-specific language to provide tests that customers can read but that can also be easilyautomated.\nThis line of code describes behavior in a more literal manner too—the code uses a normal everyday phrase like shouldBe, which is distinctly different than the previously written assertEquals.\nThis is a fundamental point of the notion of behavior-driven development, which strives to more appropriately validate a software system by thinking in terms of the term “should” rather than test.\nbehavior-driven development converges on the idea of executable documentation. Indeed, through leveraging a stakeholder’s language, there is a decreased impedance mismatch between what he wants and what he ultimately receives;\nfacilitates a deeper level of collaboration between all parties.\nBy leveraging the customer’s language, the customer has the ability to collaboratively facilitate in validating the system he or she wantsbuilt.\nthere is a direct link between what stakeholders ask for and what they receive.\nexecutable documentation using a domain-specific language that everyone on both the customer and developer teams understands.\nThe goal of business-facing tests that support the team is to promote communication and collaboration between customers and developers, and to enable teams to deliver real value in each iteration.\nmanual exploratory testing that helps us learn about the functionality and provides immediate feedback gets pretty tedious and slow without any assistance from automation.\nMaking tests easy for testers and customers to write, while keeping the automation framework designed for optimum maintainability, reduced the total cost of ownership of the tests.\nImplementing a new test automation tool usually requires some experimentation to get a good balance of testable code and well-designed test scripts. Involving the whole team makes this much easier.\n“home-brewed” for the tools agile teams create to meet their own unique testing needs.\nMake sure the most obvious use case is working first. Write a simple, happy path automated test to show the code accomplishes the most basic task that it should. After that test passes, you can start getting more creative. Writing the business-facing tests is an iterative process.\nDiscussing the test often leads the programmer to realize he missed or misunderstood a requirement.\nConfine each test to one business rule or condition.<br>\nAfter a test passes, it shouldn’t fail unless the requirements were changed. If that happens, the test should be updated before the code is altered.\nWhenever a test fails in a continuous integration and build process, the team’s highest priority (other than a critical production problem)should be to get the build passing again.\nWe want our regression suite to run in a timely manner, and having too many tests for edge cases would slow it down.\nTesting with data in a real database can be a means of automating a test against legacy code whose data access and business logic layers aren’t easily separated.\nFinding the right pattern for each type of test ensures the test communicates clearly, is easy to maintain, and runs in an optimal amount of time.\nBring programmers and testers together to brainstorm test approaches and to help decide what tests can be automated and how the code should be designed to support testing.\nBusiness logic and algorithms should be accessible by test fixtures, without having to go through a user interface or batch scheduling process.\nData-driven testing is a tool that can help reduce test maintenance and enable you to share your test automation with manual testers.\nKeyword-driven testing is another tool used in automated testing, where predefined keywords are used to define actions. These actionscorrespond to a process related to the application.\nBusiness-facing tests built with appropriate design patterns and written ahead of any coding help the team achieve a testable code design.\nNot all code is testable using automation, but work with the programmers to find alternative solutions to your problems.\nCoding and testing are part of one process in agile development.\nYou can’t write tests without a testable code design, and you can’t write code without well-designed tests that clearly communicaterequirements and are compatible with the system architecture.\nThis is why we always consider coding and testing together. When we estimate stories, we include time for both coding and testing, and when we plan each iteration and story, we budget time to design both tests and code.\nWe’ve assumed that at least a good-sized portion of the tests that guide programming will be automated.\nYou might come up with one-off tests that are important to do but not important to repeat over and over in a regression suite.\nWe want some way for all tests, even those that won’t be automated, to be accessible to everyone on the development team and understandable to our customers.\nbusiness-facing tests are those you could describe in terms that would (or should) be of interest to a business expert.\nWe recommend showing customers what you’re developing early and often.\nThe incremental and iterative nature of agile development gives you a chance to demonstrate business value as you produce it, even before you release it.\nWhen testing different scenarios, both the data and the flow need to be realistic.\nand it enables you to go beyond the obvious variations that have already been tested.\nExploratory testing is not a means of evaluating the software through exhaustive testing. It is meant to add another dimension to your testing.\nsimultaneous test design, test execution, and learning.”<br>\nUpon receiving new builds, the exploratory tester would tend to deemphasize repetition and emphasize variation in order to discover problems missed by older tests that are no longer revealing interesting information.\nExploratory testing is characterized by the degree to which the tester is under her own control, making informed choices about what he or sheis going to do next, and where the last outcome of the last activity consciously informs the next choice.\nGood exploration requires continuous investigation of the product by engaged human testers, in collaboration with the rest of the project community, rather than following a procedurally structured approach, performed exclusively by automation.\nExploration emphasizes individuals and interactions over processes and tools.\nExploratory approaches use variation to drive an active search for problems instead of scripted manual or automated test cases that merely confirm what we already knew.\nAnd to be effective, good exploration requires frequent feedback between testers, developers, customers, and the rest of the project community,not merely repetition of tests that were prepared at the beginning of the iteration, before we had learned important things about the project.\nPeople unfamiliar with exploratory testing often confuse it with ad hoc testing. Exploratory testing isn’t sitting down at a keyboard andtyping away.\nExploratory testing starts with a charter of what aspects of the functionality will be explored. It requires critical thinking, interpretingthe results, and comparing them to expectations or similar systems.\nFollowing “smells” when testing is an important component. Testers take notes during their exploratory testing sessions so that they can reproduce any issues they see and do more investigation as needed.\nExploratory testing uses the tester’s understanding of the system, along with critical thinking, to define focused, experimental “tests” which can be run in short time frames and then fed back into the test planning process.\nStarting early in each development cycle, consider exploratory tests based on: • Risk (analysis): The critical things you and the customer/user think can go wrong or be potential problems that will make people unhappy. • Models (mental or otherwise) of how software should behave: You and/or the customer have a great expectation about what the newly produced function should do or look like, so you test that. • Past experience: Think about how similar systems have failed (or succeeded) in predictable patterns that can be refined into a test, and explore it.• What your development team is telling you: Talk to your developers and find out what “is important to us.” • Most importantly: What you learn(see and observe) as you test. As a tester on an agile team, a big part of your job is to constantly learn about your product, your team, andyour customer. As you learn, you should quickly see tests based on such things as customer needs, common mistakes the team seems to be making,or good/bad characteristics of the product.\nSome tests might be good candidates for automated regression suites. Some might just answer your exploratory charter and be “done.” The agileteam must critically think about what they are learning and “evolve” tests accordingly.\nUse automation for what it is good at (repetitive tasks) and use agile humans for what we are good at (seeing, thinking, and dealing with theunexpected).\nEach type of testing feeds into the other.<br>\nToo often as testers, we can go off track and end up chasing a bug that might or might not be important to what we are currently testing.\nUse automation to do test set up, data generation, repetitive tasks, or to progress along a workflow to the place you want to start. Then youstart using your testing skills and experience to find the really “good” bugs, the insidious ones that otherwise escape attention.\nExploratory testing helps us learn about the behavior of an application.\nOne approach to using personas is for your team to invent several different users of your application representing different experience levelsand needs.\nWe can test the same scenario as each persona in turn and see what different experiences they might encounter.\npick a fictional character or famous celebrity and imagine how they would use our application.\nYou can also just assume the roles of novice, intermediate, and expert users as you explore the application.\nIf your application is custom-built for specific types of users, it might need to be “smart” rather than intuitive.\nIf a user has a choice of applications or websites, and has a bad first experience, they likely won’t use your application again.\nYou can start with mock-ups and flows drawn on paper, get opinions, and try HTML mock-ups next, to get early feedback.\nIf you can get access to competing software, take some time to research how those applications work and compare them with your product.\nAn API (application programming interface) is a collection of functions that can be executed by other software applications or components.\nThe more complicated testing patterns occur when the parameters work together to give many possible variations. Sometimes parameters areoptional, so it’s important that you understand the possibilities.\nBoundary conditions should be considered as well, for both the inputs and expected results. For example, use both valid and invalid strings forparameters, vary the content, and vary the length of the strings’ input.\nChanging the sequence might produce unexpected results and reveal bugs that would never be found through UI testing.\nWe tested about 100 variations of both valid and invalid data.\nWe didn’t keep all of these tests in the regression suite because they were just a means of quickly trying every combination we could think\nValuable exploratory testing of APIs is possible with or without benefit of automation.\nYour testing will need to confirm the quality of service that the external customers expect.\nMake time for exploratory testing to simulate the different ways users might access the web services.\ndomain-specific language that encapsulates implementation details “behind the scenes” works well for testing web services.\nAs with all other components of the product, your whole team is responsible for the quality of the documentation, and that includes both hardcopy and electronic.\nYour team might do Quadrant 2 tests to support the team as they produce documentation;\nWhen testing, try out worst-case scenarios, which could eventually become the most common scenario.\nUse mock-ups to help the customers decide on report contents and formatting. Find the “thin slice” or “critical path” in the report, code thatfirst, and show it to your customer before you add the next slice. Incremental development works as well with reports as it does with othersoftware.\nSome of the best testing happens because a person is paying attention to details that often get missed if we are following a script.\nTools shouldn’t replace human interaction; they should enhance the experience.\nComputers are good at doing repetitive tasks and performing calculations. These are two areas where they are much better than humans, so let’suse them for those tasks.\nWe’ve found that one of the most time-consuming tasks is the test setup and getting to the right starting point for your actual test. If\nAutomated functional test scripts can be run to set up data and scenarios to launch exploratory testing sessions.\nMany error messages are never displayed on the screen, so if you’re testing via the GUI, you never see them. Get familiar with tools likethese, because they can make your testing more effective and efficient.\nWhen testing this application, it is better for both the programmers and the testers to test various devices as early as possible.\nAs you plan your releases and iterations, think about the types of tools that might help with creating production-like test scenarios.\nDriving development with tests is critical to any project’s success.\nwe humans won’t always get all of the requirements for desired system behavior entirely correct. Our business experts themselves can missimportant aspects of functionality or interaction with other parts of the system when they provide examples of how a feature should work.\nAfter the code is written, we are no longer driving the development but are looking at ways to critique the product.\nThe technology-facing tests that critique the product are more concerned with the nonfunctional requirements than the functional ones.\nWe worry about deficiencies in the product from a technical point of view. Rather than using the business domain language, we describerequirements using a programming domain vocabulary.\nNonfunctional requirements include configuration issues, security, performance, memory management, the “ilities” (e.g., reliability,interoperability, and scalability), recovery, and even data conversion.\nOur customer should think about all of the quality attributes and factors that are important and make informed trade-offs. However, manycustomers focus on the business side of the application and don’t understand the criticality of many nonfunctional requirements in their roleof helping to define the level of quality needed for the product. They might assume that the development team will just take care of issuessuch as performance, reliability, and security.\nWe believe that the development team has a responsibility to explain the consequences of not addressing these nonfunctional or cross-functionalrequirements.\nHowever, when you are planning your project, you should think about the risks in each of these areas, address them in your test plan, andinclude the tools and resources needed for testing them in your project plan.\nMy theory is that it’s because agile development is driven by customers, from user stories. Customers simply assume that software will bedesigned to properly accommodate the potential load, at a reasonable rate of performance.\n“How many concurrent users should the application support?” and “What’s the average response time required?”\nAll of the agile literature talks about teams being generalists; anyone should be able to pick up a task and do it. We know that isn’t alwayspractical, but the idea is to be able to share the knowledge so that people don’t become silos of information.\nWe’re not talking about security within an application, such as who has access rights to administer it. Because that type of security is reallypart of the functional requirements and will be covered by regular stories, verifying that it works falls within the first three quadrants.We’re talking about probing for external security flaws and knowing the types of vulnerabilities in systems that hackers exploit.\nWhen teams understand the priority of qualities such as performance and reliability, they figure out how to improve their code to ensure them.\nPSR testing is a combination of math, science, analysis, programming, and problem solving.\nPSR testing is really just telling me “How fast?” (performance), “How long?” (stability), “How often?” (reliability), and “How much?”(scalability). So, as long as the awareness is there and the organization is seriously asking those questions with everything they develop,then PSR testing is successfully integrated into a team.\nRegardless of whether or not your team brings in additional resources for these types of tests, your team is still responsible for making surethe minimum testing is done.\nJust because this is the fourth out of four agile testing quadrants doesn’t mean these tests come last. Your team needs to think about when todo performance, security, and “ility” tests so that you ensure your product delivers the right business value.\nAs with functional testing, the sooner technology-facing tests that support the team are completed, the cheaper it is to fix any issues thatare found. However, many of the cross-functional tests are expensive and hard to do in small chunks.\n“As user Abby, I need to retrieve report X in less than 20 seconds so that I can make a decision quickly.”\nYou should be able to create a performance test that can be run and continue to run as you add more and more functionality to the workflow.\nFor many applications, correct functionality is irrelevant without the necessary performance.\nThe time to think about your nonfunctional tests is during release or theme planning. Plan to start early, tackling small increments as needed.\nIn the rush to deliver functionality, both business experts and development teams in newly started organizations may not be thinking ofsecurity first. They just want to get some software working so they can do business.\nTesters who are skilled in security testing can perform security risk-based testing, which is driven by analyzing the architectural risk,attack patterns, or abuse and misuse cases.\nAs a security tester, you need to have the same mind-set that attackers do, which means that you have to use your creativity in discovering andexploiting vulnerabilities in your own application.\nAs with any automated testing effort, running these tools is no guarantee that your code and your application will be free of security defects.\nAgile teams often use pair programming,<br>\nWe encourage development teams to develop standards and guidelines that they follow for application code, the test frameworks, and the teststhemselves.\nThe kinds of standards we mean include naming conventions for method names or test names. All guidelines should be simple to follow and makemaintainability easier.\nStandards for developing the GUI also make the application more testable and maintainable, because testers know what to expect and don’t needto wonder whether a behavior is right or wrong.\nSimple standards such as, “Use names for all GUI objects rather than defaulting to the computer assigned identifier” or “You cannot have twofields with the same name on a page” help the team achieve a level where the code is maintainable, as are the automated tests that providecoverage for it.\nMaintainable code supports shared code ownership.<br>\nInteroperability testing looks at end-to-end functionality between two or more communicating systems.\nThe API you develop for your system might enable your users to easily set up a framework for them to test easily. Easier testing for yourcustomer makes for faster acceptance.\nConsider all of the systems with which yours needs to communicate, and make sure you plan ahead to have an appropriate environment for testingthem together.\nWhen you start a new theme or project, think about the resources you might need to verify compatibility.\nMake sure your team gets information on your end users’ hardware, operating systems, browsers, and versions of each.\nReliability of software can be referred to as the ability of a system to perform and maintain its functions in routine circumstances as well asunexpected circumstances.\nTo do a reliability test, we simply need to use those same tests and run them over and over. Ideally, you would use statistics gathered thatshow daily usage, create a script that mirrors the usage, and run it on a stable build for however long your team thinks is adequate to provestability.\nYou can input random data into the tests to simulate production use and make sure the application doesn’t crash because of invalid inputs. Ofcourse, you might want to mirror peak usage to make sure that it handles busy times as well.\nRunning a thousand tests without any serious problems doesn’t mean you have reliable software. You have to run the right tests.\nTo make a reliability test effective, think about your application and how it is used all day, every day, over a period of time.\nAsk the customer team for their reliability criteria in the form of measurable goals.\nDriving development with the right programmer and customer tests should enhance the application’s reliability, because this usually leads tobetter design and fewer defects.\nThis means that a build is ready for testing anytime during the day.\nWhatever “ility” you need to test, use an incremental approach. Start by eliciting the customer team’s requirements and examples of theirobjectives for that particular area of quality.\nWrite business-facing tests to make sure the code is designed to meet those goals.\nThe next step might be to create a suitable test environment, to research tools, or to start with some manual tests.\nScalability testing verifies the application remains reliable when more users are added. What that really means is, “Can your system handle thecapacity of a growing customer base?”\nIdeally, the organization would have replaced the old system before it was an issue. This is an example of why it is important to understandyour system and its capability, as well as future growth projections.\nLoad testing evaluates system behavior as more and more users access the system at the same time. Stress testing evaluates the robustness ofthe application under higher-than-expected loads.\nyou don’t know where you want to go in terms of the system, then it matters little which direction you take (remember Alice and the CheshireCat?).”\nPerformance tuning can turn into a big project, so it is essential to provide a baseline that you can compare against new versions of thesoftware on performance.\nFor accurate results, tests need to be run on equipment that is similar to that of production.\nYou should be aware of memory usage and watch for leaks, because they can cause catastrophic failures when the application is in productionduring peak usage.\nYour team can evaluate what tests it needs from this quadrant. Talk about these tests as you plan your release; you can create a test planspecifically for performance and load if you’ve not done it before.\nEvery time they touched a piece of code in the legacy system, they added unit tests and refactored the code as necessary.\nGradually, the legacy system became more stable and was able to withstand major refactoring when it was needed.\nThe team found that examples helped clarify the expectations for many of the stories.\nThese acceptance tests served three purposes. They were business-facing tests that supported development because they were given to the teambefore coding started. Secondly, they were used by the test team as the basis of automation that fed into the regression suite and providedfuture ideas for exploratory testing. The third purpose was to confirm that the implementation met the needs of the customer. The productengineer did this solution verification.\nClasses written in Ruby contained code that could perform certain functions in the AUT,\nFor example, the log-in class didn’t know what username to log in with.\nQuadrant 2 tests that support the team may incorporate a variety of technologies, as they did in this project.\nThe team performed exploratory testing to supplement the automated test suites and get the best coverage possible. This human interaction withthe system found issues that automation didn’t find.\nUse the quadrants to help identify all of the different types of testing that will be needed and when they should be performed.\nInvest in a test architecture that accommodates the complexity of the system under test.\nFor each type of test, your team should work together to choose tools that solve your testing problems.\nTest automation is a core agile practice. Agile projects depend on automation. Good-enough automation frees the team to deliver high-qualitycode frequently.\nThe most basic reason a team wants to automate is that it simply takes too long to complete all of the necessary testing manually.\nAgile teams are able to deliver production-ready software at the end of each short iteration by having production-ready software every day.\nIf you execute your regression testing manually, it takes more and more time testing every day, every iteration.\nthe testers will probably spend much of their time researching, trying to reproduce and report those simple bugs, and less time findingpotentially serious system level bugs.\ncode design is more likely to be less testable and may not provide the functionality desired by the business.\nSetting up data for a variety of complex scenarios can be an overwhelming task if you have no automated way to speed it up.\nManual testing gets repetitive, especially if you’re following scripted tests, and manual tests get boring very quickly.\nBecause manual testing is slow, you might still be testing at midnight on the last day of the iteration. How many bugs will you notice then?\nHaving continual builds run all of the unit tests and the functional regression tests means more time to do interesting exploratory testing.\nAutomating the setup for exploratory tests means even more time to probe into potentially weak parts of the system.\nAutomating tests can actually help with consistency across the application.\nProjects succeed when good people are free to do their best work.\nKnowing the code has sufficient coverage by automated regression tests gives a great feeling of confidence.\na change might produce an unexpected effect, but we’ll know about it within a matter of minutes\nTeams that have good coverage from automated regression tests can make changes to the code fearlessly.\nThe tests will tell them right away whether or not they broke anything. They can go lots faster than teams relying exclusively on manualtesting.\nAfter an automated test for a piece of functionality passes, it must continue to pass until the functionality is intentionally changed.\nRunning an automated suite of tests every time new code is checked in helps ensure that regression bugs will be caught quickly.\nQuick feedback means the change is still fresh in some programmer’s mind, so troubleshooting will go more quickly than if the bug weren’t founduntil some testing phase weeks later.\nFailing fast means bugs are cheaper to fix.<br>\nhaving an automation framework in place enabled us to start focusing on doing a better job of capturing requirements in up-front tests.\nThe team also doesn’t accrue too much technical debt, and their velocity is bound to be stable or even increase over time.\nWhen tests that illustrate examples of desired behavior are automated, they become “living” documentation of how the system actually works.\nThey get into the mode of fixing the “bug of the day,” instead of looking at the root cause of the bug and redesigning the code accordingly.\nEducation is the key to getting programmers and the rest of the team to understand the importance of automation.\nPoor practices produce tests that are hard to understand and maintain, and may produce hard-to-interpret results or false failures that taketime to research.\nYou want to automate tests so you can refactor some of the legacy code, but the legacy code isn’t designed for testability, so it is hard toautomate tests even at the unit level.\nThe agile whole-team approach is the foundation to overcoming automation challenges.\ndevelopment is oriented more toward design than testing, so business-facing tests may still not enter their consciousness.\nGetting the whole team involved in test automation may be a cultural challenge.\nAfter a team has mastered the art of TDD, these tests are by far the quickest and least expensive to write. They provide the quickest feedback,too, making them highly valuable. They have the biggest ROI by far of any type of test.\nThese are the functional tests that verify that we are “building the right thing.”\nThese tests operate at the API level or “behind the GUI,” testing the functionality directly without going through the GUI. We write test casesthat set up inputs and fixtures that feed the inputs into the production code, accept the outputs, and compare them to expected results.\nWe try to write them in a domain-specific language that the customers can understand, so they take more work than unit-level tests. They alsogenerally run more slowly, because each test covers more ground than a unit test and may access the database or other components.\nThe top tier represents what should be the smallest automation effort, because the tests generally provide the lowest ROI.\nThey are written after the code is completed, and so are usually written to critique the product and go directly to the regression suite.\nBecause components of the user interface tend to be changed often, these tests are much more brittle than tests that work at a functional orunit level.\nNo matter how many automated tests they have, most systems also need manual testing activities, such as exploratory testing and user acceptancetesting.\nPatrick Wilson-Welsh [2008] adds a descriptive dimension to the test automation pyramid with a “three little pigs” metaphor.\nchek th note later<br>\nMost new agile teams don’t start with this shape pyramid—it’s usually inverted, a left-over from previous projects.\nBecause tests drive development, the whole team is always designing for maximum testability, and the pyramid can grow to the right shape.\nWhen programmers can’t run tests quickly at the touch of a button, they may not be motivated enough to run tests at all.\nAny tedious or repetitive task involved in developing software is a candidate for automation.\nYour team needs the immediate feedback from the unit-level tests to stay on\nPeril: Waiting for Tuesday’s<br>\ngreat story<br>\nMost agile teams find an ongoing build longer than eight to ten minutes to be unworkable.\nA fast-running continuous integration and build process gives the greatest ROI of any automation effort.\nIf your programmers are using TDD as a mechanism to write their tests, then they are not only creating a great regression suite, but they areusing them to design high-quality, robust code.\nA point about testability here—make sure the programmers name their objects or assign IDs to them. If they rely on system-generatedidentifiers, then every time a new object is added to the page, the IDs will change, requiring changes to the tests.\nCheck things like making sure the buttons really work and do what they are supposed to. Don’t try to try to test business functionality.\nAutomation isn’t just for testing.<br>\nIf you are constantly setting up your data, automate the process.\nCleaning up test data is as important as generating it.<br>\nWe don’t think you should automate “look and feel” testing, because an automated script can only look for what you tell it to see.\nOne major goal of exploratory testing is to learn more about the product by doing, and then use that information to improve future development.\nIf a requirement is so obvious that there’s only one way to implement it, and no programmer will ever look at that code later without knowingexactly what it should do, the chances of someone introducing a defect in that code are next to nothing.\nIf you feel comfortable that one-time manual testing does the job and that the risk of future failures doesn’t justify automating regressiontests, don’t automate them.\nyour decision turns out to be wrong, you’ll get another chance to automate them later.\nWeigh the automation cost against the amount of valuable time eaten up by manually doing the test. If it’s easy to do manually, and automatingwouldn’t be quick, just keep it manual.\nThe team members started “strangling” the legacy code by writing all new features in a new test-friendly architecture. They’re graduallyreplacing all of the old code with code written test-first. When they do work on old code to fix bugs, or in the cases where the old code needsupdating, they simply add unit tests for all of the code they change.\nTest automation won’t pay off unless other good development practices are in place. Continuous integration\nRefactoring<br>\ngood unit test coverage.<br>\nprimary reason to write tests is to help guide development.<br>\nApproach automation just as programmers approach coding. Get one small unit of the steel thread working, and then move on to the next.\nLimit the scope of each test case to one test condition or one business rule.\nAvoid dependencies between tests, because they quickly increase complexity and maintenance expense.\nIf you have good coverage in your unit and code integration tests, you don’t need to automate as many functional tests.\nIf you do automate at the higher levels, don’t go overboard and automate every possible path through the system. You don’t have to keep everyautomated test created during the development phase in the regression suite; consider tradeoffs of build time and the chance of findingdefects.\nStriking a balance isn’t an agile principle, it’s just common sense. You need a good-enough solution right now, but it doesn’t have to beperfect.\nThe most important factor is whether your automation tools fit your particular situation right now.\nFind a balance between “It finds the bugs we need to know about and doesn’t cost too much to maintain” and “This is the most elegant and coolsolution we can find.”\nStay away from recorded scripts, invest in maintainability, and minimize the required GUI testing with a good architecture of the application.\nWhen everyone on the team collaborates on a test automation solution, there’s a much better chance it’s going to succeed.\nEvery so often, step back and take a look at the tools you’re using. Is everyone on the team happy with them? Are you missing problems becauseyou don’t have the right tools? Budget time to explore new tools and see if they might fill gaps or replace a tool that isn’t paying off.\nDon’t expect to be able to deliver much business value if you’re still creating your test infrastructure.\nConcepts such as courage, feedback, simplicity, communication, continuous improvement, and responding to change aren’t just agile ideas—they’requalities that are common to all successful teams.\nThe agile maxim of “do the simplest thing that could possibly work” applies to tests as well as code. Keep the test design simple, keep thescope minimal, and use the simplest tool that will do the job.\nShort iterations allow us to experiment with various automation approaches, evaluate results, and change course as quickly as needed.\nAgile development can’t work without automation.<br>\nSolving problems and implementing good solutions takes time.\nWe must help our management understand that without enough time to do things the right way, our technical debt will grow, and our velocity willslow. Implementing solutions the “right” way takes time up front but will save time in the long term.\nThere is never enough time to go back and fix things.<br>\nTake the time to refactor as you go or you’ll end up with a mess eventually.\nWhat are the risks? How much will a production problem cost? What are the benefits of releasing a quick hack? How much technical debt will itadd? What’s the long-term return on investment of a solid design supported with automated tests? How will each approach affect companyprofitability and customer satisfaction? What about the intangible costs, such as the effect that doing poor-quality work has on team morale?\ntalk to the “rubber ducky”: Imagine you’re describing the problem to a coworker. The process of explaining can often make the cause of theproblem jump into view. Simply reading a test aloud to yourself can help you find the weaknesses in it.\nAny solution that removes enough tedium to let you discover potential issues about the application is worth trying.\nYour first choice for testing should try to have tests that can run completely in-memory.\nA fake object such as an in-memory database lets the test do what it needs to do and still give instant feedback.\nIf you’re testing business logic, algorithms, or calculations in code, you’re interested in the behavior of the code itself given certaininputs; you don’t care where the data comes from as long as it accurately represents real data.\nThis supports the idea that tests are independent of each other.\nWe try to design the tests to keep maintenance costs down.<br>\nGenerally, when you’re talking about functional or end-to-end testing, a clone of the production database is most useful for manual exploratorytesting.\nThe first step in choosing an automation tool is to make a list of everything the tool needs to do for you.\nReducing your technical debt and establishing a good testing infrastructure will improve your velocity in the future and free time forexploratory testing.\nBe open to the idea that it might not be right and that you have to throw it out and start over.\nLower your expectations and open your mind. Creative solutions rely on art as much as science.\nRecord/playback scripts are notoriously costly from a maintenance perspective.\nUsing a DSL that employs the nouns and verbs of the application allows an engineer writing a test to focus on the test, not the underlyingcomplexity of interacting with on-screen controls.\nNot everything can be automated, because of budgetary or technical reasons.\nBrainstorm ways to cope in the short term, while you plan how to put together the infrastructure you really need to minimize risk, maximizevelocity, and deliver the best possible product.\nTest management systems, like the tests themselves, should promote communication and collaboration among team members and between differentteams.\nEveryone involved with delivering software needs easy access to tests and test results.\nsimply keep new tests out of the integration and build process until they pass for the first time.\nDon’t be afraid to get something—anything—in place, even if it’s somewhat deficient.\nIf you don’t start somewhere, you’ll never get traction on automation.\nneed the right test automation to deliver business value frequently.\nA year or two from now, you’ll wonder why you thought test automation was so hard.\nWe’ll show how coding and testing are part of one integrated process of delivering software,\nOver time, variations on individual story sizing will average out, and we find that a theme or related group of stories takes about the amountof time expected.\n“No story is done until it’s tested.”<br>\nThere are many times when a story will have a large testing component, and the coding effort is small.\nThe customers prioritize the stories, but there may be dependencies, so it makes sense to do certain stories first, even if they aren’t thehighest priority.\nOne of the basic premises of agile is to deliver working software, so it is important to have the highest-value stories completed first so thatthe software we do deliver meets the customer’s needs.\nThe team needs to develop in small, testable chunks in order to help decide what stories are tentatively planned for which iteration.\nAgile teams continually manage scope in order to meet business deadlines while preserving quality.\nIf you’re using a third-party product as part of your solution, you might assume it has been tested, but that might be a poor assumption.\nIf you’ll be working with other teams developing different components of the same system, or related systems, budget time to coordinate withthem.\nDetailed test planning needs to wait for iteration planning. Still, we need to think about testing at a high level, and try to budget enoughtime for it. We might even take time separately from the release planning meeting to strategize our testing for the release.\nDuring release planning, it’s helpful to know the business conditions of satisfaction for each story or high-level user acceptance test case.\nwhat’s in scope, and what assumptions we’re making. We do some quick risk analysis and plan our test approach to address those risks. Weconsider automation and what we need for test environments and test data. We certainly want to identify milestones and deliverables.\nFor those of you who are starting a brand new project with no previous processes in place, now is the time to consider what testing you willneed. We don’t mean you have to decide how to test each story, but look at the big picture and think about the quadrants.\nIf you’re planning your first release, test environments are a key consideration. You might need a story or iteration just to set up theinfrastructure you need.\nbecause the environment is never stable enough for effective testing.\nUsing test data that closely resembles real data is generally a good practice.\nReal data provides a good base for different scenarios for exploratory testing.\nTest data tends to get stale and out of date over time. Older data, even if it came from production, may no longer accurately reflect currentproduction data.\nUnderstand your needs so that you can choose the approach that is right for your team.\nA test plan should not cover every eventuality or every story, and it is not meant to address traceability. It should be a tool to help youthink about testing risks to your project.\nhigh-level test matrix can be used by the team to show the customer team or management what has been tested already and what is left. A moredetailed test matrix can be used by the team to show what is planned for testing and track the progress of the testing.\nTest results are one of the most important ways to measure progress, see whether new tests are being written and run for each story, andwhether they’re all passing.\nThese metrics should give you continual feedback about how development is proceeding, so that you can respond to unexpected events and changeyour process as needed. Remember, you need to understand what problem you are trying to solve with your metrics so that you can track the rightones.\nIf some functionality was missed, your code coverage report will not bring that to light. You might have 80% code coverage with your tests, butyou’re missing 10% of the code you should have. Driving development with tests helps avoid this problem, but don’t value code coveragestatistics more than they deserve.\nGood metrics require some good planning.<br>\nIt doesn’t tell you how good your tests are but only if a certain chunk of code was run during the test.\nDon’t get caught up with committing to your plans—the situation is bound to change.\nPlan for extra testing time and resources when features may affect systems or subsystems developed by outside teams.\nWe sure don’t want to spend all our time in meetings, or planning for stories that might be re-prioritized. However, if we can make ouriteration planning go faster, and reduce the risk of the stories we’re about to undertake, it’s worth doing some research and brainstormingbefore we start the iteration.\nAfter many experiences of misunderstanding stories and having them far exceed estimations, and finding most “bugs” were missed requirements, wedecided to budget time in the iteration to start talking about the next one.\nWe found that to go fast, we needed to slow down first.<br>\nA deeper understanding of the feature behavior can speed up testing and coding, and can help make sure you deliver the right functionality.\nBecause there are many ways to implement any given story, someone has to decide the specific requirements and capture them in the form ofexamples, conditions of satisfaction, and test cases.\nHelp participants stay focused on concrete examples that crystallize the meaning of the stories.\nyou’re considering each story from multiple points of view. It helps to know what the story means to people in different roles.\nSometimes it is important for the whole team to understand the need, and sometimes it is sufficient for one or two of the team members to dothe research.\nWe know there will always be discoveries along the way, but if we can catch the big “gotchas” first, that will help the team work aseffectively as possible.\nWhile agile principles say to collaborate closely with the customer, in some situations you have to be creative and find another way to getclear business requirements.\nThis keeps the discussion at a concrete level and is a fast way to learn how the new features should work.\nAsk your customers to write down examples and high-level test cases before the iteration. This can help them think through the stories more andhelp define their conditions of satisfaction. It also helps them identify which features are critical, and which might be able to wait. It alsohelps to define when the story is done and manage expectations among the team.\nMock-ups are essential for stories involving the UI or a report. Ask your customers to draw up their ideas about how the page should look.Share these ideas with the team.\nSometimes we have legacy system defects to worry about, and sometimes fixing a defect is just not high enough value for the business to fix.\nBefore the next iteration is an ideal time to review outstanding issues with the customer and triage the value of fixing versus leaving them inthe system. Those that are deemed necessary to be fixed should be scheduled into the next iteration.\nWhen we get small stories to test on a regular basis, we do not have them all finished at once and stacked up at the end of the iterationwaiting to be tested.\nIt’s better to address uncertainty early on and then do more research or a spike to get more information.\nWhen writing programmer task cards, make sure that coding task estimates include time for writing unit tests and for all necessary testing byprogrammers.\nCertainly think about it as you prep for the next iteration. When the iteration starts, whatever test data is missing must be created orobtained, so don’t forget to allow for this in estimates.\nIn XP, we can’t exceed the number of story points we completed in the last iteration. In Scrum, we commit to a set of stories based on theactual time we think we need to complete them.\nmake sure enough time is allocated to testing, and to remind the team that testing and quality are the responsibility of the whole team.\nWhen you are looking at stories, and the programmers start to think about implementation, always think how you can test them.\nthink about what kind of variations you will need to test.<br>\nWhen testability is an issue, make it the team’s problem to solve. Teams that start their planning by writing test task cards probably have anadvantage here, because as they think about their testing tasks, they’ll ask how the story can be tested.\nThey may include examples of both desired and undesired behavior.\nWhat’s important as you begin the iteration is that you quickly learn the basic requirements for each story and express them in context in away that works for the whole team.\nThey might produce code that’s technically bug-free but doesn’t quite match the customer’s desired functionality.\nThe great advantage of having executable tests as part of your requirements document is that it’s hard to argue with their results.\nHowever, if we’re using tests to guide coding, we have to start with the basics. Write the simplest happy path test you can in order to showthat the core functionality works.\nWe’re working on an extremely tight schedule, and neither the programmers nor the testers have time to stop and run manual tests over and over.They do have time to click a button and run an automated test. That test needs to fail in a way that makes the cause as obvious as possible.Ideally, we would give these tests to the programmers so that they could execute them as they code. That is one reason why picking the rightautomation framework is so important.\nHowever, to be successful in the long run, these tests do need to be automated.\nGet the basics working first. If you think of more cases based on some risk analysis, you can always add extra tests later.\nWhen disagreements or questions arise, having three different viewpoints is an effective way to make sure you get a good solution and you won’thave to rehash the issue later.\nBe aware that some of what you learn in testing the final story may be considered “nice to have,”\n“Rubber Ducking” and “Thinking Out Loud” are surprisingly effective ways to solve your own problems.\nWe want to keep the process as simple as possible; simplicity is a core value.\nMissing some functionality from a release is better than missing the entire release because testing couldn’t be completed on all or moststories.\nWe stress testing early in order to catch as many bugs as possible while the programmers are still working on the story.\nThe more bugs you can fix immediately, the less technical debt your application generates and the less “defect” inventory you have.\nthe cost to fix an error found after product release was four to five times as much as one uncovered during design, and up to 100 times morethan one identified in the maintenance phase\nJanet finds that having both may seem like a duplication of effort, but the visibility of progress to the team far outweighs the extra overheadof writing up the task cards and moving them as they are completed. Having the story board gives your team focus during the stand-ups or whenyou are talking to someone outside the team about your progress.\nWork through as many examples as you need until the team understands enough different aspects of the system.\nWe have unit tests for a reason, so whenever one fails, the team’s highest priority (apart from a showstopper production issue) should be tofix it and get the build working again.\nIf the build takes longer than the average frequency of code check-ins, builds start to stack up, and testers can’t get the code they need totest. The XP guideline for build time is ten minutes\nKnow what problem you are trying to solve before you start measuring data points and going to all the work of analyzing the results.\nDon’t be afraid to stop using metrics when they are no longer useful.\nJanet encourages rotating this honor.<br>\nTaking time to celebrate successes lets your team take a step back, get a fresh perspective, and renew its energy so it can keep improving yourproduct.\nIn agile development, we get a chance to stop and get a new perspective at the end of each short iteration. We can make minor coursecorrections,\nBefore we release, we want to make sure all of the deliverables are ready and polished up appropriately.\nBecause testing and coding are part of one process in agile development,\nMost teams accumulate some technical debt, despite the best intentions, especially if they’re working with legacy code.\nIt’s the time when the team applies the finishing touches to the product.\nand if you have a test integration system, we recommend that you be sure that you have tried to integrate long before the end game.\nIf you have identified these risks early and done as much up-front testing as possible, the testing done during the end game should be finalverification only.\nConstant communication between different development team members is always important, but it’s especially critical as we wrap up the release.\nDid the testing needs change, or is the team taking a chance and sacrificing quality to meet a deadline? The team should cut the release scopeif the delivery date is fixed and in jeopardy.\nThe customers have the ultimate say in what will work for the business.\nIt’s performed by all affected business departments to verify usability of the system and to confirm existing and new (emphasis on new)business functionality of the system.\nIf that is the case, then try moving the UAT cycle up to run parallel with your end game. The application should be stable enough so that yourteam could deploy to the customer’s test system at the same time as they deploy to staging.\nBoth of us have found it helpful to provide customers involved in doing UAT with a report of all of the testing done during development, alongwith the results. That helps them decide where to focus their own testing.\nYour team will want to get feedback on new features from your real customers, and this is one mechanism for doing so.\nAlpha testing is to get feedback on the features—not to report bugs.\nBeta testing is closer to UAT. It is expected that the release is fairly stable and can actually be used.\nWhile plans rarely work as expected, planning ahead can still help you make sure the right people are in place to deliver the product in atimely manner.\nWho is accepting the product, and what are their expectations?\nThe problem is that the number by itself is just a number, and there are so many reasons why it might be high or low.\nThey don’t downgrade the severity of bugs to medium so they can say they achieved the criterion of no high-severity bugs. Instead, theyfrequently look at bug trends and think of ways to ensure that high-severity bugs don’t occur in production.\nYour quality level should be negotiated with your customer up front so that there are no unpleasant surprises.\nIf your customer has a very low tolerance for bugs, and 100% of those acceptance tests must be passing, your iteration velocity should takethat into consideration. If new features are more important than bug fixes, well, maybe you will be shipping with bugs.\n“Doneness” includes testing, and testing is often the thing that gets postponed when time is tight. Make sure your success criteria at everylevel includes all of the necessary testing to guide and validate development.\nWe must be sure they know what new functionality to expect and that they have some means to deal with problems that arise.\nNew releases should be as transparent as possible to the customer. The fewer emergency releases or patches required after a release, the moreconfidence your customer will have in both the product and the development team.\nWhen testing is a team priority, and anyone can sign up for testing tasks, the team designs testable code.\nMake your problems the team’s problems, and make their problems yours. Ask your teammates to adopt a whole-team approach.\nWe don’t mean that you should put on your Super Tester cape and go protect the world from bugs.\nBe courageous in seeking help and experimenting with new ideas. Focus on delivering value. Communicate as directly and as often as possible. Beflexible in responding to change. Remember that agile development is people-centric, and that we should all enjoy our work.\nWhen you’re faced with problems that impact testing, bring those problems to the team. Ask the team to brainstorm ways to overcome theseobstacles.\nCan an agile team succeed with no test automation? Maybe, but the successful teams that we know rely on automated regression tests.\nWithout the short feedback cycle and safety net regression that suites provide, your team will soon become mired in technical debt, with agrowing defect queue and ever-slowing velocity.\nTesters are in a unique position to help provide feedback in the form of automated test results, discoveries made during exploratory testing,and observations of actual users of the system.\nMost agile practices are valuable because they create feedback loops that allow teams to adapt.\nIf you don’t have meaningful feedback, then you’re not agile. You’re just in a new form of chaos.\nWhile we think of these as agile practices, they’ve been around longer than the term “agile development,” and they’re simply core practices ofsuccessful software development.\nImplementing a continuous integration process should be one of the first priorities of any software development team.\nYou can’t test productively without a test environment that you control.\nEven good software development teams, feeling time pressure, neglect refactoring or resort to quick fixes and hacks to solve a problem quickly.\nAs the code becomes more confusing and hard to maintain, more bugs creep in, and it doesn’t take long before the team’s velocity is consumed bybug fixes and trying to make sense out of the code in order to add new features.\nBusinesses need their software development teams to remain consistently productive. They may have to reduce the scope of their desired featuresin order to allow enough time for good, test-guided code design and good practices such as continual small refactoring.\nIf a story is tested in the iteration after which it was coded and bugs are found, the programmer has to stop working on the new story,remember how the code worked for the last iteration’s story, fix it, and wait for someone to test the fix.\nThere are few facts in software development, but we know for sure that bugs are cheaper to fix the sooner they’re found.\nIf your team doesn’t share this view, ask everyone to think about their focus on quality, their desire to deliver the best possible product,and what steps they can take to ensure that the team achieves its goals.\nthe combination of multiple agile practices is greater than the sum of the parts.\nIt’s easy for everyone on the team to narrowly focus only on the task or story at hand. That’s a drawback of working on small chunks offunctionality at a time.\nthe value of any practice depends on its context.<br>\nA legacy system is one that does not have any (or few) automated regression tests.\nProduct Backlog is a Scrum term for the prioritized master list of all functionality desired in the product.\ncan be defined as actions taken to ensure compliance with a quality standard.\nA regression test verifies that the behavior of the system under test hasn’t changed.\nRegression tests should be automated to ensure continual feedback.\nshort description of functionality told from the perspective of the user that is valuable to either the user or the customer team.\nTechnical Debt Ward Cunningham first introduced this metaphor. When a team produces software without using good practices such as TDD,continuous integration, and refactoring, it may incur technical debt. Like financial debt, technical debt accrues interest that will cost theteam more at a later date. Sometimes this debt may be worthwhile, such as to take advantage of a sudden business opportunity. Usually, though,technical debt compounds and slows the team’s velocity. Less and less business value can be produced in each iteration because the code lacks asafety net of automated regression tests or has become difficult to understand and maintain.\nan XML format for describing network services as a set of endpoints operating on messages </p>","frontmatter":{"title":"Agile Testing - A practical guide for testers and agile teams","language":"en-US","coverPath":"agile-testing-a-practical-guide-for-testers-and-agile-teams","status":"Read","date":"2015-06-01"}}}]}},"pageContext":{}},"staticQueryHashes":["1507822185","2095566405","2894216461","4246675000","425755332"]}