{"componentChunkName":"component---src-templates-note-template-js","path":"/notes/site-reliability-engineering","result":{"data":{"markdownRemark":{"frontmatter":{"title":"Site Reliability Engineering","date":"2021-05-07","published":null,"tags":null},"tableOfContents":"<ul>\n<li><a href=\"#foreword\">Foreword</a></li>\n<li><a href=\"#preface\">Preface</a></li>\n<li>\n<p><a href=\"#part-i---introduction\">Part I - Introduction</a></p>\n<ul>\n<li>\n<p><a href=\"#chapter-1---introduction\">Chapter 1 - Introduction</a></p>\n<ul>\n<li><a href=\"#the-sysadmin-approach-to-service-management\">The Sysadmin Approach to Service Management</a></li>\n<li><a href=\"#googles-approach-to-service-management-site-reliability-engineering\">Google’s Approach to Service Management: Site Reliability Engineering</a></li>\n<li><a href=\"#devops-or-sre\">DevOps or SRE?</a></li>\n<li>\n<p><a href=\"#tenets-of-sre\">Tenets of SRE</a></p>\n<ul>\n<li><a href=\"#ensuring-a-durable-focus-on-engineering\">Ensuring a Durable Focus on Engineering</a></li>\n<li><a href=\"#pursuing-maximum-change-velocity-without-violating-a-services-slo\">Pursuing Maximum Change Velocity Without Violating a Service’s SLO</a></li>\n<li><a href=\"#monitoring\">Monitoring</a></li>\n<li><a href=\"#alerts\">Alerts</a></li>\n<li><a href=\"#tickets\">Tickets</a></li>\n<li><a href=\"#logging\">Logging</a></li>\n<li><a href=\"#emergency-response\">Emergency Response</a></li>\n<li><a href=\"#change-management\">Change Management</a></li>\n<li><a href=\"#demand-forecasting-and-capacity-planning\">Demand Forecasting and Capacity Planning</a></li>\n<li><a href=\"#provisioning\">Provisioning</a></li>\n<li><a href=\"#efficiency-and-performance\">Efficiency and Performance</a></li>\n</ul>\n</li>\n<li><a href=\"#my-summary\">My Summary</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#chapter-2---the-production-environment-at-google-from-the-viewpoint-of-an-sre\">Chapter 2 - The Production Environment at Google, from the Viewpoint of an SRE</a></p>\n<ul>\n<li>\n<p><a href=\"#system-software-that-organizes-the-hardware\">System Software That \"Organizes\" the Hardware</a></p>\n<ul>\n<li><a href=\"#networking\">Networking</a></li>\n</ul>\n</li>\n<li><a href=\"#our-development-environment\">Our Development Environment</a></li>\n<li><a href=\"#my-summary-1\">My Summary</a></li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>","html":"<h1>Foreword</h1>\n<blockquote>\n<p>Tools were only components in processes, working alongside chains of software, people, and data. Nothing here tells us how to solve problems universally, but that is the point. Stories like these are far more valuable than the code or designs they resulted in. Implementations are ephemeral, but the documented reasoning is priceless. Rarely do we have access to this kind of insight.</p>\n</blockquote>\n<h1>Preface</h1>\n<blockquote>\n<p>We apply the principles of computer science and engineering to the design and development of computing systems: generally, large distributed ones. Sometimes, our task is writing the software for those systems alongside our product development counterparts; sometimes, our task is building all the additional pieces those systems need, like backups or load balancing, ideally so they can be reused across systems; and sometimes, our task is figuring out how to apply existing solutions to new problems.</p>\n</blockquote>\n<blockquote>\n<p>a system isn’t very useful if nobody can use it! Because reliability is so critical, SREs are focused on finding ways to improve the design and operation of systems to make them more scalable, more reliable, and more efficient.</p>\n</blockquote>\n<blockquote>\n<p> it’s still worth putting lightweight reliability support in place early on, because it’s less costly to expand a structure later on than it is to introduce one that is not present.</p>\n</blockquote>\n<h1>Part I - Introduction</h1>\n<h2>Chapter 1 - Introduction</h2>\n<h3>The Sysadmin Approach to Service Management</h3>\n<blockquote>\n<p>Direct costs are neither subtle nor ambiguous. Running a service with a team that relies on manual intervention for both change management and event handling becomes expensive as the service and/or traffic to the service grows, because the size of the team necessarily scales with the load generated by the system.</p>\n</blockquote>\n<blockquote>\n<p>At their core, the development teams want to launch new features and see them adopted by users. At <em>their</em> core, the ops teams want to make sure the service doesn’t break while they are holding the pager. Because most outages are caused by some kind of change—a new configuration, a new feature launch, or a new type of user traffic—the two teams’ goals are fundamentally in tension.</p>\n</blockquote>\n<blockquote>\n<p>The ops team attempts to safeguard the running system against the risk of change by introducing launch and change gates.</p>\n</blockquote>\n<h3>Google’s Approach to Service Management: Site Reliability Engineering</h3>\n<blockquote>\n<p>Site Reliability Engineering teams focus on hiring software engineers to run our products and to create systems to accomplish the work that would otherwise be performed, often manually, by sysadmins.</p>\n</blockquote>\n<blockquote>\n<p>SRE is what happens when you ask a software engineer to design an operations team.</p>\n</blockquote>\n<blockquote>\n<p>Common to all SREs is the belief in and aptitude for developing software systems to solve complex problems.</p>\n</blockquote>\n<blockquote>\n<p>The result of our approach to hiring for SRE is that we end up with a team of people who (a) will quickly become bored by performing tasks by hand, and (b) have the skill set necessary to write software to replace their previously manual work, even when the solution is complicated.</p>\n</blockquote>\n<blockquote>\n<p>By design, it is crucial that SRE teams are focused on engineering. Without constant engineering, operations load increases and teams will need more people just to keep pace with the workload.</p>\n</blockquote>\n<blockquote>\n<p>Google places <em>a 50% cap on the aggregate \"ops\" work for all SREs</em>—tickets, on-call, manual tasks, etc.</p>\n</blockquote>\n<blockquote>\n<p>Because SREs are directly modifying code in their pursuit of making Google’s systems run themselves, SRE teams are characterized by both rapid innovation and a large acceptance of change. Such teams are relatively inexpensive—supporting the same service with an ops-oriented team would require a significantly larger number of people. </p>\n</blockquote>\n<h3>DevOps or SRE?</h3>\n<blockquote>\n<p>One could equivalently view SRE as a specific implementation of DevOps with some idiosyncratic extensions.</p>\n</blockquote>\n<h3>Tenets of SRE</h3>\n<blockquote>\n<p>SRE team is responsible for the <em>availability, latency, performance, efficiency, change management, monitoring, emergency response, and capacity planning</em> of their service(s).</p>\n</blockquote>\n<h4>Ensuring a Durable Focus on Engineering</h4>\n<blockquote>\n<p> by monitoring the amount of operational work being done by SREs, and redirecting excess operational work to the product development teams: reassigning bugs and tickets to development managers, [re]integrating developers into on-call pager rotations, and so on.</p>\n</blockquote>\n<blockquote>\n<p>Postmortems should be written for all significant incidents, regardless of whether or not they paged</p>\n</blockquote>\n<blockquote>\n<p>This investigation should establish what happened in detail, find all root causes of the event, and assign actions to correct the problem or improve how it is addressed next time</p>\n</blockquote>\n<blockquote>\n<p>Google operates under a <em>blame-free postmortem culture</em>, with the goal of exposing faults and applying engineering to fix these faults, rather than avoiding or minimizing them.</p>\n</blockquote>\n<h4>Pursuing Maximum Change Velocity Without Violating a Service’s SLO</h4>\n<blockquote>\n<p>thus, the marginal difference between 99.999% and 100% gets lost in the noise of other unavailability, and the user receives no benefit from the enormous effort required to add that last 0.001% of availability.</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>What level of availability will the users be happy with, given how they use the product?</li>\n<li>What alternatives are available to users who are dissatisfied with the product’s availability?</li>\n<li>What happens to users’ usage of the product at different availability levels?</li>\n</ul>\n</blockquote>\n<blockquote>\n<p>Once that target is established, the error budget is one minus the availability target.</p>\n</blockquote>\n<blockquote>\n<p>An outage is no longer a \"bad\" thing—it is an expected part of the process of innovation, and an occurrence that both development and SRE teams manage rather than fear.</p>\n</blockquote>\n<h4>Monitoring</h4>\n<blockquote>\n<p>Monitoring should never require a human to interpret any part of the alerting domain. Instead, software should do the interpreting, and humans should be notified only when they need to take action.</p>\n</blockquote>\n<h4>Alerts</h4>\n<blockquote>\n<p>Signify that a human needs to take action immediately in response to something that is either happening or about to happen, in order to improve the situation.</p>\n</blockquote>\n<h4>Tickets</h4>\n<blockquote>\n<p>Signify that a human needs to take action, but not immediately. The system cannot automatically handle the situation, but if a human takes action in a few days, no damage will result.</p>\n</blockquote>\n<h4>Logging</h4>\n<blockquote>\n<p>No one needs to look at this information, but it is recorded for diagnostic or forensic purposes. The expectation is that no one reads logs unless something else prompts them to do so.</p>\n</blockquote>\n<h4>Emergency Response</h4>\n<blockquote>\n<p> Reliability is a function of mean time to failure (MTTF) and mean time to repair (MTTR). The most relevant metric in evaluating the effectiveness of emergency response is how quickly the response team can bring the system back to health—that is, the MTTR.</p>\n</blockquote>\n<blockquote>\n<p>Humans add latency. Even if a given system experiences more <em>actual</em> failures, a system that can avoid emergencies that require human intervention will have higher availability than a system that requires hands-on intervention</p>\n</blockquote>\n<blockquote>\n<p>When humans are necessary, we have found that thinking through and recording the best practices ahead of time in a \"playbook\" produces roughly a 3x improvement in MTTR as compared to the strategy of \"winging it.\"</p>\n</blockquote>\n<h4>Change Management</h4>\n<blockquote>\n<p>SRE has found that roughly 70% of outages are due to changes in a live system</p>\n</blockquote>\n<blockquote>\n<ul>\n<li>Implementing progressive rollouts</li>\n<li>Quickly and accurately detecting problems</li>\n<li>Rolling back changes safely when problems arise</li>\n</ul>\n</blockquote>\n<h4>Demand Forecasting and Capacity Planning</h4>\n<blockquote>\n<p>Capacity planning should take both organic growth (which stems from natural product adoption and usage by customers) and inorganic growth (which results from events like feature launches, marketing campaigns, or other business-driven changes) into account.</p>\n</blockquote>\n<h4>Provisioning</h4>\n<blockquote>\n<p>Adding new capacity often involves spinning up a new instance or location, making significant modification to existing systems (configuration files, load balancers, networking), and validating that the new capacity performs and delivers correct results. Thus, it is a riskier operation than load shifting, which is often done multiple times per hour, and must be treated with a corresponding degree of extra caution.</p>\n</blockquote>\n<h4>Efficiency and Performance</h4>\n<blockquote>\n<p>Resource use is a function of demand (load), capacity, and software efficiency.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>Historically, companies use the sysadmin model. Which is responsible for dividing the Developers and Operations. At its core developers and operations have different goals. Developers want to deliver code to production as fast as possible. Operations wants to have a reliable system. Since, changes might cause outage, there is a tension between developers and operations goals. This model is also expensive due to the need to add more people as the software scales.</p>\n<p>Google's SRE teams are composed of software engineers hired to do operations. These teams have the skill to automate the process and can be rapidly bored by repetitive tasks. Therefore, creating a great combination for continuous improvement.</p>\n<p>The SRE skills are very similar to developers skills, UNIX and networking being the main differentiators.</p>\n<p>It's important that these teams focus on engineering. Otherwise, it will be required to increase the staff of the teams. To achieve the focus on engineering, Google provides a cap of 50% of work for engineering. If they workload on operations exceeds this number, bugs, on-call and so on are redirected to the development team.</p>\n<p>SRE can be considered a Google's implementation of DevOps.</p>\n<p>SRE team is responsible for the:</p>\n<ul>\n<li>\n<p>Availability</p>\n<p>There's no gain to have a 100% reliable system, because there are other players into play with lower reliability (WiFi, ISP, power grid, ...)</p>\n<p>The availability is a product decision. The error budget is value of permitted unavailability. Developers may use this budget to speed up releases. The aim is not to have zero outages, but to not exceed the error budget.</p>\n<p>Postmortem it's important to expose faults and improve systems. A culture of fixing errors instead of hiding or minimizing them.</p>\n</li>\n<li>Latency</li>\n<li>Efficiency and Performance</li>\n<li>\n<p>Change Management:</p>\n<ul>\n<li>Implementing progressive rollouts</li>\n<li>Quickly and accurately detecting problems</li>\n<li>Rolling back changes safely when problems arise</li>\n</ul>\n</li>\n<li>\n<p>Monitoring: </p>\n<p>Keep track of a system’s health and availability. Software monitors human are notified when need to take action.</p>\n<ul>\n<li>Alerts: Human needs to take an action immediately in response to something that is either happening or about to happen.</li>\n<li>Tickets: a human needs to take action, but not immediately. If a human takes action in a few days, no damage will result</li>\n<li>Logging: Recorded for diagnostic or forensic purposes. People read the logs when something prompts them to do so.</li>\n</ul>\n</li>\n<li>\n<p>Emergency Response:</p>\n<p>How quickly the response team can bring the system back to health. Adding a \"playbook\" produces roughly a 3x improvement.</p>\n<ul>\n<li>MTTR (Repair)</li>\n<li>MTTF (Failure)</li>\n</ul>\n</li>\n<li>\n<p>Capacity Planning: </p>\n<p>There is sufficient capacity and redundancy to serve projected future demand with the required availability</p>\n<ul>\n<li>Organic growth (natural adoption)</li>\n<li>Inorganic growth (feature launches, marketing campaign, ...)</li>\n</ul>\n</li>\n</ul>\n<h2>Chapter 2 - The Production Environment at Google, from the Viewpoint of an SRE</h2>\n<h3>System Software That \"Organizes\" the Hardware</h3>\n<blockquote>\n<p>Given the large number of hardware components in a cluster, hardware failures occur quite frequently. In a single cluster in a typical year, thousands of machines fail and thousands of hard disks break; when multiplied by the number of clusters we operate globally, these numbers become somewhat breathtaking.</p>\n</blockquote>\n<h4>Networking</h4>\n<blockquote>\n<p>In order to minimize latency for globally distributed services, we want to direct users to the closest datacenter with available capacity.</p>\n</blockquote>\n<h3>Our Development Environment</h3>\n<blockquote>\n<p>When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it notifies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.</p>\n</blockquote>\n<h3>My Summary</h3>\n<p>This chapter they describe Google's Environment.</p>\n<p>The cluster operating system Borg handles resource allocation. Borg is the precedent of Kubernetes. r</p>"}},"pageContext":{"title":"Site Reliability Engineering"}},"staticQueryHashes":["1507822185","2095566405","2894216461","425755332"]}